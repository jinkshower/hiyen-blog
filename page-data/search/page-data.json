{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"갈만해 프로젝트를 소개합니다 갈만해 웹사이트 갈만해 Github ![Pasted image 20240912211252.png] '갈만해'라는 웹서비스를 운영하고 있습니다. 블로그에 웹 서비스를 개발하며 겪은 문제점과 해결 과정을 공유하려고 합니다. 이번 글은 갈만해 프로젝트의 초반부에 겪었던문제들과 그에 대한 제 나름대로의 해법을 담았습니다. 갈만해 프로…","fields":{"slug":"/galmanhae_first/"},"frontmatter":{"date":"August 25, 2024","title":"외부 API 호출과 데이터 처리","tags":["project","galmanhae"]},"rawMarkdownBody":"\n\n## 갈만해 프로젝트를 소개합니다\n\n- [갈만해 웹사이트](https://galmanhae.site/)\n- [갈만해 Github](https://github.com/jinkshower/galmanhae)\n\n![[Pasted image 20240912211252.png]]\n\n'갈만해'라는 웹서비스를 운영하고 있습니다. 블로그에 웹 서비스를 개발하며 겪은 문제점과 해결 과정을 공유하려고 합니다. 이번 글은 갈만해 프로젝트의 초반부에 겪었던문제들과 그에 대한 제 나름대로의 해법을 담았습니다.\n\n갈만해 프로젝트는 실시간으로 서울 인구밀집 장소들의 날씨와 혼잡도를 계산하여 외출하기 적합한 정도를 알려주는 서비스입니다.\n\n## 어떤 API를 호출하지?\n\n가장 처음 처리해야 하는 부분은 이 공공데이터들을 받느냐 였습니다. 저에게 필요한 데이터는 `장소 이름, 위도, 경도, 온도, 강수확률, 혼잡도` 입니다. \n\n이를 한번에 해결해주는 공공 API가 있습니다. 장소 코드만 요청에 담으면 모든 정보를 내려주는 서울 실시간 도시데이터 API였죠. \n\n하지만 이 API는 특정 값만 받을 수 없어서 한 장소마다 35KB, 서비스에 필요한 100여곳의 데이터 크기를 합치면 대략 35MB의 데이터 전송이 필요합니다. 저에게 필요한 데이터는 기껏 해봐야 1KB도 안될텐데 말이죠.\n\n따라서 일정 간격으로 35MB의 데이터를 주고 받는 것보다 원하는 데이터만 받게 해 네트워크 오버헤드를 줄이는 것으로 결정했습니다. \n\n최종적으로 선택하게 된 두 공공데이터 API는 [서울 실시간 인구 데이터]([https://data.seoul.go.kr/dataList/OA-21778/A/1/datasetView.do](https://data.seoul.go.kr/dataList/OA-21778/A/1/datasetView.do), [기상청 단기예보]([https://www.data.go.kr/tcs/dss/selectApiDataDetailView.do?publicDataPk=15084084](https://www.data.go.kr/tcs/dss/selectApiDataDetailView.do?publicDataPk=15084084) 였습니다.  두 API에서 원하는 데이터만 제가 골라서 애플리케이션에서 조립하면 될 거라고 생각했습니다.\n\n## 호출 파라미터를 어떻게 만들지?\n\n하지만 문제가 있었습니다. 장소 코드만 보내면 원하는 데이터를 바로 받을 수 있는 인구 데이터API와 달리 기상청 단기예보는 기상청 자체의 x,y 좌표 계산법을 사용하고 이 x,y 값을 파라미터로 받고 있었던 것이었습니다. \n\n엑셀파일을 하나하나 보면서 위경도를 검색하고 x,y좌표를 손수 계산하여 적는.. 그런 일을 상상했으나 우리는 개발자니까 자동화하는 스크립트를 작성하여 csv파일로 만들었습니다.  \n\n서울시는 장소 목록에 대한 위도, 경도를 엑셀이 아니라 shp파일로 제공하고 있습니다.\n\n![[Pasted image 20240912161539.png]]\n(서울시가 제공하는 zip 파일의 shp 데이터를 파싱하여 csv로 만드는 python 스크립트 일부)\n\n![[Pasted image 20240912161715.png]]\n(만들어진 csv파일)\n\n해당 csv파일을 프로젝트내에서 사용해서 파싱, 호출하는 로직을 작성하면 되겠군요.\n\n## csv파일을 어떻게 사용하지 ?\n\n매번 api호출을 할 때마다 csv파일을 읽고 해당 파일에서 값을 파싱하여 사용할 수도 있지만 매번 디스크 I/O가 발생, 매번 모든 데이터에 대한 객체를 생성하게 되어 성능에 좋지 않을 수 있다 생각되었습니다.\n\n따라서 애플리케이션 실행시 Bean으로 만들어서 주입받을 수 있게 한다면 재사용할 수 있고 Bean내부에서 쓰기 좋은 형태로 Data를 Parsing한다면 클라이언트 입장에서 모든 column을 읽지 않아도 되어 처리하는 코드가 줄어 들고, 성능상 이점이 있을 것으로 판단하였습니다. \n\n```java\n@Component  \npublic class DataParser {  \n  \n    private static final String CSV_FILE = \"src/main/resources/location_mapping.csv\";  \n    private static final String DELIMITER = \",\";  \n  \n    /*  \n     * CSV 파일을 읽고 PlaceInfo 객체로 변환한다  \n     */    public List<PlaceInfo> readCSV() {  \n       List<PlaceInfo> placeInfos;  \n  \n       try (final BufferedReader reader = Files.newBufferedReader(Paths.get(CSV_FILE))) {  \n          placeInfos = reader.lines()  \n             .skip(1) // skip header  \n             .map(line -> line.split(DELIMITER)) // split by delimiter  \n             .map(tokens -> new PlaceInfo(  \n                new AreaInfo(tokens[1], tokens[2]),  \n                new LocationInfo(tokens[3], tokens[4]),  \n                new WeatherInfo(tokens[5], tokens[6])  \n             ))// map to PlaceInfo  \n             .toList();  \n       } catch (IOException e) {  \n          throw new RuntimeException(e);  \n       }  \n  \n       return placeInfos;  \n    }  \n}\n```\n이렇게 DataParser라는 클래스에 상세한 csv파일 파싱 로직을 넣고 \n\n```java\n@Component  \n@RequiredArgsConstructor  \npublic class DataStore {  \n  \n    private final DataParser dataParser;  \n    private List<PlaceInfo> placeInfos;  \n  \n    @PostConstruct  \n    public void initializeData() {  \n       placeInfos = dataParser.readCSV();  \n    }\n}\n```\n이 파싱한 데이터들을 가지고 있을 클래스에 @PostConstruct를 사용하여 ApplicationContext의 빈생성시 데이터 초기화 로직을 호출하게 만들었습니다. \n\n## 어떻게 호출하지?\n\n외부 API를 호출하는데 쓸 수 있는 기술은 많습니다. 전통적으로 Spring에서는 RestTemplate를 제공하고 있습니다. \n\n하지만 RestTemplate는 이제 더 이상 업데이트가 되지 않는 Maintenance mode이고 저는 빠른 개발을 위해서 retry, 예외 처리 로직들이 잘 정리되어 있고 쉽고 간단하게 호출 할 수 있는 방법을 찾게 되었습니다.\n\n그 중 최근 아주 많이 쓰이고 어노테이션을 사용하여 선언적인 api 호출 코드를 만들 수 있어 가독성이 좋고 보일러 플레이트가 적은 [OpenFeign](https://github.com/OpenFeign/feign)을 적용하기로 결정하였습니다. 상세한 구현 방법은 [PR](https://github.com/jinkshower/galmanhae/pull/6)을 참조하면 좋을 것 같습니다!\n\n구현을 하다보니 코드가 복잡해지고 길어지는 것을 발견하고 \n\n1. 각각의 api 호출을 담당하는 service 객체를 만들고 feign client를 주입받게 했습니다. 이후 api 호출방법이 달라져도 영향을 최소화하기 위해서 입니다.\n2. 그 서비스들을 통제하는 'DataProcessor'클래스를 만들어 데이터 처리 과정이 한눈에 보이고 고치기 쉽게 만들었습니다.\n\n```java\n@Component  \n@RequiredArgsConstructor  \npublic class DataProcessor {  \n  \n    private final WeatherService weatherService;  \n    private final CongestionService congestionService;  \n    private final DataSaveService dataSaveService;  \n    private final CSVDataStore csvDataStore;  \n  \n    public void process() {  \n       final List<PlaceInfo> placeInfos = csvDataStore.getPlaceInfos();  \n       final List<Place> places = new ArrayList<>();  \n  \n       for (final PlaceInfo placeInfo : placeInfos) {  \n          final Place place = aggregatePlace(placeInfo);  \n          places.add(place);  \n          break;  \n       }  \n  \n       dataSaveService.saveAll(places);  \n    }  \n  \n    private Place aggregatePlace(final PlaceInfo placeInfo) {  \n       final AreaInfo areaInfo = placeInfo.areaInfo();  \n       final LocationInfo locationInfo = placeInfo.locationInfo();  \n       final WeatherInfo weatherInfo = placeInfo.weatherInfo();  \n  \n       final Congestion congestion = congestionService.fetch(areaInfo.areaCode());  \n       final Weather weather = weatherService.fetch(weatherInfo.latitude(), weatherInfo.longitude());  \n       final Location location = Location.of(Double.valueOf(locationInfo.latitude()), Double.valueOf(locationInfo.longitude()));  \n  \n       return PlaceMapper.toPlace(areaInfo.areaName(), location, weather, congestion);  \n    }\n}\n```\n\n![[Pasted image 20240912164826.png]]\n따라서 csv파일을 읽고 각 api를 호출하여 Place라는 객체를 만드는 과정은 이렇게 이루어집니다. \n\n## 문제점과 개선\n\n현 방식의 문제점은 무엇일까요 ?\n\n1. 서울시에서 제공하는 장소가 추가되면 csv파일을 다시 수동으로 만들어서 업로드 해야한다. 수동작업의 오류를 감당하고 배포도 다 새로해야 한다.\n2.  한 쓰레드에서 for문을 돌며 모든 API 호출을 한다.\n\n첫번째 문제점은 누구나 알 수 있고 두번째 문제점은 조금 더 설명하자면, 외부 API를 호출하는 것은 즉각적인 성공을 보장할 수 없습니다. 네트워크의 지연을 감수해야 하고 외부 서버가 장애가 날 경우에도 대처해야 합니다. 따라서 실패하는 가능성을 두고 코드를 구현해야 합니다.\n\n한 쓰레드에서 해당 작업이 일어나면 여느 로직의 실패가 그렇듯이 feign도 예외를 던집니다. 즉, 우리는 몇 번의 실패를 예상하고 그 실패로 예외가 발생하는 상황도 대비해야 합니다.\n\n## 개선 \n\n그리하여 timeout, retry 등 기본적인 외부 API 호출에 필요한 설정을 해주었습니다. (수치는 각자의 서비스에 맞춰 조절하면 되겠죠?) 또한 저는 circuit breaker도 설정을 해주었습니다. circuit breaker는 일정 수치 이상의 외부 API 통신 실패시 open 상태가 됩니다.\n\ncircuit breaker가 필요한 이유는 외부 api의 장애가 감지될 시 요청을 fast fail하게 만들고 api 호출과 timeout 대기를 하지 않게 해주어 서버의 스레드가 무의미하게 block되는 상황을 피하게 해줍니다. 저는 open-feign에서 지원하는 resilience4j를 사용하였습니다.\n\n또한 외부 API호출과 저장에 비동기처리하는 방식을 선택했는데요, 위에서 말했듯이 하나의 스레드가 모든 요청을 처리할 시 순서대로 응답을 timeout만큼 기다리고 retry를 수행하면이후 요청을 모두 하나의 쓰레드가 처리할 때까지 이 과정이 반복됩니다. 호출에서 예외가 생길 시 다른 작업을 할 수 없다는 것도 문제이고요. \n\n따라서 I/O로 인해 블로킹된 스레드가 있을 경우 다른 스레드로 작업을 자동으로 할당하고 지정된 스레드풀 내에서 스레드를 재활용하며 콜백으로 완료시의 작업을 커스텀 할 수 있는 CompletableFuture를 활용하게 되었습니다. \n\nCompletableFuture는 해당 [글](https://techblog.woowahan.com/2722/)을 참고하여 학습하여 프로젝트에 적용했습니다. \n\n```java\npublic void process() {  \n    final List<Place> places = dataQueryService.findAllPlaces();  \n    final List<CompletableFuture<WeatherAndCongestion>> futures = new ArrayList<>();  \n  \n    for (final Place place : places) {  \n       futures.add(toFuture(place));  \n    }  \n  \n    final List<WeatherAndCongestion> weatherAndCongestions = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))  \n       .thenApply(Void -> futures.stream()  \n          .map(CompletableFuture::join)  \n          .filter(Objects::nonNull)  \n          .toList())  \n       .join();  \n  \n    dataQueryService.saveAllWeatherAndCongestions(weatherAndCongestions);  \n}  \n  \nprivate CompletableFuture<WeatherAndCongestion> toFuture(final Place place) {  \n    return CompletableFuture.supplyAsync(() -> fetch(place), weatherCongestionAPIThreadPool)  \n       .exceptionally(exception -> {  \n          log.debug(\"외부 API 호출 및 가져오기에 실패했습니다: 장소: {}. Error: {}. 현재 시간 : {}\", place, exception, LocalDateTime.now());  \n          return null;  \n       });  \n}  \n  \nprivate WeatherAndCongestion fetch(final Place place) {  \n    final PlaceNameAndCode placeNameAndCode = place.placeNameAndCode();  \n    final WeatherPosition weatherPosition = place.weatherPosition();  \n  \n    final Congestion congestion = congestionFetchService.fetch(placeNameAndCode.code());  \n    final Weather weather = weatherFetchService.fetch(weatherPosition.weatherX(), weatherPosition.weatherY());  \n  \n    return new WeatherAndCongestion(place.id(), weather, congestion);  \n}\n```\n\n이렇게 호출 방식을 변경한 이후 로컬에서 1분 이상 걸리던 호출이 6~7초로 걸리는 것을 확인하게 되었습니다. (timeout으로 지정한 시간과 비슷합니다.)\n\n또한 csv파일을 수동으로 업로드하지 않고 코드 기반으로 애플리케이션 시작마다 zip파일을 다운로드하고 shp파일을 파싱하여 db에 저장하는 로직을 작성했습니다.  자세한 코드는 [PR](https://github.com/jinkshower/galmanhae/pull/14) 과 [커밋](https://github.com/jinkshower/galmanhae/pull/14/commits/4b6351393ea3c3e5516df7a828322088af8e5d19)을 참고해주시면 될 것 같습니다. \n\n![[Pasted image 20240912211946.png]]\n\n'DataProcessor' 클래스와 마찬가지로 데이터 처리를 통제하는 'PlaceInfoDataProcessor'를 두고 zipfile을 다운로드하는 객체, zip파일을 파싱하고 애플리케이션에서 사용할 수 있는 객체를 주입 받아 데이터 처리 과정을 쉽게 알아 볼 수 있게 만들었습니다.\n\n## saveAll은 ForEach save이다. \n\n위의 코드에서 보시면 알겠지만 데이터 처리에서 saveAll을 사용할 일이 많습니다. 한꺼번에 처리한 데이터를 한꺼번에 저장하여 DB에 날아가는 쿼리를 줄이기 위해서죠. 하지만 Data JPA가 제공하는 saveAll()메서드는 함정이 있습니다.\n\n![[Pasted image 20240912212538.png]]\n\nsaveAll을 호출해도 하나하나씩 insert쿼리가 나가기 때문이죠. insert쿼리가 이렇게 많이 동일 테이블에 나가면 MySQL기준은 insert 쿼리 자체로 배타락을 걸기 때문에 테이블 잠금이 계속 이어지고 다른 write 요청과 겹칠시 데드락이 걸릴수도 있는 위험을 가집니다. \n\n![[Pasted image 20240912213235.png]]\n\nSimpleJpaRepository는 내부적으로 for문을 돌며 하나씩 save를 진행하기 때문에 insert쿼리를 객체 수만큼 영속성 컨텍스트에 쌓아두게 됩니다. \n\n따라서 이를 한번에 넣는 방식으로 바꾸겠습니다. \n\n```java\n@Override  \npublic void saveAll(final List<WeatherAndCongestion> weatherAndCongestions) {  \n    final List<CongestionEntity> congestionEntities = toEntities(weatherAndCongestions);  \n  \n    final String sql = \"\"\"  \n       INSERT INTO congestion (place_id, current_people, congestion_indicator, created_at, updated_at)  \n       VALUES (?, ?, ?, now(), now())  \n       \"\"\";  \n  \n    jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {  \n       @Override  \n       public void setValues(PreparedStatement ps, int i) throws SQLException {  \n          final CongestionEntity congestion = congestionEntities.get(i);  \n          ps.setLong(1, congestion.getPlaceId());  \n          ps.setInt(2, congestion.getCurrentPeople());  \n          ps.setString(3, congestion.getCongestionIndicator());  \n       }  \n  \n       @Override  \n       public int getBatchSize() {  \n          return congestionEntities.size();  \n       }  \n    });  \n}\n```\n\n저는 프로젝트에서 테이블에 Identity전략을 사용하고 있기 때문에 JPA가 제공하는 batch insert를 사용할 수 없었습니다. 따라서 JdbcTemplate가 제공하는 batchupdate를 사용하여 하나의 쿼리로 save하는 방법을 선택했습니다.\n\n## 최신의 장소목록 어떻게 유지하지?\n\n이제 다 된 것 같지만 남아 있는게 있습니다. 바로 아까 전에 설계만 설명했던 장소목록 데이터 저장(PlaceInfoDataProcessor)입니다. \n\n해당 메서드는 애플리케이션 배포마다 실행되게 되어 있었는데요.\n\n```java\npublic void process() {  \n    dataQueryService.deleteAllPlaceInfos();    \n    final InputStream fetch = placeInfoService.fetch();  \n    List<PlaceInfo> placeInfos;  \n    try {  \n       Map<String, byte[]> fileMap = dataParser.processZipFile(fetch);  \n       placeInfos = dataParser.parse(fileMap, fileName);  \n    } catch (Exception e) {  \n       throw new FailReadingFileException(e);  \n    }  \n    dataQueryService.saveAllPlaceInfos(placeInfos);  \n}\n```\n\n바로 deleteAll - 외부 API 호출- saveAll이라는 로직이 문제입니다. 해당 메서드들은 단 하나라도 실패할시 서비스에 영향을 미치게 되는데요, 사이트의 근간이 되는 데이터가 바로 이 장소 목록들이기 때문에 지금의 로직은 서비스에 큰 영향을 줄 수 있습니다.\n\ndelete가 실패한다면? 중복된 장소가 사이트에 뜨게 될 것입니다. 외부 API호출이 실패한다면 서비스에 장소가 없어지게 될 것이고 save도 마찬가지 입니다. \n\n게다가 이 메서드 전체를 하나의 트랜잭션으로 감쌀 수도 없습니다.  외부 API호출이 껴있기 때문에 트랜잭션이 얼마나 길어질지 예상할 수 없기 때문이죠. \n\n이에 따라 저는 삭제하지 않고 데이터의 version을 관리한다면 되겠다고 생각했습니다. 외부 API에서 호출이 되었다면 데이터들의 버전이 올라갈거고 나머지 비즈니스 로직은 최신의 버전만 쿼리하면 되게 말이죠.\n\n```java\npublic class PlaceEntity {\n\t//\n\tprivate int version;\n\t//\n}\n```\n\n하지만 문제가 있습니다. 장소 목록을 최신으로 유지하는 건 좋지만 쓸데없는 데이터가 쌓이는 것이 불만족스러웠습니다. 저는 딱 최신의, 100여곳의 데이터만 있길 원했기에 다른 방법이 필요했습니다.\n\n이에 방법을 찾은 것이 MySQL의 Upsert문법입니다. `Insert on duplicate key update` 인데요, 중복되는 유니크한 컬럼이 있다면 update를 하고 아니면 insert를 시행하는 문법입니다. \n\n따라서 서울시가 200여개의 장소를 추가한다고 하면 (서비스의 경사네요) 이미 있는 100여개의 장소는 update가 될 것이고 200여개의 장소만 insert되어 테이블의 데이터는 적게, 하지만 최신의 데이터는 유지될 것입니다.\n\n이에 따라 PlaceEntity의 장소 코드에 유니크 제약을 주고 \n\n```java\npublic class PlaceEntity {\n\t//\n\t@Column(length = 10, unique = true)  \n\tprivate String code;\n\t//\n}\n\nfinal String sql = \"\"\"  \n    INSERT INTO place (name, code, latitude, longitude, weatherX, weatherY, created_at, updated_at)  \n    VALUES (?, ?, ?, ?, ?, ?, now(), now())  \n    ON DUPLICATE KEY UPDATE  \n    name = VALUES(name),  \n    latitude = VALUES(latitude),  \n    longitude = VALUES(longitude),  \n    weatherX = VALUES(weatherX),  \n    weatherY = VALUES(weatherY),  \n    updated_at = now()  \n    \"\"\";\n```\n\nbatchUpdate에서 사용하는 쿼리문을 upsert문법에 맞게 바꾸어 주었습니다. \n\n## 마치며\n\n갈만해 프로젝트를 만들면서 초반부에 겪었던 데이터 처리에 관한 고민과 해결과정을 정리해봤습니다. 이후 갈만해 프로젝트의 후반부, 그리고 지금도 겪고 있는 고민과 개선 과정을 공유하도록 하겠습니다. 감사합니다!"},{"excerpt":"단일 DB 그리고 외부 API도 호출하지 않을 경우 데이터의 전달에 대해 크게 걱정할 필요가 없다. DBMS의 트랜잭션 기능을 사용하여 처리하면 되고 실패시 실패했다는 응답을 사용자에게 보내고 롤백하면 된다. 하지만 실제 웹서비스는 여러 개의 다른 DBMS를 사용하거나 다른 서비스와 네트워크로 연결되는 경우가 많다.  만약  결제 서비스에서 결제가 성공된…","fields":{"slug":"/distributed_information/"},"frontmatter":{"date":"August 05, 2024","title":"분산시스템에서 데이터를 전달하는 방법","tags":["distributedtransaction","messagebroker","unrealiablenetwork"]},"rawMarkdownBody":"\n단일 DB 그리고 외부 API도 호출하지 않을 경우 데이터의 전달에 대해 크게 걱정할 필요가 없다. DBMS의 트랜잭션 기능을 사용하여 처리하면 되고 실패시 실패했다는 응답을 사용자에게 보내고 롤백하면 된다.\n\n하지만 실제 웹서비스는 여러 개의 다른 DBMS를 사용하거나 다른 서비스와 네트워크로 연결되는 경우가 많다.  만약  결제 서비스에서 결제가 성공된 후 상품 서비스에서 재고가 감소되지 않는다면? 결제는 실패했으나 재고가 감소되었다면? 결제가 완전히 성공되었다고 보기 어려울 것이다.\n\n## 2 Phase Commit\n\n여러 DBMS를 하나의 트랜잭션으로 처리하기 위해 2PC을 적용할 수도 있다. \n\n![Pasted image 20240725133342](https://github.com/user-attachments/assets/57110df0-21e7-4189-b4e6-aeaae1f85f15)\n\n데이터베이스 1,2(3,4...)의 로컬 트랜잭션 기능을 사용하고 로컬 트랜잭션들을 관리하는 코디네이터(트랜잭션 관리자)를 둔다. \n\n각 데이터베이스들은 쓰기 요청을 평소처럼 실행하고 코디네이터는 커밋을 할 준비가 되었는지 질의한다. 모든 데이터베이스들이 준비가 되었다면 코디네이터는 2단계에서 커밋 요청을 보내고 커밋이 실제로 일어난다. 1단계에서 어느 DB하나라도 no를 보낸다면 코디네이터는 모든 DB에 Abort를 보낸다. \n\n이 방식은 여러 문제점을 가진다.\n\n1. NoSQL과 같이 트랜잭션에 참여하는 DBMS가 트랜잭션을 지원하지 않는다면 사용할 수 없다.\n2. 1단계에서 yes라 대답한 DB가 2단계에서는 커밋이 불가할 수도 있다. \n3. 코디네이터에 장애가 나면 DB사이의 일관성이 지켜지지 않을 수 있다.\n4. ACID를 위한 잠금이 길어져 성능면에서 불리하다. 3번과 연결되어 영원한 잠금이 일어날수도 있다. \n\n이외에도 이 방식은 실제 분산 시스템에서 거의 사용되지 않는다. 일반적으로 MSA 환경은 각각의 DB, WAS를 따로 두고 다른 마이크로 서비스와 API 통신을 하거나 Message 시스템을 사용한다.\n\n글로벌 트랜잭션을 관리하기 위해 코디네이터를 사용하는 것은 기껏 분리한 서비스를 다시 하나로 묶어버려 수많은 마이크로 서비스를 한 가지 시스템에 모두 의존하게 만든다.\nMSA를 하는 목적(각 서비스의 독립성, 유연성, 장애 격리)이 불분명해진다. \n\n## 네트워크의 비신뢰성\n\nMSA서비스들은 네트워크에 의존한다. 하지만 애플리케이션 로직과 달리 물리세계는 통제할 수 없다. 패킷 손실, 네트워크 지연, 네트워크 다운 등의 문제는 예측 불가능하고 개발자가 통제할 수 없다.\n\n```java\n@Transactional\npublic void save(final Member member) {\n\trepository.save(member);\n\t//외부 API 요청\n\tother.send(member);\n}\n```\n\n이러한 코드가 있다고 생각해보자. 문제가 없어보이지만 사실 네트워크 문제가 개입된다면 문제가 생길 수 있다. \n\nRDB에 save 되었으나 send에서 네트워크 문제가 생긴 경우. RDB에서 rollback이 되었으나 send에서 요청은 보낸 경우와 같이 하나로 묶여야 하지만 원자성을 지킬 수 없는 경우가 많이 일어날 수 있다. \n\n즉, 네트워크를 사용하는 분산 시스템에서 데이터는 즉각적인 일관성을 가질 수 없다.\n\n## SAGA 패턴\n\nSAGA 패턴은 분산 환경에서 데이터 일관성을 보장하기 위한 설계 패턴이다. 연속된 개별 서비스의 로컬 트랜잭션이 이어져 전체 비즈니스 트랜잭션 하나를 구성한다. 서비스1의 로컬 트랜잭션 -> 서비스2의 로컬 트랜잭션으로 트랜잭션을 순차적으로 구성하고 개별 트랜잭션이 실패했을 때는 이를 보상하는 트랜잭션을 발생시켜 '최종적 일관성'을 목표로 한다.\n\n- 보상 트랜잭션?\n\n보상 트랜잭션은 실패한 트랜잭션을 기준으로 이전의 작업들에 대해 트랜잭션이 시작되기전의 상태로 돌려서 트랜잭션의 원자성을 즉각적으로는 아니지만 최종적으로 보장하기 위한 방법을 말한다.\n\n- 보상 트랜잭션은 꼭 필요한가?\n\n결제 성공이 재고 감소, 회원 주문 목록 갱신, 배송 등록, 리뷰 알림 목록 추가.. 등등 많은 서비스에 걸쳐 순차적 트랜잭션을  형성해야 한다고 가정한다. \n\n마지막에 가서 딱 하나의 (엄청 중요하지는 않은) 서비스에서 예외가 발생했다면 앞의 모든 트랜잭션들을 롤백하는 것은 불필요한 리소스 낭비일 가능성이 크고 또한 모든 서비스에서 커밋된 데이터를 이전으로 돌려내는 구현은 굉장히 복잡할 가능성이 높다.\n\n따라서 모든 서비스의 모든 작업에 대해 보상 트랜잭션을 적용해야 한다!보다는 정말 중요하고 하나가 되어야 하는 로직에 (이체-잔고 감소 등) 사용해야 할 것 같다. \n\n(내 의견으로는 굳이 이런 중요한 로직에 복잡성을 늘리는 것보다 서비스 분리가 정말 필요하지 않다면 DBMS의 트랜잭션을 사용하는 게 속 편하지 않을까? 라는 생각이 든다.)\n\n## At Least Once를 위하여\n\nSAGA패턴은 설계 패턴일 뿐 분산 환경과 서비스의 성격에 따라 하나의 비즈니스 트랜잭션을 구성하는 방법은 모두 다를 수 있다. 다만 공통적으로 중요한 것은 로컬 트랜잭션이 성공하고 이 성공에 대한 데이터를 다른 서비스에 신뢰성 있게 전달할 수 있느냐 이다. \n\n신뢰있게 데이터를 전달한다는 건 무엇일까? 데이터를 전달 받는 시스템의 상황, 네트워크에 상관없이 적어도 한 번은 메시지가 전달되는 것을 보장하는 것을 말한다. \n\nAt Most Once는 위의 예제에서 보았던 신뢰성 없는 데이터 전달을 말한다. 즉 API호출, Message 발행등을 딱 한번하고 이 후 상황은 신경쓰지 않는다. \n\nAt Least Once는 최소 한 번 전달, 즉 A -> B가 있을 때 A는 최소 한 번이상 발송해야 하고 B는 이를 최소 한 번이상 수신해야 한다. 여기서 중요한 것은 B가 한번 이상 수신해야 하므로 멱등성(여러 번 요청해도 결과가 같음)이 보장되어야 하는 것이다.\n\n### 트랜잭션 Outbox 패턴\n\n트랜잭션 Outbox패턴은 외부 API 호출을 이벤트로 보고 해당 이벤트를 RDBMS에 같이 저장하는 방법을 말한다. \n\n따라서 이벤트+데이터가 DBMS의 하나의 트랜잭션으로 묶이게 되어 send와 save는 원자성을 가지게 된다.\n\n이제 적재된 이벤트 데이터를 Polling으로 처리하는 쓰레드나 시스템을 두어 해당 이벤트를 발행하면 된다. 이벤트 레코드에 상태를 명시하는 컬럼을 두어 commit된 데이터는 처리되게 할 수 있다.\n\n이 때 중요한 것은 이 발행도 당연히 네트워크를 사용해야 하니 발행을 처리하는 시스템은 멱등하게 구성되어야 한다. \n\n### 메시지 브로커 사용\n\nRabbitMQ, Kafka와 같은 메시지 브로커 시스템을 사용한다. Pub-Sub의 기본모델은 Pub이 이벤트 발행하면 Message Queue에 적재, Sub이 이 적재된 메시지를 처리하는 방식이다. \n\n이 때 아무 장치 없이 구현한다면 Pub의 이벤트 발행, Queue의 적재, Sub의 처리를 모두 보장할 수 없다. Pub이 이벤트를 발행해도 네트워크 때문에 실패할 수 있고 Queue가 꽉차서 메시지는 유실될 수 있고 Sub이 받은 메세지를 제대로 처리하지 못할 수도 있다.\n\nRabbitMQ와 Kafka는 pub-sub 모델에서 신뢰성 있는 메시지 전달을 도와주는 프레임워크들이다. \n\nRabbitMQ는 Exchanger를 Message Queue내에 두고 Publisher에게는 메시지를 큐에 잘 적재했다는 Publisher Confirm을 쏘고 , Consuer로부터는 메시지를 잘 받았다는 Consumer Ack을 확인하고 큐에서 메시지를 뺀다. 또한 Dead letter큐를 따로 두어 정상적으로 처리하지 못한 메시지를 따로 관리한다.\n\nKafka도 발행자에게 ack와 같이 잘 메세지를 기록했다는 신호를 보내고 소비자가 메시지를 처리한 만큼의 오프셋을 커밋하는 방식으로 메시지의 전달을 보장한다.\n\n### 실제로는 어떻게 ?\n\n![Pasted image 20240725163559](https://github.com/user-attachments/assets/219622c4-03e7-4b87-98b6-4e582bec271d)\n(초록색은 스프링 이벤트, 회색은 AWS SNS)\n우아한 형제들의 회원시스템 이벤트기반 아키텍처 구축하기 [글](https://techblog.woowahan.com/7835/) 에서 일부분이다. \n\nSpringEvent 발행 이후 SNS에 메세지를 발행하는 과정에서 네트워크 통신이 있어 이벤트 발행을 보장하기 위해 트랜잭션 Outbox 패턴을 적용하여 회원에 대한 이벤트를 RDB에 같이 적재하여 다른 서비스로 발행하는 모습을 볼 수 있다. \n\n즉, 분산환경에서 비즈니스 트랜잭션을 처리하는 방법에는 여러가지가 있을 수 있고 중요한 것은 A가 B에 데이터를 네트워크의 비확실성에도 불구하고 신뢰있게 전달할 수 있느냐 인 것을 알 수 있다. \n\nhttps://monday9pm.com/%EB%B6%84%EC%82%B0-%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98-distributed-transaction-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-d0a10ad5dd53\n\nhttps://tech.kakaopay.com/post/msa-transaction/\n\nhttps://www.youtube.com/watch?v=b65zIH7sDug&t=1079s\n\nhttps://techblog.woowahan.com/7835/"},{"excerpt":"인증과 인가 인증 : 사용자의 신원을 확인하는 것\n인가 : 신원이 확인된 사용자에게 접근 권한을 부여하는 것. Http는 비연결성, 무상태성을 가진다. 이전 요청을 기억하지 못하기 때문에 한 번 인증, 인가가 되었다고 해서 사용자의 세션(사용자의 이전 상태를 기억하는 지속된 연결 경험)이 마법처럼 유지되지 못한다. 이를 위해 다양한 방식으로 세션을 유지하…","fields":{"slug":"/authentication/"},"frontmatter":{"date":"July 20, 2024","title":"인증과 인가","tags":["session","token","cookie"]},"rawMarkdownBody":"\n## 인증과 인가\n\n인증 : 사용자의 신원을 확인하는 것\n인가 : 신원이 확인된 사용자에게 접근 권한을 부여하는 것.\n\nHttp는 비연결성, 무상태성을 가진다. 이전 요청을 기억하지 못하기 때문에 한 번 인증, 인가가 되었다고 해서 사용자의 세션(사용자의 이전 상태를 기억하는 지속된 연결 경험)이 마법처럼 유지되지 못한다.\n\n이를 위해 다양한 방식으로 세션을 유지하는 방법들이 있다.\n\n## 세션 유지 방식\n\n- url\n\n`hiyen.com/홍길동/mypage...` 와 같이 url을 기준으로 유저에게 맞는 웹 페이지를 제공하는 방식이다.  당연하게도 url만 알면 모두가 유저 개인페이지에 접근할 수 있고 url기반이기 때문에 정보를 가로채는 것도 아주 쉽다.\n\nurl은 브라우저 히스토리, 북마크 등에 저장되어 쉽게 노출되며 referer header등에 남아 서버 간 쉽게 공유되기 때문에 거의 사용되지 않는다.\n\n- 쿠키\n\n홍길동이 id와 password로 로그인을 하면 사용자가 인증되었다는 정보 서버에서 쿠키로 만들어 응답한다. 브라우저는 해당 쿠키를 이후 요청마다 http요청에 포함하고 서버에서는 쿠키를 보고 개인에 맞는 서비스를 실행한다.\n\n쿠키는 클라이언트 쪽에 저장되기 때문에 탈취, 조작 당하기 쉽고,  이를 당해도 서버에서 이를 구분할 방법이 없어 보안상 문제가 생길 수 있다.\n\n예를 들어 크롬에서 관리자 도구만 켜도 웹 사이트에서 받은 쿠키를 볼 수 있다. 쿠키의 값이 11이라면, 이를 12로만 바꾸어도 다른 사용자의 정보를 볼 수 있을 지도 모른다. 따라서 쿠키를 인증과 인가에서 사용한다면 예측할 수 없는 암호화, 주요 정보는 쿠키에 저장하지 않아야 한다.\n\n- 헤더\n\n헤더에 사용자의 개인정보를 넣는 방식은 쿠키보다 안전하다.\n\n헤더는 기본적으로 클라이언트 자바 스크립트에서 접근할 수 없고(쿠키는 documents.cookies로 바로 접근 가능) 명시적으로 헤더를 사용할 때만 포함할 수 있기 때문에 모든 요청에 자동으로 포함되는 쿠키보다 노출되는 위험이 줄어든다.\n\n사용자의 세션을 유지하기 위해서는 위의 세가지 방법 중 하나가 필요하다.\n\n## 세션 유지 방식의 활용\n\n- 서버내에서 세션 관리 + 쿠키 or 헤더에 식별자를 주기\n\n서버가 세션을 관리한다. 상태를 유지한다고 말한다. 클라이언트는 세션 식별자를 쿠키나 헤더에 포함시켜 서버에 요청을 보낸다.\n\n1. 로그인 시 서버는 세션에 관련된 정보를 저장(대개 DB, 세션 스토리지라 불림)하고 이에 대한 식별자(대개 세션ID)를 쿠키나 헤더에 넣어 클라이언트에 응답한다.\n2. 클라이언트는 이후 요청마다 이 세션ID를 쿠키or 헤더에 포함하여 보낸다.\n3. 서버는 세션ID를 통해 세션 스토리지를 조회하고 유효성을 확인, 사용자에 맞는 응답을 제공한다.\n\n이 방식은 사용자의 개인정보를 서버 내에서 관리하기 때문에 상대적으로 보안이 취약한 클라이언트에 저장할 필요가 없어 보안상 이점이 있다.\n\n또한 세션ID가 갈취당하거나 조작된 경우 이에 대해 서버 내에서 해당 세션을 무효화하는 것이 쉽다.\n\n이 방식에서 주의할 것은 클라이언트로 보내지는 세션 ID가 암호화되어야 한다는 점이고 거의 모든 요청에서 세션 스토리지 조회가 일어나 성능에 영향이 간다는 점, 또한 서버가 수평확장을 할 경우 세션 불일치 문제가 생길 수 있다는 점이다.\n\n- 서버는 stateless + 토큰 사용\n\n서버는 상태를 유지하지 않고 클라이언트가 요청마다 인증 정보를 포함시켜 서버에 보내는 형식이다. 주로 JWT(Json Web Token)가 사용된다.\n\n1. 로그인 시 서버는 토큰을 생성해 클라이언트에 응답한다.\n2. 이후 요청마다 클라이언트는 토큰을 헤더에 포함하여 서버에 보낸다.\n3. 서버는 토큰을 검증하고 유효한 토큰이면 요청을 처리한다.\n\n일견 보면 위의 방식과 별 차이가 없어 보이지만 서버에서 세션 스토리지를 운영, 조회하는 과정을 하지 않는다는 차이점이 있다.\n\nJWT를 예로 들자면 JWT는 헤더,페이로드, 시그니처로 이루어져 있는데 헤더와 시그니처는 토큰의 무결성을 보장하고 페이로드는 Claims이라고 불리는 json 형식의 데이터를 지원하여 해당 부분에 세션 유지를 위한 사용자의 개인정보를 넣을 수 있다.\n\n이 방식은 서버가 상태를 유지하지 않으므로 수평으로 확장하는데 아무런 문제가 없다. 또한 세션 스토리지를 운영할 필요가 없으므로 시스템 설계의 복잡도가 줄어들 수 있다. (토큰 발급, 유효성 검사 로직만 각 서버에 있으면 됨)\n\n단점은 장점과 같이 stateless하다는 점, 즉, 서버가 토큰에 대한 제어권이 없다는 점이다. 토큰은 만료될 때까지 유효하기 때문에 이를 갈취당해도 서버 내에서 무효화할 방법이 없다.\n\n이를 방지하기 위해서 만료시간을 짧게 하면 사용자의 잦은 재로그인이 필요하고 만료시간이 길면 보안에 취약해 진다.\n\n- Access Token + Refresh Token\n\n토큰 방식의 문제를 해결하기 위해 많이 사용하고 있는 방식이다. Access Token + Refresh Token으로 토큰을 두 개로 운영하며 Access Token은 만료 시간을 짧게 설정하여 탈취 당해도 피해를 최소화할 수 있게하고 Refresh Token은 Access Token과 함께 발행하는 비교적 만료 시간이 긴 토큰으로 이 토큰을 가지고 있다면 새로운 Access Token을 재발급 받을 수 있게 한다.\n\n하지만 Refresh Token 자체가 탈취당할 가능성도 있다. 즉, stateless하다는 토큰의 장점을 계속 가지고 싶다면 보안을 일정 수준 포기해야 한다.\n\n이에 따라 토큰 방식을 사용하되 토큰에 대한 관리를 서버에서 하는 방법이 사용되기도 한다. 토큰 블랙리스트를 운영하는 방법, Refresh Token을 발급 할때마다 이전 Refresh 토큰을 무효화 리스트에 추가하는 방법등이 있다. 이러한 방법들은 전부 서버내의 스토리지를 요구하기 때문에 수평적 확장을 할 시 고려해야 한다.\n\n## Oauth\n\n사용자의 다른 웹사이트에 있는 정보에 대해 접근 권한을 부여해 주는 프로토콜.\n\n구글캘린더의 일정을 받아 온다고 치자. 사용자는 구글 id 패스워드를 우리의 hiyen닷컴에 제공하는 것을 신뢰할 수 없다. 우리 서버도 다른 서비스의 id 패스워드를 관리하는 것이 부담스럽다.\n\n따라서 프로토콜을 통해 사용자는 구글에 인증을 하고 사용자의 구글 정보에 대한 접근 권한을 히엔닷컴에 주는 방식이다 우리의 서버는 이제 이 접근 권한으로 사용자의 구글 캘린더를 구글에서 받아올 수 있다.\n\n용어 정리\n\n1. Resource Owner(사용자) - 자원에 접근 권한을 가지고 있는 사용자. 홍길동\n2. Clinet(클라이언트) - 홍길동 대신 구글 캘린더에 접근하려는 서버. 히앤닷컴\n3. Authorization Server(인증 서버) - 클라이언트를 인증, 인가 코드 발급, 액세스 토큰 발급\n4. Resource Server(자원 서버) - 홍길동의 구글 캘린더를 가지고 있는 서버.\n\nOauth과정\n\n1. 홍길동이 로그인을 요청한다.\n2. 히앤닷컴은 구글에 미리 등록했던 Client Id, Redirect URI, Scope를 가지고 인증서버에 사용자가 로그인요청을 했음을 알린다\n3. 인증 서버는 홍길동에게 로그인 페이지를 제공한다.\n4. 홍길동이 id, password로 로그인을 한다.\n5. 인증서버는 홍길동에게 인증 코드(Authorization code)를 준다\n6. 홍길동은 Redirect URI로 리다이렉트 된다 이때 인증코드가 같이 히앤닷컴에 전달된다.\n7. 히앤닷컴은 이제 인증코드로 인증서버에 접근 권한(여기서는 Access Token)을 요구한다\n8. 인증서버는 access token을 히앤닷컴에 준다\n9. 히앤닷컴은 이 access token을 어딘가에 저장한다\n10. 홍길동이 구글 캘린더를 사용하는 히앤닷컴 서비스를 요청한다.\n11. 히앤닷컴은 이제 자원서버에 토큰을 가지고 요청을 보낸다\n12. 자원서버는 토큰을 확인하고 자원을 준다.\n13. 히앤닷컴은 이제 사용자에게 구글 캘린더를 사용하는 서비스를 제공할 수 있다!!\n"},{"excerpt":"자바의 GC c,c++ 처럼 프로그래머가 직접 메모리를 관리하지 않는다는 것은 프로그래머에게 큰 장점이었음. c에서 malloc()을 한 메모리는 free()로 메모리 release가 되어야 함. 그렇지 않으면 메모리 누수(동적으로 할당한 메모리를 해제하지 않아 사용 가능 메모리가 줄어드는 현상)가 생김. Human Error를 일일히 잡았어야 함. 자바…","fields":{"slug":"/garbage_collection/"},"frontmatter":{"date":"July 13, 2024","title":"GC와 자바의 변화과정","tags":["java","jvm","garbagecollector"]},"rawMarkdownBody":"\n## 자바의 GC\n\nc,c++ 처럼 프로그래머가 직접 메모리를 관리하지 않는다는 것은 프로그래머에게 큰 장점이었음. c에서 malloc()을 한 메모리는 free()로 메모리 release가 되어야 함. 그렇지 않으면 메모리 누수(동적으로 할당한 메모리를 해제하지 않아 사용 가능 메모리가 줄어드는 현상)가 생김. Human Error를 일일히 잡았어야 함.\n\n자바의 JVM은 Execution Engine내에 Heap영역의 메모리를 관리하는 Garbage Collector를 둠. 또한 프로그래머에게 메모리 주소에 대한 접근 방식을 숨김으로써 메모리 관리를 JVM의 내부 구현으로 숨겨둠. JDK 5로 쓰여진 코드가 Migration을 통해 JDK 8의 GC방식으로 메모리를 관리 할 수 있는 이유임.\n\n이는 단점과도 연관 됨. 메모리 관리가 JVM의 구현에 기대기 때문에 메모리 관리에 대한 정보를 직접적으로 얻기 어려움. C는 코드 레벨에서 메모리 이슈를 추적할 수 있지만 자바는 이에 대한 정보를 얻으려면 각종 툴과 JVM이 지원하는 기능을 이용해야 한다.\n\n또한 메모리의 해제 타이밍을 개발자가 명확히 알기 힘들며 오히려 메모리 누수 문제가 되는 상황과 원인을 추적하기 어렵게 만들기도 한다.\n\n### GC살펴보기\n\nGC알고리즘은 기본적으로 Weak Generational Thesis를 기반으로 이루어져 있다. 이는생성된 객체 중의 대다수는 짧은 시간내에 접근 불가능 상태(unreachable)이 되고 오래된 객체에서 젊은 객체로의 참조는 아주 적게 존재한다는 이론이다.\n\n이 이론은 JVM의 Heap을 영역으로 나누어 관리하는 근거가 되는데 모든 객체를 하나의 공간에서 관리하지 않고 짧은 시간 내에 쓰이고 해제되어야 할 많은 객체를 위한 공간과 오래 살아 남아야 할 객체를 위한 공간을 분리하여 각각의 탐색 범위를 줄이고 각기 다른 해제 로직을 적용하기 쉽기 때문이다.\n\n- 왜 Heap 영역만 관리하는가?\n\nGC는 Heap 영역에서 메모리를 해제 하지만 GC가 관여하는 영역은 JVM의 Runtime Data Area 전체이다. GC는 접근 불가능 상태를 판별하기 위해 객체의 Root를 탐색하는데 이에 Root가 되는 영역이 Method와 Stack 영역이다.\n\nMethod 영역은 클래스에 대한 메타데이터와 정적인 변수, 상수의 영역으로 애플리케이션 실행동안 고정되므로 JVM은 이 영역에 있는 데이터를 해제하지 않는다.\n\nStack 영역에서 쓰이는 모든 메모리는 스택 프레임에 놓이게 되고 이는 메서드 종료와 함께 소멸되어 사용 중인 메모리로 인식하지 않기 때문에 GC의 관여가 필요하지 않다.\n\n- minor GC? major GC?\n\n객체를 구분하여 객체가 더 이상 접근 가능하지 않음을 확인하고 해당 객체에 할당된 메모리를 Garbage Collector가 해제하는 작업을 Garbage Collecion이라고 한다.\n\n이 collection이 어디에서 일어나는지에 따라 이를 minor(young) , major(old)로 나눈다. Garbage Collection은 객체가 접근 가능한지를 보기 위해 Root부터 탐색하여 객체의 접근 상태를 표시하고 접근 불가능하다면 메모리를 해제하는 작업이다.\n\n- Stop The world\n\nGC에 대해 공부하면 언제나 STW라는 키워드를 들을 수 있다. minor GC든, major GC든 모든 메모리 해제는 메모리 해제에 관여하는 쓰레드(들)을 제외하고 실행을 멈춘다.\n\n위에서 설명한 접근 상태 탐색과 연관하면 이유는 명확하다. 다른 쓰레드들의 실행이 멈추지 않으면 객체의 접근 상태와 참조가 변할 수 있기 때문이다. 이러면 탐색이 무의미해지고 해제되지 않아야 할 메모리가 해제될 수 있다.\n\n- Serial GC\n\n현재 다양한 GC 알고리즘이 있지만 근간이 되는 알고리즘은 JDK5버전에서 기본으로 사용하던 Serial GC다.\n\n크게 힙 영역을 Young, Old로 나뉜다. Perm영역을 이 분류에 포함시키는 글도 많은데 Perm은 Non-Heap이다. [참고](https://stackoverflow.com/questions/41358895/permgen-is-part-of-heap-or-not)(이 영역에도 GC가 발생할 수 있는데 JDK8 이후로 이 영역이 MetaSpace로 바뀐 이유와 관련 있다. 이후에 설명한다)\n\nYoung 영역을 다시 Eden, Survior0, Survior1로 나누어 관리 한다. Eden 영역에 최초로 객체가 만들어지고 eden영역이 다 차면 minor gc가 발생하고 살아남은 객체를 Survior0영역으로 옮긴다. 모든 객체들은 객체 헤더에 자신이 얼마나 많은 minor gc를 거쳐 살아남았는지가 명시 되어 있다. (GC counter, Age라고 한다.)\n\n다시 eden이 차면 minor gc가 일어나고 이 때 Survior0에 있던 객체에 대한 minor GC도 같이 일어난다. 또 살아 남았다면 Survior1으로 복사되며 eden에서 살아남은 객체가 Survior1으로 같이 이동한다. Survior0을 비우는데 이는 살아남은 객체를 메모리 단편화 없이 관리하기 위해서다.\n\n이를 반복하며 살아남은 객체가 일정 수준 이상의 GC counter를 넘게 되면 Old영역으로 옮겨지고 이 Old영역의 메모리가 임계점을 넘어서면 이 영역에서 GC가 발생한다. Old영역은 Young보다 크기 때문에 GC에 걸리는 시간이 오래 걸린다.\n\nSerial GC의 Major GC알고리즘은 Mark-Sweep-Compaction을 거친다. 접근 불가능한 객체를 Mark 하고 메모리를 일괄 해제, Sweep하고 단편화된 메모리를 압축하는 과정으로 이루어진다.\n\n- Parallel GC, Parallel Old GC, CMS\n\n이 후 등장한 GC 알고리즘들은 Serial GC를 기반으로 어떻게 STW의 시간을 줄이는가에 초점을 맞추었다.\n\nParallel GC는 GC를 멀티쓰레드로 수행하여 STW 시간을 줄였고 Parallel Old GC는 Old 영역의 major GC의 알고리즘을 Mark-Summary-Compaction 방식으로 바꾸었다. Summary에서 미리 이동할 공간을 계산하고 Compaction에서 살아 있는 객체를 이동시키며 메모리를 해제 했다.\n\nCMS GC는 조금 다른 알고리즘을 가지고 있었는데 살아 있는 객체를 Mark하는 과정을 3단계로 나누어 initial, concurrent, remark로 나누었다. 세 단계로 살아 있는 객체를 마킹하고 Concurrent Sweep 단계에서 메모리를 해제하는데 이 과정이 STW로 일어나지 않고 메모리 compaction을 하지 않는다.\n\nCMS GC는 메모리 Compaction을 하지 않기 때문에 충분한 연속된 메모리를 확보하지 못하면 Concurrent Mode Failure가 발생하는 것을 트리거로 Mark-Sweep-Compact의 Major GC를 수행하고 이 경우 긴 STW가 발생하여 애플리케이션이 길게 멈추는 현상이 CMS GC의 큰 문제점으로 꼽혔다.\n\n- G1GC\n\nJDK 9부터 기본 GC가 된 알고리즘이다. 크게 Young, Old로 Heap을 이분화하지 않고 여러개의 Region의 격자판 형식으로 나눈다. eden - survior - old로의 aging 과정은 동일하지만 Heap이 동일한 크기를 가지는 여러개의 Region으로 나누어져 있고 Region마다 다른 역할을 부여할 수 있다.\n\n예상 가능한 지연시간을 제공한다. 일정 Region에 GC를 하는 사이클을 돌며 계속 GC를 수행하는 방식으로 Young, Old 영역을 전체 관리해야 하던 이전의 GC알고리즘보다 더 효율적으로 메모리를 관리한다.\n\nYoung only라는 minor GC를 계속 수행하며 Old 영역이 일정 한계에 다다르면 Concurrent Phase에 들어가며 STW를 하지 않으면서 Old영역의 살아 있는 객체와 해제해야할 객체를 계산한다. 이 Phase가 끝나면 Space Reclamation 사이클로 minor GC + majorGC를 같이 수행한다.\n\n- ZGC\n\n모든 GC과정이 concurrent하게 진행되어 아주 낮은 지연시간을 가진다.\n\n- 어떤 GC를 선택해야 하는가?\n\n지연시간이 크게 의미가 없는 애플리케이션이라면 (대규모 배치 처리, 백그라운드 작업 등)높은 처리량을 가진 Parallel GC가 더 좋은 선택일수도 있다. G1GC는 비교적 복잡하고 다양한 단계로 GC를 수행하는데 이 과정들이 CPU의 연산을 요구하며 Concurrent Phase처럼 애플리케이션 쓰레드들과 동시에 실행되는 과정도 있어 Parallel GC보다 더 낮은 처리량을 가진다.\n\nZGC는 아주 높은 힙사이즈와 최소한의 지연시간을 보장해야 할 때 적절하며 Serial GC는 힙사이즈가 적고 시스템리소스가 아주 작을 때 (복잡한 GC 수행에 쓰일 CPU연산도 아까울 때) 적절하다.\n\n## 자바에 어떤 변화가 있었나\n\nJava7부터 주요한 변화를 정리한다.\n\n### Java 7\n\n- 다이아몬드 연산자 도입\n\n제네릭 타입 추론을 단순화한다. 컴파일러가 자동으로 타입을 추론해주게 되었다.\n\n```java\n//Before\nArrayList<Integer> arr = new ArrayList<Integer>();\n\n//After\nArrayList<Integer> arr = new ArrayList<>();\n```\n\n- Fork/Join Pool\n\n재귀적인 작업 분할을 통해 병렬 처리를 효율적으로 수행한다. 큰 작업을 작은 작업으로 분할(Fork)하고 결과를 결합(Join)하는 방식이다. 작업 분할 시Work Stealing을 통해 각 스레드가 다른 스레드의 작업큐에서 작업을 가져와 처리한다.\n\n- nio 패키지 개선\n\njava.nio.file 패키지가 추가되었다. 파일과 디렉터리 조작을 위한 새로운 api를 제공하고 심볼릭 링크, 파일 속성, 파일 시스템 탐색 등 java.io에서 지원하지 않던 기능들을 지원하게 되었다.\n\n- Try-with resource\n\n리소스의 자동해제를 간편하게 할 수 있게 해준다. finally에서 명시적으로 해제할 필요가 없어졌다.\n\n### Java 8 (LTS)\n\n자바에서 가장 큰 변혁을 이루어낸 버전으로 손꼽힌다.\n\n- 람다\n\n함수형 인터페이스(추상메서드 하나만 존재하는 인터페이스)의 구현 클래스를 쉽게 생성하기 위해 등장했다. 이전에는 이러한 인터페이스의 익명 클래스를 만들어야 사용 가능했다.\n\n```java\ninterface Calculate {\n\tint operation(int a, int b);\n}\n\n//Before\nprivate void caculateClassic() {\n\tCalculate caculateAdd = new Calculate() {\n\t\t@Override\n\t\tpublic int operation(int a, int b) {\n\t\t\treturn a + b;\n\t\t}\n\t}\n\tcaculateAdd.operation(1, 2);\n}\n\n//After\nprivate void caculateLambda() {\n\tCaculate calculateAdd = (a, b) -> a + b;\n\tcaculateAdd.operation(1, 2);\n}\n```\n\njava.util.function 패키지에 반복적으로 사용되는 람다식을 위한 표준 함수형 인터페이스가 구현되어 있다.\n\n람다는 익명 클래스를 만들어내기 때문에(정확히 말하자면 invokedynamic 바이트 코드 명령어로 동적으로 생성된다) 람다로 만들어지는 객체는 Heap에 저장된다. 람다식만 보면 스택에 저장될 것 처럼 생겼지만 아니다.\n\n따라서 람다식 외부의 매개변수를 람다 내에서 사용할 시 값을 복사해서 사용하고 이때 final이거나 사실상 final이어야 한다. 즉 불변해야 한다.\b\n\n이 코드는 컴파일되지만\n```java\nint num = 1;  \nString c = \"asdf\";  \nFunction<Integer, String> function = s -> num + c;  \n```\n\n이 코드는 컴파일 되지 않는다.\n```java\nint num = 1;  \nString c = \"asdf\";  \nFunction<Integer, String> function = s -> num + c;//num 불가  \nnum = 3;  \n```\n\n첫번째 코드는 num이 초기화 된 후 변화가 없기 때문에 사실상 final로 간주된다. 따라서 람다식이 성립하지만 두번째는 num에 3을 할당함으로써 num이 가변적임을 알리게 되었다. 따라서 람다식이 성립 불가능하다.\n\n좀 더 자세히 설명하자면\nnum은 스택영역에 저장된다. function은 힙에 람다로 생성된 객체를 가리키고 있다. 이 때 람다 표현식 안의 num은 Heap의 객체 내에 생성되므로 메서드 호출이 끝나고 사라지는 스택-num과 달리 호출이 끝난 후에도 존재할 수 있다. (GC가 일어나야 없어진다)\n\n람다는 이처럼 사라지는 외부의 지역변수를 내부에서 사용하기 위해 지역변수를 복사해서 사용한다. 쓰레드끼리는 스택영역을 공유하지 않기 때문에 지역변수가 가변적이라면 람다는 예측할 수 없는 결과를 낸다. 따라서 final or effective final의 외부 지역변수가 람다 안에서 사용되어야 한다.\n\n- 스트림\n\n데이터 처리를 함수형 스타일로 선언하여 수행할 수 있게 하였다. 생성된 스트림을 중개 연산을 여러개 수행하여 종단연산을 한번 시행한다. 중개 연산은 지연연산으로 이루어져 있으며 종단 연산이 시행될때 까지 실행되지 않는다. parallelstream()으로 병렬 연산을 지원한다.\n\n자바 8에서 각종 Collection에 Stream을 반환하는 메서드가 추가되면서 같은 연산을 재사용할 수 있게 되었다.\n\n- Metaspace영역\n\nJava 8 이전에 클래스 로더가 로딩한 바이트 코드, 클래스의 메타데이터, Runtime Constant Pool, static한 데이터가 저장되던 공간은 Method or Perm Gen이라 지칭했다.\n\nJava 8 부터 PermGen영역이 사라지고 대신 Metaspace영역으로 대체되었다. PermGen은 고정된 메모리 크기로 인해 OOM이 자주 발생하는 영역이었다. Metaspace영역은 네이티브 메모리를 사용하며 동적으로 크기를 조절할 수 있게 되었다.\n\nGC는 사용되지 않는 클래스를 식별하고, 이를 Metaspace에서 언로딩하여 메모리를 회수한다.\n\n- 새로운 날짜 시간 API\n\nDate, SimpleDateFormatter는 Thread-Safe하지 않았고 구성이 복잡했다. 이에 따라 java.time이라는 새로운 패키지가 등장했다.\n\n- Optional의 추가\n\nnull을 간편하게 처리하기 위해 등장했다.\n\n- 인터페이스 default 메서드\n\n인터페이스에 새로운 기능을 해당 인터페이스를 구현하는 클래스들이 Override할 필요 없이 추가하기 위해 등장했다.\n\n### Java 9\n\n- Java Platform Module System\n\n패키지의 집합을 하나의 모듈로 구성하고 여러개의 모듈로 자바 애플리케이션을 구성할 수 있게 되었다. 클래스와 패키지에 대한 접근을 모듈로 직접 관리할 수 있기 때문에 의존성을 직접 관리할 수 있게 되었다. 모듈 선언 파일(`module-info.java`)을 통해 의존 모듈(`requires`)과 공개할 패키지(`exports`)를 명시한다.\n\n- 인터페이스 private 메서드 추가\n\n인터페이스의 default, static 메서드의 공통되는 중복 기능을 추출할 수 있게 되었다.\n\n- String 클래스 char[] -> byte[] 로 변경\n\n기본 값이 1byte로 변경되었고 문자열 내에 UTF-16값이 포함되면 coder의 값을 변경하여 2byte를 책정하는 식으로 변경되었다.\n\n해당 기능은 Compact String으로 불린다.\n\n- Publish - Subscribe 프레임 워크\n\npulling 방식의 Pub-Sub 프레임워크가 추가되었다. Publisher에 Subscriber가 구독을 하는 방식이며 Publisher가 발행하는 메시지를 Subscriber가 처리하는 방식이다. Processor를 중간에 둘 수 있으며 중간 연산이 필요한 경우 Publisher - Processor - Subscriber 형식으로 사용할 수 있다.\n\n### Java 10\n\n- 지역 변수 타입 자동 추론 키워드 var\n\n지역 변수의 타입을 자동으로 추론해준다.\n```java\n//Before\nString hello = \"hello\";\n//After\nvar hello = \"hello\";\n```\n\n다이아몬드 연산자를 사용할 시 문제가 발생할 수 있으니 우측항이 명시적인지를 확인해 봐야 한다.\n```java\nvar list = new ArrayList<>();  //모든 타입 다들어감.\nlist.add(\"hello\");  \nlist.add(1); //컴파일 된다.\n```\n\n- Unmodifiable Collection\n\n불변 컬렉션을 쉽게 생성할 수 있는 헬퍼 메서드들이 추가되었다. List.of(), Map.of()로 static 메서드를 사용하면 되고 이 컬렉션의 아이템을 수정하려고 하면 예외가 발생한다.\n\n```java\nList<Integer> list = List.of(1, 2, 3);  \nlist.set(0, 10); //예외 발생\n```\n\n![Pasted image 20240711115412](https://github.com/jinkshower/learned/assets/135244018/5abfaa5e-b1cb-4572-a322-a76dbf6a0cc9)\n\n### Java 11 (LTS)\n\n- 컴파일 없이 자바 실행 가능\n\njavac컴파일을 할 필요 없이. 단일 파일 스크립트 작성 및 실행이 가능해져, 간단한 테스트나 스크립트 작업이 편리해졌다. 예를 들어, `java HelloWorld.java` 명령어를 통해 `HelloWorld` 클래스를 직접 실행할 수 있게 되었다.\n\n- 람다식 내에서 var 사용가능\n\n이를 통해 람다 표현식을 더 간결하고 명확하게 작성할 수 있다. 예를 들어, `(var x, var y) -> x + y`와 같이 사용할 수 있다.\n\n### Java 12~17\n\n12에서 17(LTS)까지 주요 변화 기능을 다룬다.\n\n- Record (14~)\n\n불변 클래스를 쉽고 간단하게 사용할 수 있게 되었다. getter, hashcode, equals, tostring을 자동으로 생성해준다.\n\n```java\n//Before\npublic final class Person {  \n    private final String name;  \n    private final int age;  \n\n    public Person(String name, int age) {  \n       this.name = name;  \n       this.age = age;  \n    }  \n\n    public String getName() {  \n       return name;  \n    }  \n\n    public int getAge() {  \n       return age;  \n    }  \n\n    @Override  \n    public boolean equals(Object o) {  \n       if (this == o) return true;  \n       if (o == null || getClass() != o.getClass()) return false;  \n       Person person = (Person) o;  \n       return age == person.age && name.equals(person.name);  \n    }  \n\n    @Override  \n    public int hashCode() {  \n       return Objects.hash(name, age);  \n    }  \n\n    @Override  \n    public String toString() {  \n       return \"Person{\" +  \n          \"name='\" + name + '\\'' +  \n          \", age=\" + age +  \n          '}';  \n    }  \n}\n//After\npublic record Person(String name, int age) {  \n}\n```\n\n- NPE 메세지 개선\n\n```java\n//Before\nException in thread \"main\" java.lang.NullPointerException\n//After\n|Exception in thread \"main\" java.lang.NullPointerException: Cannot invoke \"String.length()\" because \"person.name\" is null\n```\n\n어느 메서드에서 왜 NPE가 일어났는지 상세하게 보여준다.\n\n- Sealed 클래스 지원\n\n상속을 제한하기 위해 Sealed Class를 지원한다. permits를 통해 상속받을 수 있는 클래스를 정할 수 있다. \n\n- Text Block 지원(13~)\n\n긴 String을 따옴표 3개로 만들 수 있게 지원한다. 개행과 +연산을 사용할 필요가 없이 자동으로 생성해준다.\n\n```java\n//Before\nString json = \"{\\n\" +  \n    \"  \\\"name\\\": \\\"John\\\",\\n\" +  \n    \"  \\\"age\\\": 30,\\n\" +  \n    \"  \\\"city\\\": \\\"New York\\\"\\n\" +  \n    \"}\";\n\n//After\nString json = \"\"\"\n              {\n                \"name\": \"John\",\n                \"age\": 30,\n                \"city\": \"New York\"\n              }\n              \"\"\";\n```\n\n### Java 21\n\n- Virtual Thread\n\nOS 스레드와 1대1로 매핑되던 기존 자바 쓰레드를 개선했다. i/o작업으로 대기만 해야하던 쓰레드를 Carrier Thread로 바꾸고 Carrier Thread가 다수의 더 가벼운 Virtual Thread를 가지게 하여 i/o작업을 대기하며 쉬는 동안 다른 Virtual Thread의 작업을 할 수 있게 하였다.\n\nVirtual Thread는 jvm 내부에서 스케쥴링을 실행하기 때문에 시스템콜이 필요없어 컨텍스트 스위칭 비용이 적어 다수의 작업을 병렬 처리하기에 좋다. 애플리케이션이 cpu burst thread 위주인지, io burst thread 위주인지에 따라 성능이 결정된다. (cpu burst인 경우 switching이 필요치 않아 오히려 생성 및 스케쥴링 비용이 더 들 수 있다.)"},{"excerpt":"데이터베이스 데이터베이스는 컴퓨터 시스템에 저장된 구조화된 정보의 집합을 의미합니다. 데이터베이스는 대개 DBMS(Database Management System)이라는 소프트웨어에 의해 관리되며 이 둘을 합쳐 일반적으로 데이터베이스라고 부릅니다. 현재 가장 많이 쓰이는 데이터베이스는 RDBMS(Relational Database Management Sy…","fields":{"slug":"/rdbms_nosql/"},"frontmatter":{"date":"July 03, 2024","title":"RDBMS와 NoSQL","tags":["rdbms","nosql","mysql","mongodb"]},"rawMarkdownBody":"\n## 데이터베이스\n\n데이터베이스는 컴퓨터 시스템에 저장된 구조화된 정보의 집합을 의미합니다. 데이터베이스는 대개 DBMS(Database Management System)이라는 소프트웨어에 의해 관리되며 이 둘을 합쳐 일반적으로 데이터베이스라고 부릅니다.\n\n현재 가장 많이 쓰이는 데이터베이스는 RDBMS(Relational Database Management System)이고 이 소프트웨어와 소통하는 방법은 SQL(Structured Query Language)를 사용합니다. 하지만 최근에 들어서 NoSQL(Not only SQL)이라는 관계형 데이터 모델을 탈피한 데이터 모델을 지향하는 DBMS가 인기를 끌고 있습니다. 해당 글에서는 RDBMS와 NoSQL 중 문서형 데이터 모델의 차이를 알아보도록 하겠습니다.\n\n## 데이터 모델링\n\n데이터 모델링은 현실의 정보를 인간과 컴퓨터가 이해할 수 있도록 추상화하여 표현하는 과정입니다. 데이터와 데이터의 관계, 데이터의 의미와 일관성, 제약조건을 기술하여 정보를 실제로 이용할 수 있는 형태로 표현할 수 있습니다.\n\n데이터 모델링의 기본 구성요소는 \n1. Entity(개체): 개념, 정보 단위 같은 현실 세계의 대상체\n2. Attribute(속성) : 개체를 구성하는 항목\n3. Relation(관계) : 개체나 속성간의 관계\n으로 이루어져 있습니다. \n\n데이터 모델링 과정은 \n1. 개념적 데이터 모델링 : 데이터의 주요 개체와 속성을 정의하고, 이들 간의 관계로 전체적인 구조를 설계\n2. 논리적 데이터 모델링 : 개념적 모델을 기반으로, 데이터베이스 구조를 논리적으로 설계\n3. 물리적 데이터 모델링 : 논리적 모델을 실제 데이터베이스 관리 시스템(DBMS)에 맞게 구현\n\n의 과정을 걸칩니다. 이 때 논리적 데이터 모델링은 데이터 간의 관계를 표현하는 방식에 따라 관계 모델(Relational Model), 계층 모델(Hierarchical Model), 네트워크 모델(Network Model) 등의 다양한 모델이 있습니다.\n\n## RDBMS\n\n### 관계형 데이터 모델의 제안\n\n초기의 DBMS는 계층형 모델로 이루어져 있었습니다. 그러다 1970년, Edgar Codd는 데이터를 테이블 형태로 저장하고 이를 관계(relation)와 튜플(tuple)로 표현하는 관계형 모델을 제안했습니다. (Excel이 1980년에 나왔으니 굉장히 획기적인 방식의 데이터 모델이었습니다.)\n\nIBM은 이 관계형 모델을 실질적으로 구현하고 활용하기 위해 SQL이라는 선언형 질의 언어를 개발합니다. 선언형 질의 언어는 선언을 수행할 방법을 따로 명시하지 않습니다.\n\n`SELECT * FROM STUDENT` 라는 SQL문에는 통상의 프로그래밍 언어처럼 변수를 선언한다던가, 제어문과 반복문으로 어떻게 데이터를 저장하고 변형할지에 대한 구체적인 방법이 적혀있지 않습니다. \n\n따라서 RDBMS는 SQL의 사용으로 내부 구현을 감출 수 있게 되었고 선언이 실현되는 구체적인 방식을 사용자는 알 필요가 없게 되었습니다. \n(RDBMS의 옵티마이저가 어떤 식으로 쿼리 수행 계획을 짜는지, 그 계획이 어떻게 실행되는지 사용자는 몰라도 됩니다)\n\n1980년대 부터 30년간 RDBMS와 SQL은 데이터베이스로 가장 많이 선택되는 도구였고 현재도 큰 인기를 얻고 있습니다.\n\n### 관계형 데이터베이스의 특징\n\n데이터 베이스에서 데이터의 정합성은 중요합니다(데이터베이스의 데이터가 잘못되었다면 이에 기반을 둔 모든 처리 결과가 부정확해지기 때문입니다). 관계형 데이터베이스가 주류가 된 이유는 데이터의 정합성을 높이기 위한 설계 노하우가 매우 발달했기 때문입니다.\n\n관계형 데이터베이스는 모든 데이터를 행열 테이블 형태로 저장합니다. 이 때 테이블은 공통적인 개념의 집합이어야 하며 각각의 행은 유일하게 식별할 수 있어야 합니다.\n\n이 두가지의 합의만 지킨다면 매우 자의적으로 제멋대로인 테이블들이 생성될 수 있고 이를 방지하기 위해 관계형 데이터베이스는 '테이블은 이렇게 정의해야 한다'라는 테이블 설계 이론 `정규화` 에 따라 설계됩니다.\n\n정규화는 간단하게 말해서 `x -> y`라는 관계를 지키며(이를 함수 종속성이라 합니다) 데이터의 중복을 최소화하게 테이블을 분리하는 것을 의미합니다. 자세한 정규화의 방법은 [링크](https://en.wikipedia.org/wiki/Database_normalization)에서 볼 수 있습니다.\n\n또다른 관계형 데이터베이스의 큰 특징은 데이터의 일관성을 굉장히 중요시 여긴다는 것입니다. 이를 위해 관계형 데이터베이스는 데이터 갱신시 트랜잭션을 지원합니다.\n\n트랜잭션은 데이터베이스에서 여러개의 작업을 하나의 단위로 묶어서 처리하는 기술이며 이는 ACID특성을 가집니다. \n\n1. Atomicity : 트랜잭션은 모두 성공 or 모두 실패\n2. Consistentcy : 트랜잭션 이전과 이후 DB 스키마는 그대로여야 합니다.\n3. Isolation : 동시에 실행되는 트랜잭션은 서로 격리된다\n4. Durability : 트랜잭션이 성공했다면 결함이나 장애가 발생해도 데이터는 손실되지 않는다.\n\n## `#NoSql` \n\nNoSql은 현재 관계형 데이터베이스가 아닌 데이터베이스를 일컫는 말이지만 원래는 2010년 경 오픈소스, 분산환경, 관계형 데이터베이스 탈피 밋업의 해시태그 였습니다. \n\n이러한 바람이 일어난 이유는 애플리케이션이 다루어야 하는 데이터의 급증입니다. 따라서 대규모 데이터셋의 높은 쓰기량을 맞추기 위한 확장성이 필요했고, 관계형 데이터베이스의 엄격한 스키마에 대한 불만과 유연한 데이터 모델에 대한 열망이 NoSQL 오픈소스 발달의 배경에 있었습니다. \n\n### RDBMS의 한계\n\n- 스키마의 엄격함과 유연성 부족\n\nRDBMS는 설계시의 테이블이 지속될 것을 가정합니다. 예를 들어 상품에 신규 이벤트 A가 적용되었는지 여부를 기존의 테이블에 적용하기 위해서는 모든 테이블의 행을 바꿔야 하며 적재된 데이터가 많을수록 많은 시간과 비용이 듭니다.\n\n- 수평적 확장의 어려움 \n\n관계형 데이터 모델은 정규화를 시행하여 데이터의 중복을 줄이지만 이는 동시에 하나의 객체에 대한 연관된 데이터들이 여러 테이블로 나뉘어 지는 것을 의미합니다. 이 테이블들은 단일 서버에서는 효율적으로 처리되지만 서버가 분산되어 있다면 관리가 어렵고 성능에 많은 영향을 미칩니다.\n\n관계를 유지하며 데이터를 연산하기 위해 조인연산이 빈번하게 발생하며 데이터가 여러 서버에 분산되어 있을 경우 네트워크 오버헤드가 추가로 발생합니다.\n\n트랜잭션을 지원하는 것 또한 여러 서버에서 동시에 트랜잭션을 수행하고 모든 트랜잭션의 성공을 검증해야 하는 방식으로 바뀌기 때문에 관리가 복잡해질 수 있습니다. \n\n- 객체지향 프로그래밍 언어와 RDBMS의 패러다임 불일치\n\n객체지향 언어는 데이터와 기능을 함께 묶어 객체로 표현하며, 객체 간의 관계를 통해 프로그래밍 로직을 구현합니다. 반면, RDBMS는 데이터를 테이블 형태로 저장하며 테이블 간의 관계를 통해 데이터 모델을 구성합니다. \n\n이러한 차이로 인해 객체지향 언어와 RDBMS 간의 데이터 변환이 필요하며, 이를 위해 ORM(Object-Relational Mapping) 도구를 사용하지만, ORM 도구의 사용은 추가적인 복잡성과 성능 오버헤드를 초래할 수 있습니다. \n\nJava의 Hibernate를 예로 들자면 N+1문제는 이러한 패러다임의 불일치에 의한 사이드이펙트라고 할 수 있습니다. (객체내의 정보에 접근한다는 객체 지향언어에서는 자연스러운 접근이 테이블끼리의 접근에 join을 사용해야만 하는 RDBMS에서는 불필요한 쿼리와 성능저하를 가져옵니다)\n\n### NoSQL의 특징\n\nNoSQL은 위와 같은 RDBMS의 한계를 탈피하기 위해 등장했습니다. NoSQL은 RDBMS의 일부 기능에 대한 집착을 버림으로써 확장성과 유연성을 획득합니다.\n\n-  스키마에 대한 집착을 버리다\n\nRDBMS의 한계에서 말했던 것처럼 고정된 스키마와 데이터의 중복을 제거하기 위한 정규화에 따른 다수의 테이블은 수많은 join의 사용이 필요했습니다. \n```\npublic class Post {\n\tString title;\n\tString content;\n\tList<Comment> comments;\n\tList<View> views;\n}\n```\nRDBMS는 위와 같은 객체를 Posts, Comments, Views 테이블 3개로 나누어서 저장할 것입니다. 따라서 Post라는 하나의 행의 정보를 온전히 얻기 위해서는 적어도 3개 테이블의 join이 필요합니다. \n\n문서형 데이터 모델을 사용하는 MongoDB는 해당 객체를 json형태로 저장합니다. \n```json\n{ \"title\": \"제목\", \"content\": \"내용\", \"comments\": [ { \"id\": 1, \"text\": \"댓글 내용 1\" }, { \"id\": 2, \"text\": \"댓글 내용 2\" } ], \"views\": [ { \"viewerId\": 1, \"viewedAt\": \"2024-07-03T10:00:00Z\" }, { \"viewerId\": 2, \"viewedAt\": \"2024-07-03T11:00:00Z\" } ] }\n```\n\n이제 Post에대한 하나의 질의로 Post객체에 대한 정보를 온전히 얻을 수 있습니다.\n다른 객체에 의존하지 않기 때문에 분산 환경에서 Post가 어느 서버에 저장되어도 다른 서버에 있는 데이터에 대한 join을 수행할 필요가 없어지며 이는 NoSQL이 수평적 확장을 쉽게 하는 이유가 됩니다.\n\n- ACID보다 BASE\n\nRDBMS의 트랜잭션은 ACID를 충실히 지킴으로써 다중 사용자 요청시 각 데이터의 즉각적인 일관성을 보장합니다. NoSQL은 트랜잭션에 대해 BASE(Basically available, Soft state, Eventual Consistency)를 추구합니다. \n\n1. Basically Available : 사용자는 언제든 데이터베이스 동시에 엑세스할 수 있다.\n2. Soft State : 데이터는 일시적, 임시 상태를 가질 수 있다.\n3. Eventual Consistency : 로컬 데이터는 일시적으로 변경을 반영하지 않을 수 있지만 변경 내용은 전파되고 최종적으로 일관성을 획득한다.\n\n즉각적인 일관성에 대한 집착을 포기함으로써 ACID가 가지는 단점(레코드의 일관성을 유지하기 위해 잠금이 필요 혹은 무결성 제약으로 잠금이 전파되는 등)을 극복하여 더 나은 사용자 요청 처리를 할 수 있습니다. \n\n### 문서형 데이터 모델의 한계\n\n위에서 Post를 json으로 쉽게 문서형 데이터 모델로 표현할 수 있다고 했습니다. 하지만 User가 추가되면서 이들이 다수의 Post를 가지는 경우는 어떨까요?\n```java\npublic class User {\n\tString name;\n\tList<Post> posts;\n}\n```\n\nNoSQL은 하나의 객체에 대해 join 없는 하나의 질의로 수평적 확장의 이점을 가진다고 했습니다. 이 User 객체도 완전한 정보를 가지게 하려면 Post의 모든 데이터를 User라는 객체도 가지고 있어야 합니다.\n\n그럼 이 User가 Team에 속한다면? 그리고 Team이 User를 다수 가질 수 있다면,  Team은 User에 대한 모든 정보와 각 User가 가지는 모든 Post에 대한 정보를 또! 가지게 되고. 중복 데이터를 여러 객체가 가지게 될 것입니다. \n\n이러한 중복 데이터는 모두 한꺼번에 변경되어야 하며 이는 쓰기 오버헤드의 증가와 일부 갱신의 위험이 있습니다. \n\nMongoDB는 이러한 다대일 관계의 데이터에 대응하기 위해 문서 참조 기능을 제공하고 있지만 이는 RDBMS의 join과 다를바가 없습니다. 즉, 데이터가 다대다 관계로 상호 연결될수록 문서형 데이터 모델이 가지고 있던 장점이 희석됩니다.\n\n이러한 문서형 데이터 모델의 한계는 관계형 모델이 제안되기 전 계층형 모델이 가지던 한계와 비슷합니다. 계층형 모델은 부모 노드가 자식 노드를 가지고 있는 형태였고 조인을 지원하지 않아 다대다 관계 표현이 어려웠습니다. \n\n관계형 모델은 이러한 문제를 테이블로 대표되는 데이터 구성과 각 테이블 사이의 join으로 (다대다의 경우 일대다 + 다대일 매핑 테이블의 삽입)으로 풀어냈고 선풍적인 인기를 끌었습니다. 즉, 지금 문서형 데이터 모델이 가지는 단점을 80년대에 관계형 모델이 풀어냈던 것입니다. \n\n### MySQL과 MongoDB\n\nMySQL은 대표적인 RDBMS이고 MongoDB는 문서형 데이터 모델을 사용하는 NoSQL 데이터베이스입니다. \nMySQL은 5.7버전부터 json타입의 데이터를 추가했고 MongoDB는 문서 참조와 각 노드에 대한 분산 트랜잭션 처리를 지원합니다. \n\nMySQL은 자체적으로 복제와 파티셔닝(서버 내에서 큰 테이블 -> 작은 테이블로)을 지원하고 있고 샤딩(여러 서버에 나누어 저장)은 애플리케이션 구현에 따라 할 수 있으므로 수평적 확장을 아예 하지 못하는 것도 아닙니다. \n\n우아한 형제들의 [발표](https://www.youtube.com/watch?v=704qQs6KoUk)는 주문 시스템이 대량의 데이터를 어떻게 처리하는지에 대해 말하고 있습니다. RDBMS에 저장되던 주문이라는 객체가 가져야하는 정보가 늘어나면서 정규화된 테이블이 늘어나게 되고 이로 인한 다수 join으로 인해 성능이 저하되었고, 이를 해결하기 위해 MongoDB를 도입하여 주문 객체를 하나로 모아 조회 성능을 높였다고 합니다.\n\n이 때 쓰기 질의에 쓰이는 DB는 RDBMS로, 읽기 질의에 쓰이는 DB는 MongoDB로 사용하여 데이터 갱신 시에는 RDBMS의 데이터 정합성의 이점을 가져가고 읽기 시에는 MongoDB의 이점을 가져가는 방식을 택했습니다. 또한 쓰기시의 RDBMS는 샤딩을 구현하여 수평적으로 확장했다고 말하고 있습니다.\n\n결국은 애플리케이션에서 사용하는 데이터가 어떠한 모델인지, 서로간에 어떠한 관계를 맺고 있는지에 따라 더 알맞은 도구를 선택해야 합니다.\n\n---\n참고\n\nhttps://www.oracle.com/database/what-is-database/ \n\n데이터 중심 애플리케이션 설계, 마틴 클레프만 저\n\n데이터베이스 첫걸음, 미크 기무라 메이지 저\n\nhttps://www.mongodb.com/resources/compare/relational-vs-non-relational-databases\n\nhttps://stackoverflow.com/questions/8729779/why-nosql-is-better-at-scaling-out-than-rdbms\n"},{"excerpt":"스레드 프로세스는 스스로 실행가능한 메모리를 가진 실행단위입니다. 쓰레드는 경량 프로세스라고도 불리며 프로세스 내의 일부분의 메모리를 공유하며 스택, 레지스터를 따로 가집니다. 쓰레드는 프로세스내의 메모리를 공유한다는 점 때문에 큰 장점을 가지지만 사용에 주의를 기울여야 합니다. Context Switching 로 문맥이 교환될 시 프로세스 A의 cont…","fields":{"slug":"/thread_deeper/"},"frontmatter":{"date":"June 28, 2024","title":"쓰레드와 톰캣 멀티 쓰레드 처리","tags":["threadpool","nio","eventloop"]},"rawMarkdownBody":"\n## 스레드\n\n프로세스는 스스로 실행가능한 메모리를 가진 실행단위입니다. 쓰레드는 경량 프로세스라고도 불리며 프로세스 내의 일부분의 메모리를 공유하며 스택, 레지스터를 따로 가집니다.\n\n쓰레드는 프로세스내의 메모리를 공유한다는 점 때문에 큰 장점을 가지지만 사용에 주의를 기울여야 합니다.\n\n## Context Switching\n\n`프로세스A - 커널 - 프로세스B`로 문맥이 교환될 시\n\n1. 프로세스 A의 context를 저장, 프로세스 B의 context을 읽어야 하고\n2. CPU의 레지스터 상태를 교체해야 하고\n3. TLB(translation Look aside Buffer)등 cache memory가 flush 되고\n4. MMU(Memory Management Unit)이 변경되어야 하기 때문에\n\n프로세스는 Context Switching 비용이 크다고 말합니다.\n\n쓰레드 컨텍스트 스위칭은 같은 프로세스 내에서 일어날 때 3,4번 과정이 필요 없기 때문에 프로세스 Context Switching에 비해 비용이 적습니다.\n\n왜일까요?\nMMU와 TLB모두 가상메모리에 올라온 프로세스와 실제 물리 메모리를 매핑하는데에 쓰입니다. 쓰레드 Context Switching은 같은 프로세스 내에서 일어나기 때문에 프로세스의 가상메모리 공간은 변하지 않기 때문에 2,3번 과정이 필요하지 않습니다.\n\n또한 같은 프로세스 내의 쓰레드들은 메모리 공간을 공유하므로 캐시 메모리에 저장된 데이터가 유효할 확률이 높습니다. 따라서 캐시 미스가 날 확률이 적어지므로 캐시 갱신을 수행하지 않을 확률이 높습니다.\n\n## Thread-Safe\n\n쓰레드는 프로세스의 메모리를 공유합니다. 따라서 두개 이상의 쓰레드가 동시에 공용 데이터에 접근할 시 프로그래머의 예상과 다른 결과가 발생할 수 있고 이러한 이슈가 발생하지 않는 코드를 Thread-Safe하다고 말합니다.\n\n공통 데이터에 접근이 가능한 것은 비단 쓰레드들만이 아닙니다. IPC(Inter Process Communication)방식 중 Shared-Memory를 사용하는 프로세스들도 공통의 메모리 공간에 접근할 수 있기 때문에 사용에 주의를 요합니다.\n\n## Synchronization\n\n위에서 쓰레드는 다른 쓰레드와 메모리를 공유하기 때문에 효율적이지만 공유 데이터를 신중하게 다루어야 한다고 말했습니다. 이 문제의 유형을 크게 thread interference와 memory consistency error로 나눌 수 있습니다. 이러한 문제를 방지하는 도구를 Synchronization(동기화)라고 합니다.\n\n`Thread Interference`\n\n```java\nclass Counter {\n    private int c = 0;\n\n    public void increment() {\n        c++;\n    }\n\n    public void decrement() {\n        c--;\n    }\n\n    public int value() {\n        return c;\n    }\n\n}\n```\n\n쓰레드 A가 increment를, B가 decrement를 한다고 가정하면 (프로그래머는 A->B 순서로 결과가 나오길 바라는 코드를 작성한다면) A-B 순서대로 쓰레드 실행을 보장할 수 없습니다.\n\n```\nA : c를 읽는다 (c = 0)\nB : c를 읽는다 (c = 0)\nA : 1을 증가한다 c = 1\nB : 1을 감소한다 c = -1\nA : c에 연산 결과를 저장한다 c = 1\nB : c에 연산 결과를 저장한다 c = -1\n```\n\n쓰레드 A의 연산결과가 B에 의해 덮어 쓰여졌습니다. 이걸 Thread Interference라고 합니다.\n\n`Memory Consistency Error`\n\n![Pasted image 20240625165005](https://github.com/jinkshower/learned/assets/135244018/1537ed39-8a8c-47f6-8f9c-7bde0c824099)\n\n쓰레드 A가 실행한 결과가 B에서 즉시 보이지 않을 때 일어나는 문제입니다. 해당 문제에 대한 해결 방법 중 하나는 volatile키워드를 사용하는 것입니다.\n\nvolatile을 변수에 붙이면 jvm은 해당 변수에 대한 읽고 쓰기 처리를 RAM에 직접 합니다. 즉, 해당 변수는 캐시 메모리에 저장되지 않습니다. 캐시 메모리가 언제 RAM에 쓰기를 하느냐는 운영체제에 따라 다릅니다.\n\n## 자바가 제공하는 Synchronization 방법\n\n### synchronized\n\n`synchronized` 키워드는 자바에서 가장 기본적인 동기화 방법입니다. 메서드나 블록에 `synchronized`를 붙여 특정 객체의 모니터를 사용하여 동기화할 수 있습니다. 모니터는 잠금을 의미하며, 한 번에 하나의 쓰레드만 모니터를 소유할 수 있습니다.\n\n모니터는 프로그래머가 직접 코딩해야 하는, 그래서 실수의 가능성이 있는 뮤텍스(한 자원에 대해 잠금), 세마포어(여러 자원을 관리)의 방법론을 언어 수준에서 제공합니다.\n\nJava의 모든 인스턴스들은 모니터들을 가지고 있고 synchronized에 접근시 모니터를 얻으며 모니터가 이미 다른 쓰레드에 의해 획득되어 있으면 \b큐에서 대기하며 모니터 획득을 기다립니다.\n\n### java.lang.concurrent 패키지\n\n하나의 락에 의존하지 않는 Thread-Safe한 컬렉션 클래스들을 제공합니다.\n\n위에서 다루었던 Memory Consistency Error에 대해서 oracle은 이 패키지가 해당 문제를 해결하기 위한 고수준의 동기화를 제공하며 (괜히 딴 거 쓰다가 버그 나지 말고) 쓰레드의 Synchronization을 다룰 때 해당 패키지를 사용하길 권장합니다.  [참고](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html#MemoryVisibility)\n\n###  java.lang.concurrent.atomic 패키지\n\n락을 사용하지 않는 Thread-Safe한 변수 클래스를 제공합니다. 락을 사용하지 않는 non-blocking으로 작동합니다.\n\n내부적으로 value를 volatile로 관리하여 Memory Consistency Error로 가시성이 확보되지 않는 문제를 방지하고\n```java\npublic class AtomicInteger extends Number implements java.io.Serializable {\n\tprivate volatile int value;\n}\n```\n\nexpected value를 설정하고 연산을 실행하여 expected value와 연산값이 같지 않으면(Thread interference에 의해) false를 반환합니다.\n\n## 전 겁이 많아서.. 그냥 모든 메서드 synchronized 쓸게요?\n\nsynchronized는 키워드만 붙이면 Synchronization을 실행해주지만 주의해서 사용해야 합니다.\n\n[이전 글](https://jinkshower.github.io/ticket_reservation_concurrency/)에서 @Transactional이라는 스프링 AOP를 사용할 때 synchronized가 생각대로 작동하지 않는다는 것도 다루었지만, 하나의 쓰레드의 접근만을 허용한다는 것은 다른 쓰레드들은 대기해야 한다는 말과도 같습니다.\n\n쓰레드가 대기한다는 것은 쓰레드가 BLOCKED가 된다는 말이고 이는 Context Switching을 자주 발생시켜 성능에 저하가 될 수 있습니다.\n\n## 불변\n\n그럼 어떻게 해야할까요? 위의 모든 문제를 보면 코드 한 줄 작성하는 것이 매우 불안해집니다. ++같은 연산자 하나도 함부로 쓰지 못할 것 같네요..\n\noracle은 이에 대해 객체의 불변을 지향할 것을 추천합니다. 간단히 말해서 변하지 않는 객체는 다른 쓰레드에 의해 오염되지 않고 불안정한 상태에서 보여지지 않기 때문입니다. [참고](https://docs.oracle.com/javase/tutorial/essential/concurrency/imstrat.html)\n\n불변 객체를 만드는 방법을 간단하게 소개합니다.\n\n```\n1. setter 쓰지 않기\n2. 모든 필드를 private, final로 만들기\n3. class를 final로 만들어 상속 금지하기\n4. 가변 객체를 참조하는 변수가 있다면 변경 메서드 제공하지 않기\n5. 4의 참조를 바로 제공하지 않고 복사본을 제공하기\n```\n\n## Java에서 쓰레드는 어떻게 생성하나\n\n위에서 Atomic 패키지, synchronized에 대해 설명할 때 non-blocking과 context switching에 대해 말했습니다. 이에 대해 좀 더 설명하기 위해 Java에서 쓰레드를 좀더 살펴보려합니다.\n\nJava에서 쓰레드를 실행하는 방법은 Thread 객체를 생성하고 start()메서드를 호출하는 것입니다.\n\nThread 클래스의 쓰레드 통제를 위한 메서드를 가져왔습니다.\n```java\npublic class Thread implements Runnable {\n\tprivate native void start0();\n\t//\n\tpublic static native void sleep(long millis) throws InterruptedException;\n\t//\n\tprivate native void setPriority0(int newPriority);  \n\tprivate native void stop0(Object o);  \n\tprivate native void suspend0();  \n\tprivate native void resume0();  \n\tprivate native void interrupt0();  \n\tprivate static native void clearInterruptEvent();  \n\tprivate native void setNativeName(String name);\n}\n```\n\n쓰레드의 정보를 확인하는 메서드들와 달리 Java는 쓰레드의 생성과 통제를 JNI(Java Native Interface)를 통해 운영체제에 맡김을 알 수 있습니다.\n\n따라서 자바에서 쓰레드가 비싸다라고 하는 이유는 운영체제가 일반적으로 만드는 쓰레드 생성비용이 그대로 들기 때문이고, 쓰레드가 실행되려면 언제나 운영체제를 거치기 때문에 시스템콜이 발생하기 때문입니다.\n![Pasted image 20240624160016](https://github.com/jinkshower/learned/assets/135244018/fc812d0d-73ae-4f35-9a08-c176a1e29c87)\n\n## Thread의 상태\n\n![Pasted image 20240626151527](https://github.com/jinkshower/learned/assets/135244018/89846ac1-a456-42e3-95a0-cd9f6f86c7ac)\n\n`NEW`\n\n쓰레드가 생성되었습니다.\n\n`READY`\n\n쓰레드가 RUNNING할 준비가 되었습니다. Ready큐에서 대기합니다. 어떤 쓰레드가 RUNNING이 될지는 운영체제의 스케쥴링 방식에 따라 다릅니다.\n\n`RUNNING`\n\n쓰레드가 실행중입니다. 이전과 다른 쓰레드라면 Context Switching이 일어납니다.\n\n`TIMED WAITING, WAITING, BLOCKED`\n\n쓰레드가 일시정지된 상태입니다. 쓰레드 실행에 할당된 시간이 지나거나 (스케쥴러의 time quantum을 다 씀) 락 획득을 기다리거나, i/o요청의 완료를 기다리는 상태입니다.\n\n`TERMINATED`\n\n쓰레드가 종료되었습니다.\n\n## blocking, non-blocking\n\nblocking과 non-blocking은 I/O를 다룰 때에 많이 접했던 키워드였고 제어권에 대한 것이라는 말을 많이 접했지만 쓰레드의 Synchronization을 공부하면서 쓰레드가 어떠한 요청을 했을 때 쓰레드 상태가 어떻게 되는지에 대한 설명으로 쓰인다는 것을 알게 되었습니다.\n\n즉, 쓰레드 A가 요청을 했을 때 (락에 들어가고 싶어요, DB에서 이거 읽어줘요) A가 어떠한 상태가 되냐의 차이입니다.\n\nblocking은 쓰레드의 상태를 바꿉니다.  Synchronized의 주의점에 대해 설명할 때도 쓰레드는 락을 획득하기 위해서 BLOCKED 상태가 된다고 했습니다. 이는 I/O요청에서도 마찬가지 입니다. I/O요청 이후 쓰레드가 BLOCKED가 되면 이를 blocking이라고 합니다.\n\nnon-blocking은 쓰레드의 상태를 바꾸지 않습니다. Atomic 패키지를 설명할 때 간단하게 non-blocking이라고 설명했는데요, 대표적으로 AtomicInteger 클래스는 synchronized를 사용하지 않습니다. 이 클래스 함수들은 락 없이 수행되기 때문에 이 함수를 사용하는 쓰레드는 BLOCKED가 되지 않기 때문이었습니다.\n\nBLOCKED된 쓰레드가 다시 RUNNING이 되려면 Context Switching이 일어나야 합니다. 따라서 Blocking과 Non-blocking은 쓰레드의 상태가 변경되느냐, 그래서 Context Switching이 일어나느냐의 차이가 있습니다.\n\n## Thread Pool\n\n웹서버를 생각해보면 수십만의 요청이 들어오는 것이 당연한 상황이 많습니다.\n\nWAS가 사용자의 요청마다 쓰레드를 만들어 실행(Thread per Request)된다고 하면 비싼 쓰레드를 생성하는 비용이 매번 들고 메모리는 순식간에 고갈될 것입니다. 수 많은 쓰레드들이 Context Switching을 하면서 생기는 오버헤드도 엄청날 것입니다.\n\n따라서 Tomcat은 ThreadPool을 만들어 미리 정해진 숫자만큼의 쓰레드들을 생성해두고 사용자 요청을 작업큐에 담아 쓰레드풀 내에서 사용가능한 쓰레드들에게 작업을 할당하여 작업큐를 순서대로 비우며 요청을 처리합니다.\n\n하지만 이러한 방식도 단점이 있습니다. 앞에서 살펴본 blocking때문에 일어나는 상황인데요, 쓰레드들은 I/O 작업이 길어지거나 락 획득을 기다릴 때 BLOCKED된다고 했습니다.\n\n이 일시정지 상태가 길어지면 쓰레드풀을 대기만 하는 쓰레드가 점유하게 되고 이러한 쓰레드들이 많아 작업 큐에 대기만 하는 요청들이 많아져 성능이 저하되는 현상을 쓰레드 풀 고갈(Thread Pool Exhaustion)이라고도 합니다.\n\n이러한 쓰레드풀 고갈현상을 해결하기 위한 방법으로는 쓰레드풀 크기 설정(크기 늘리기), 타임아웃 설정(일정시간 대기가 길어지면 쓰레드 해체), 그리고 I/O방식을 변경하는 방법이 있습니다.\n\n이 중 Tomcat이 I/O 방식을 변경한 방법에 대해 살펴보겠습니다.\n\n## Java NIO(new I/O)\n\nJava에서 입출력을 담당하는 java.io 패키지를 개선하는 java.nio 패키지가 JDK 1.4부터 추가되었습니다. 그리고 Java 7부터 파일 읽기 쓰기를 보완하는 nio2가 추가되었습니다. 갑자기 JDK 이야기를 왜 하냐고요?\n\nJava nio는 전통적인 Java I/O의 한계를 넘는 기능들을 제공하고 있기 때문입니다. oracle은 이 패키지에서 Buffer, Channel, Selector 클래스를 multiplexed, non-blocking i/o 기능을 지원하는 클래스들이라고 말합니다.\n\nmultiplexing은 클라이언트마다 별도의 스레드를 이용하는 것이 아니라 하나의 스레드에서 다수의 클라이언트에 연결된 소켓들을 관리하며 소켓에 이벤트가 발생할때만 처리하여 다중 입출력을 처리하는 기법을 말합니다.\n\n간단히 요약하면\n```\n스레드가 Buffer로 데이터를 읽어달라고 Channel에 요청\nChannel이 Buffer를 채우는 동안 호출한 스레드는 다른일을 할 수 있음(non-blocking)\nChannel이 Buuffer를 다 채우면 스레드는 Buffer로 하고 싶은 일을 할 수 있음\n\n이 Channel들이 Buffer 채우거나 다 채운 지는 Selector가 감시함\n```\n\n위에서 쓰레드 blocking에 대해 락에 관한 예시를 들었는데 I/O작업도 마찬가지라고 했습니다.  \nTomcat은 Connector(서버 요청을 받아들이고 처리하는 객체)가 작동하는 방식을 BIO(Blocking I/O)에서 NIO(Non-blocking I/O)로 바꾸었습니다.\n\nBIO는 위의 Thread Pool에서 설명한대로 작동했습니다. NIO는 위의 java.nio 패키지를 사용합니다.\n\nNIO가 간략하게 어떤 방식을 거치는 지 요약하면,\n```\n1. Acceptor가 Socket Connection을 얻습니다. \n2. Socket Connection을 Channel로 변환합니다. \n3. Channel을 Event로 변환하고 Event Queue에 넣습니다. \n4. Poller는 Selector(java.nio)를 가지고 있고 Event Queue의 Event들을 Selector에 등록합니다. \n5. Selector는 이 Channel들에서 I/O가 가능한 Channel들을 모니터링하고, I/O 작업이 가능한 Channel을 선택합니다. \n6. I/O 작업이 가능한 Channel이 생기면 Selector는 Worker Thread Pool의 Worker Thread를 할당합니다. (worker thread 할당을 하며 이 작업의 끝남을 기다리지 않습니다. 즉, non-blocking으로 이루어집니다) \n7. Worker Thread는 내부 로직을 실행하고 그 결과를 Buffer에 씁니다. \n```\n\n요청과 응답을 호출한 쓰레드가 일시정지하여 무한히 기다리지 않고, 작업이 가능한 Channel에서 오는 알림을 받을 때만 실행하기 때문에 대기만 하는 쓰레드가 쓰레드풀을 점유하는 현상이 덜 일어나게 됩니다.\n\n---\n참고\n\nhttps://www.geeksforgeeks.org/thread-interference-and-memory-consistency-errors-in-java/\n\nhttps://docs.oracle.com/javase/tutorial/essential/concurrency/procthread.html\n\nhttps://badcandy.github.io/2019/01/14/concurrency-02/\n\nhttps://techblog.woowahan.com/15398/\n\nhttps://tecoble.techcourse.co.kr/post/2021-10-23-java-synchronize/\n\nhttps://sihyung92.oopy.io/spring/1\n\nhttps://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-summary.html\n\nhttps://mark-kim.blog/understanding-non-blocking-io-and-nio/\n\nhttps://docs.oracle.com/javase/tutorial/essential/concurrency/pools.html"},{"excerpt":"(Java8 이전 버전에 대한 설명입니다) 이 코드, 어떻게 실행될까?  Cat을 만들고 각 객체의 메서드를 실행하는 아주 단순한 코드입니다. 해당 코드가 대체? 어떻게? 실행되는지에 대해 정리해보고 싶었습니다. javac HelloJVM.java로 .class파일을 만든다. 자바 기초책을 보면 항상 javac로 컴파일하는 과정을 거쳐야 하는데요, 왜 j…","fields":{"slug":"/java_bytecode/"},"frontmatter":{"date":"June 17, 2024","title":"바이트코드와 함께 알아 보는 자바 실행과정","tags":["java","jvm"]},"rawMarkdownBody":"\n\n(Java8 이전 버전에 대한 설명입니다)\n\n## 이 코드, 어떻게 실행될까?\n\n`HelloJVM.java`\n```java\npublic class HelloJVM {  \n  \n    public static void main(String[] args) {  \n        Cat hiyen = new Cat(5);  \n        Cat yihen = new Cat(7);  \n  \n        hiyen.meow();  \n        yihen.meow();  \n    }  \n}\n\nclass Cat {  \n  \n    int age;  \n    static int tail = 1;  \n  \n    public Cat(int age) {  \n        this.age = age;  \n    }  \n  \n    public void meow() {  \n        System.out.println(\"meow\");  \n    }  \n}\n```\n\nCat을 만들고 각 객체의 메서드를 실행하는 아주 단순한 코드입니다. 해당 코드가 대체? 어떻게? 실행되는지에 대해 정리해보고 싶었습니다.\n\n## javac HelloJVM.java로 .class파일을 만든다.\n\n자바 기초책을 보면 항상 javac로 컴파일하는 과정을 거쳐야 하는데요, 왜 javac로 컴파일 하는 과정을 거쳐야 하는 걸까요? \n\n### Write Once Use Anywhere\n\ncpu는 기계어(0과 1)를 처리하는 연산장치입니다. 따라서 우리가 어떠한 고급언어를 사용하든지 간에 해당 언어를 컴퓨터가 이해할 수 있는 기계어로 번역하는 과정이 필요합니다. 게다가 cpu는 종류마다 해석할 수 있는 기계어가 다릅니다.\n\nC는 컴파일을 통해 실행파일을 만드는 대표적인 컴파일 언어입니다. 하지만 OS마다 다른 컴파일러가 필요했고, 한 OS에서 쓰이는 함수가 다른 OS에서 쓰이지 않는 경우도 있기 때문에 코드를 고치는 일도 필요했다고 합니다. \n\nJava는 이러한 C언어의 한계를 넘어 플랫폼에 중립적인 Java Virtual Machine을 채용했습니다. \n\nOS에 맞는 JVM만 한 번 설치하면 Java로 작성된 파일은 모두 실행할 수 있는 것입니다. 이를 Write Once Use Anywhere라고 표현합니다.\n\n### JVM의 언어, Java Byte Code\n\nJava는 JVM을 통해 OS에 종속되지 않지만 JVM은 OS에 종속됩니다. 따라서 C처럼 한번의 컴파일로 코드를 기계어로 만들지 않고 JVM이 이해할 수 있는 언어인 바이트 코드로 만들고,  JVM은 자신이 존재하는 OS에 맞게 해당 바이트 코드를 실행합니다. \n\n`HelloJVM.java`를 `javac HelloJVM.java`로 (자바)컴파일을 하면 바이트 코드 파일인 `HelloJVM.class`와  `Cat.class`가 생성됩니다.\n\n이 파일은 인간이 이해할 수 없는 언어로 이루어져 있는데, 이를 javap를 이용하여 Dissemble하면 우리가 이해할 수 있는 형태로 보여줍니다.\n\n`javap -v HelloJVM`\n\n(굉장히 깁니다)\n```\npublic class practice.HelloJVM\n  minor version: 0\n  major version: 61\n  flags: (0x0021) ACC_PUBLIC, ACC_SUPER\n  this_class: #15                         // practice/HelloJVM\n  super_class: #2                         // java/lang/Object\n  interfaces: 0, fields: 0, methods: 2, attributes: 1\nConstant pool:\n   #1 = Methodref          #2.#3          // java/lang/Object.\"<init>\":()V\n   #2 = Class              #4             // java/lang/Object\n   #3 = NameAndType        #5:#6          // \"<init>\":()V\n   #4 = Utf8               java/lang/Object\n   #5 = Utf8               <init>\n   #6 = Utf8               ()V\n   #7 = Class              #8             // practice/Cat\n   #8 = Utf8               practice/Cat\n   #9 = Methodref          #7.#10         // practice/Cat.\"<init>\":(I)V\n  #10 = NameAndType        #5:#11         // \"<init>\":(I)V\n  #11 = Utf8               (I)V\n  #12 = Methodref          #7.#13         // practice/Cat.meow:()V\n  #13 = NameAndType        #14:#6         // meow:()V\n  #14 = Utf8               meow\n  #15 = Class              #16            // practice/HelloJVM\n  #16 = Utf8               practice/HelloJVM\n  #17 = Utf8               Code\n  #18 = Utf8               LineNumberTable\n  #19 = Utf8               main\n  #20 = Utf8               ([Ljava/lang/String;)V\n  #21 = Utf8               SourceFile\n  #22 = Utf8               HelloJVM.java\n{\n  public practice.HelloJVM();\n    descriptor: ()V\n    flags: (0x0001) ACC_PUBLIC\n    Code:\n      stack=1, locals=1, args_size=1\n         0: aload_0\n         1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n         4: return\n      LineNumberTable:\n        line 3: 0\n\n  public static void main(java.lang.String[]);\n    descriptor: ([Ljava/lang/String;)V\n    flags: (0x0009) ACC_PUBLIC, ACC_STATIC\n    Code:\n      stack=3, locals=3, args_size=1\n         0: new           #7                  // class practice/Cat\n         3: dup\n         4: iconst_5\n         5: invokespecial #9                  // Method practice/Cat.\"<init>\":(I)V\n         8: astore_1\n         9: new           #7                  // class practice/Cat\n        12: dup\n        13: bipush        7\n        15: invokespecial #9                  // Method practice/Cat.\"<init>\":(I)V\n        18: astore_2\n        19: aload_1\n        20: invokevirtual #12                 // Method practice/Cat.meow:()V\n        23: aload_2\n        24: invokevirtual #12                 // Method practice/Cat.meow:()V\n        27: return\n      LineNumberTable:\n        line 6: 0\n        line 7: 9\n        line 9: 19\n        line 10: 23\n        line 11: 27\n}\n```\n\n좀 더 자세한 설명은 뒤에서 할테지만 지금은 해당 클래스에 대한 정보 (인터페이스 인지 상위 클래스는 어떤지 메서드는 몇 개인지 등)와 Constant Pool, 그리고 코드를 명령어 형태로 저장한다는 사실만 알고 있으면 좋을 것 같습니다.\n\n## JVM 구조\n\n이 바이트 코드가 어떻게 실행되는 지를 이해하기 위해서는 JVM의 구조를 살펴봐야 합니다.\n\n(추상화된 JVM의 구조이며 세부구현은 벤더에 따라 달라질 수 있습니다.)\n![Pasted image 20240616201254](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/8acc672b-3009-4022-9720-d563b40d28ac)\n\n\n- Class Loader\n\n바이트 코드를 Runtime Data Area에 적재하여 실행가능한 상태로 만듭니다.\nLoading - Linking - Initialization의 과정을 거칩니다.\n\n`Loading`\n\n.class파일을 바탕으로 클래스에 대한 정보를 Runtime Area의 Method Area에 로드합니다.\n이 클래스에 대한 정보를 바탕으로 `java.lang.Class`의 객체를 Heap Area에 만듭니다. \n이 객체는 클래스의 최상위 Object와 다르며 클래스의 메타데이터(이름,  패키지, 상속 , 메서드 정보 등)를 가지고 있습니다.\n\n`Linking`\n\nVerify - Prepare - (Resolve)의 과정을 거칩니다.\n\nVerify - 클래스가 자바 언어, JVM 명세에 맞는지 확인합니다.\nPrepare - 클래스 변수에 필요한 메모리를 할당하고 초기값(사용자가 정한 값이 아님)을 메모리에 할당합니다.\nResolve - 반드시 일어나지 않을 수도 있습니다. Constant Pool 내의 모든 심볼릭 레퍼런스를 실제 메모리 참조로 바꿉니다.(실제 참조가 일어날 때 해당 부분이 Lazy하게 작동됩니다)\n\n`Initialization`\n\n클래스 변수를 사용자가 정한 값으로 할당하고 static 블록 내의 코드를 실행합니다.\n\n```\nJava는 동적 로딩으로 이루어집니다. 즉, 모든 클래스를 한번에 Class Loader가 가져와서 메모리에 적재하는게 아니라 필수적인 클래스만을 미리 메모리에 적재하고 이후 런타임에서 클래스를 처음 참조할 때 위의 과정을 거쳐 메모리에 가져다 줍니다.\n```\n\n이렇게 클래스가 Class Loader의 모든 과정을 거치면 static 필드, static method에 접근할 수 있고 Heap 영역의 Class 객체를 통해 해당 클래스의 객체를 생성할 수 있는 준비가 됩니다.\n\n- Runtime Data Area\n\n`Method Area`\n\nClass Loader가 읽어 들인 클래스의 정보와 정적필드를 가지고 있습니다. 위에서 살펴 본 Constant Pool이 포함됩니다.\n\n`Heap Area`\n\n런타임에 생성되는 객체가 저장되는 곳입니다. new 연산자로 생성된 객체, 인스턴스 변수, 배열 타입 등이 저장됩니다. 해당 영역은 Execution Engine의 Garbage Collector가 사용하지 않는 객체를 삭제하며 메모리를 관리합니다.\n\n```\nOne Method Area, One Heap Area  \nJVM에서 Method Area와 Heap Area는 하나가 생성되고 생성된 쓰레드들은 이 두 영역을 공유합니다. \n```\n\n`Java Stacks`\n\n각 쓰레드 별로 별도로 할당되는 영역입니다. 쓰레드들은 메소드를 호출할 때마다 Frame이라는 단위를 push하게 됩니다. Frame은 Constant Pool에 대한 참조, Local variable을 저장하는 배열, 연산을 위한 Operand Stack을 따로 가집니다.\n\n`PC registers`\n\n현재 수행중인 JVM 명령어 주소를 저장하는 공간입니다. 쓰레드들은 context switching을 반복하며 실행되기 때문에 어디까지 실행되었는지 저장이 되어야 하기 때문입니다. 이 때 실행되고 있는 메서드가 native(자바 코드로 작성되지 않은 메서드)라면 undefined로 저장됩니다.\n\n`Native Method Stack`\n\nnative메서드 수행이 PC register에서 undefined로 저장된다고 했는데요, 일반적으로 java 메서드를 수행하는 경우 Java Stacks에 쌓이다가 native메서드를 실행하면 Native Method Stack에 쌓이게 됩니다.\n\n- Execution Engine\n\nExecution Engine은 Class Loader에 의해 Runtime Data Area에 배치된 바이트 코드를 실행하는 부분입니다.\n이 때 Interpretor와 JIT Compiler가 혼용되어 코드를 실행합니다. 아까 Heap 영역의 메모리를 관리하는 Garbage Collector도 이 부분에 존재합니다.\n\n`Interpretor`\n\n바이트 코드 명령어를 하나씩 읽어서 해석하며 실행합니다. \n따라서 같은 메서드라도 여러 번 호출 된다면 다시 해석하고 실행하기 때문에 효율이 좋지 않습니다.\n\n`JIT Compiler`\n\n위의 단점을 보완하기 위해 JIT Compiler가 도입되었습니다. 반복되는 코드를 검색하고 바이트 코드를 컴파일하여 Native Code로 변경, 더 이상 반복되는 코드를 한줄씩 Interpreting하지 않고 캐싱해두어 빠르게 실행되게 합니다. \n\n`Garbage Collector`\n\nHeap 영역에서 더 이상 사용하지 않는 메모리를 회수해줍니다. (c의 free()를 JVM내에서 한다고 생각해주시면 됩니다.) \n\n- Native Method Interface\n\nJava Native Interface(JNI)라고도 불리며 Java 프로그램이 네이티브 코드와 상호 작용할 수 있도록 하는 기술입니다. 이 인터페이스는 Java 언어와 네이티브 코드(C, C++) 간의 다리 역할을 하며, 네이티브 메서드를 통해 네이티브 라이브러리를 호출하고 Java 객체에 접근할 수 있도록 합니다.\n\n- Native Method Library\n\n네이티브 메서드 라이브러리는 JNI를 통해 Java 언어에서 호출할 수 있는 네이티브 코드의 집합입니다. 이 라이브러리는 특정 플랫폼에 맞춰 컴파일된 바이너리 파일(.so, .dll 등)로 제공되며, Java에서는 `native` 키워드를 사용하여 이 라이브러리의 메서드를 선언하고 호출합니다.\n\n##  java HelloJVM으로 컴파일된 HelloJVM.class를 실행한다.\n\n`java HelloJVM`를 입력하면 meow가 2번 출력되는 것을 볼 수 있습니다. 해당 과정은 어떻게 이루어질까요?\nJVM의 구조와 역할을 알아봤으니 위에서 봤던 HelloJVM.class와 함께 봅시다.\n\nClass Loader가 바이트 코드를 Method Area에 로드합니다. 이 때 위에서 설명한 것처럼 Method Area에는 HelloJVM의 Constant Pool도 같이 저장됩니다. \n\n(바이트 코드가 너무 길어서 설명에 필요한 부분만 잘라서 사용하겠습니다)\n```\nConstant pool:\n   #7 = Class              #8             // practice/Cat\n   #8 = Utf8               practice/Cat\n   #9 = Methodref          #7.#10         // practice/Cat.\"<init>\":(I)V\n  #10 = NameAndType        #5:#11         // \"<init>\":(I)V\n  #11 = Utf8               (I)V\n  #12 = Methodref          #7.#13         // practice/Cat.meow:()V\n  #13 = NameAndType        #14:#6         // meow:()V\n  #14 = Utf8               meow\n\n public static void main(java.lang.String[]);\n         0: new           #7                  // class practice/Cat\n         3: dup\n         4: iconst_5\n         5: invokespecial #9                  // Method practice/Cat.\"<init>\":(I)V\n         8: astore_1\n```\n\nmain 메서드를 보면서 설명하겠습니다.\n\n`0: new #7` : Constant Pool의 7번 참조를 위해 Heap 영역에 메모리를 할당하라는 명령어 입니다.\n\nConstant Pool에 7번을 보면 8번으로 참조를 가지고 있고 8번은 UTF-8이라는 타입으로 Cat 클래스의 이름을 문자열로 저장하고 있음을 알 수 있습니다. 문자열로 이름을 저장하고 있는 현 상태를 `심볼릭 레퍼런스` 라고 합니다.\n\nClass Loader의 Linking 과정을 설명하며 Resolve(심볼릭 레퍼런스 -> 실제 메모리 주소)는 반드시 일어나지는 않는다고 설명했습니다.  즉 나중에 로드하기 위해 심볼릭 레퍼런스로 Constant Pool에 일단 저장해두고 new와 같이 Heap의 메모리를 할당하는 연산자가 실행되면 할당된 메모리 주소로 Constant Pool을 업데이트 합니다. \n\n이를 `Constant Pool Resolution`이라고 합니다.\n\n- 정말 Cat은 나중에 로딩될까?\n\n```java\npublic static void main(String[] args) {  \n    System.out.println(\"Hello, JVM!\"); //added  \n  \n    Cat hiyen = new Cat(5);  \n    Cat yihen = new Cat(7);  \n  \n    hiyen.meow();  \n    yihen.meow();  \n}\n```\n\nCat이 로딩되는 타이밍을 알기 위해 잠시 출력문을 추가했습니다.\n\n`java -verbose:class HelloJVM`로 클래스 로딩에 대한 정보를 얻을 수 있습니다.\n![Pasted image 20240617160033](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/74cf3a8e-6a4b-429f-9152-1196e7637188)\n\nCat은 Hello,JVM이 출력된 이후에 실제로 사용될 때 동적으로 로딩됨을 알 수 있습니다.\n\nJVM 구조를 설명하며 Java Stacks는 Frame단위로 쌓인다고 했습니다. main() 메서드를 실행하면 Java Stacks에 Main의 Frame이 하나 생기게 되고 해당 Frame의 내부는 현재 클래스의 Constant Pool에 대한 참조, 지역 변수 배열, 그리고 연산에 필요한 Operand Stack이 할당됩니다. \n\n![Pasted image 20240617193002](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/086748ce-32e0-4a96-94e4-7f6c46d19e98)\n\nLocal Variable Array의 크기는 컴파일시 정해지며 0번 인덱스는 언제나 객체 자신입니다.\n\n```\n3 : dup\n4 : iconst_5\n```\n\ndup은 최상위 스택을 복사하여 스택 위에 다시 넣으라는 명령어 이고 iconst_5는 int 5를 스택에 푸시하는 명령어 입니다. \n\nnew 이후 Operand Stack의 가장 위는 new로 만든 객체에 대한 참조가 있습니다. JVM Specification에 따르면 이 참조는 초기화 되기 전(hiyen에 할당되기 전) 스택에 푸시되어 있는 상태입니다. [참고](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.10.1.9.new)\n\niconst_5는 스택의 가장 위에 int 5를 푸시하라는 명령입니다.  이 두 명령 이후 Frame은 다음과 같습니다.\n![Pasted image 20240617195441](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/591f331b-9dfc-4ff8-a4ce-b4886409155f)\n\n```\n5 : invokespecial #9 // Method practice/Cat.\"<init>\":(I)V\n8 : astore_1\n```\n\ninvokespecial은 Constant Pool의 9참조인 Cat 클래스의 생성자를 호출하는 명령어입니다.  생성자에 필요한 객체 참조와 age값이 pop되며 Cat의 객체가 생성되며 참조가 초기화 됩니다. [참고](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.10.1.9.invokespecial)\n\n![Pasted image 20240617195525](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/9430fd2b-b4ad-4b47-8471-a5e4420d013b)\n\nastore_1은 Local Variable Array의 1번 인덱스에 스택의 최상위 결과를 저장합니다.\n\n![Pasted image 20240617194956](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/1ef55460-71ef-4400-92be-0929720975e1)\n`Cat hiyen = new Cat(5)` 이 끝난 이후 Frame의 메모리 모습입니다. 이제 hiyen이라는 변수로 Heap에 있는 객체에 접근하는 것이 가능해졌습니다.\n\n```\n19: aload_1\n20: invokevirtual #12                 // Method practice/Cat.meow:()V\n```\n\naload_1로 Local Variables Array의 1번 인덱스에 접근하고 invokevirtual로 객체의 메서드를 사용하는 모습입니다.\n\n## static 변수는 어떻게 할당되나?\n\n```java\nprivate static Cat methodCat = new Cat(3);\n```\n\nstatic 변수에 객체를 생성하게 하면 어떻게 될 지도 궁금해졌습니다.\n\n![Pasted image 20240623215721](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/9cc235ed-ece2-456a-a04b-6aa1ddfe7733)\n해당 부분을 추가한 뒤의 바이트코드입니다. 해당 바이트 코드는\n\n```java\nprivate static Cat methodCat;  \n  \nstatic {  \n    methodCat = new Cat(3);  \n}\n```\n의 바이트 코드와 동일합니다.\n\n따라서 static변수는 로딩 중 초기화 과정에서 static 블록을 실행시킨 것과 같다고 생각하면 되겠습니다. \n\n물론 바이트 코드에서 보이는 것처럼 new , dup, iconst_3등이 같고 putstatic이라는 명령어만 다르니 힙 영역에 같은 과정으로 생성되고 다만 그 참조가 method영역에 가지고 있는 게 다르겠습니다.\n\n---\n\n참고\n\nhttps://docs.oracle.com/javase/specs/jvms/se7/html/index.html"},{"excerpt":"학습예제 코드 글에서 실습을 진행한 레포지토리입니다. 문제 상황 현재 진행하고 있는 프로젝트에서 예매시 동시에 사용자가 같은 좌석을 선택하는 상황을 테스트하기 위해 멀티스레드를 생성하는 테스트를 작성하게 되었습니다.  이 때 데이터 롤백을 위해 @Transactional을 사용했을 때 테스트 데이터가 삽입되지 않는 문제를 발견하게 되었습니다.  처음에는 …","fields":{"slug":"/thread_test_transation/"},"frontmatter":{"date":"May 20, 2024","title":"스레드테스트와 트랜잭션 전파","tags":["threadtest","transation"]},"rawMarkdownBody":"\n\n## 학습예제 코드\n\n글에서 실습을 진행한 [레포지토리](https://github.com/jinkshower/transaction)입니다.\n\n## 문제 상황\n\n현재 진행하고 있는 프로젝트에서 예매시 동시에 사용자가 같은 좌석을 선택하는 상황을 테스트하기 위해 멀티스레드를 생성하는 테스트를 작성하게 되었습니다. \n\n이 때 데이터 롤백을 위해 @Transactional을 사용했을 때 테스트 데이터가 삽입되지 않는 문제를 발견하게 되었습니다. \n\n처음에는 문제 원인을 파악하기 위해 데이터베이스를 바꿔보며 삽질을 했는데요, 해당 문제를 해결한 기록을 공유하고자 합니다.\n\n## 테스트 코드\n\n```java\n@DisplayName(\"동시에 한자리 예매시 첫번째 요청만 예매성공한다.\")  \n@Test  \n@Sql(\"/reservation-test-data.sql\")  \nvoid concurrency_test() throws InterruptedException {  \n    //given  \n    int tryCount = 10;  \n    long userId = 1L;  \n    Long concertId = 1L;  \n    ReservationRequestDto reservationRequestDto = ReservationRequestDto.builder()  \n        .horizontal(\"A\")  \n        .vertical(\"0\")  \n        .build();  \n    ExecutorService executor = Executors.newFixedThreadPool(16);  \n  \n    //when  \n    CountDownLatch latch = new CountDownLatch(tryCount);  \n    for (int i = 0; i < tryCount; i++) {  \n        int finalI = i;  \n        executor.submit(() -> {  \n            try {  \n                reservationService.createReservation(userId + finalI, concertId,  \n                    reservationRequestDto);  \n            } catch (Exception e) {  \n                log.error(e.getMessage());  \n            } finally {  \n                latch.countDown();  \n            }  \n        });  \n    }  \n    latch.await();  \n  \n    //then  \n    assertThat(reservationRepository.count()).isEqualTo(1);  \n}\n```\n\n![Pasted image 20240429173849](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/7ff90430-8eb7-43d6-8b06-673766b7b56b)\n\n해당 테스트는 @Sql을 이용해 빠른 데이터 삽입을 하고 있습니다. 이 때 ExecutorService를 사용해 멀티쓰레드 테스트를 하기만 하면 데이터 삽입이 되지 않았습니다.\n\n그런데 @Transactional 어노테이션만 지우면 테스트 데이터가 삽입되는 것을 발견했습니다.\n\n@Transactional과 멀티쓰레드를 같이 쓰면 어떤 현상이 생기길래 테스트 데이터가 삽입되지 않았을까요?\n\n## 트랜잭션 전파\n\n원인을 분석하기 전에 트랜잭션의 전파수준에 대해 잠시 짚고 가겠습니다.\n\n트랜잭션 전파속성(Transaction Propagation)이란 한 트랜잭션이 다른 트랜잭션에 어떻게 참여할지에 대한 설정입니다.\n\nSpring의 @Transactional은 아무 설정도 하지 않을 경우 기본적으로 Required입니다. 이 속성은 트랜잭션이 없다면 트랜잭션을 시작하고 이미 한 트랜잭션이 열려 있다면 열려있는 트랜잭션에 합류합니다.\n\n즉 2개의 논리적 트랜잭션(트랜잭션 매니저가 처리하는 트랜잭션)을 하나의 물리 트랜잭션(데이터 베이스 커넥션을 가져오고 커밋 or롤백 하는 단위)으로 만들 수 있습니다. \n\n![Pasted image 20240518225538](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/5c5788d5-7abb-4135-a9e4-2e673e9690d1)\n\n제가 테스트에서 사용하고 있는 @Sql의 [공식문서](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/test/context/jdbc/SqlConfig.TransactionMode.html)를 확인해봤습니다.\n\n> Using the resolved transaction manager and data source, SQL scripts will be executed within an existing transaction if present; otherwise, scripts will be executed in a new transaction that will be immediately committed. An existing transaction will typically be managed by the TransactionalTestExecutionListener).\n\nTransaction 모드를 명시하지 않을 시 Required와 같이 이미 생성된 트랜잭션에 합류한다는 것을 확인할 수 있습니다.\n\n## @Transactional과 멀티쓰레드\n\n그렇다면 왜 하필 @Transactional과 멀티쓰레드를 사용하는 테스트에서 이런 일이 일어날까요?\n실험을 위해 간단한 예제 프로젝트를 생성해 봤습니다.\n\n`Book`\n```java\n@Getter  \n@NoArgsConstructor(access = AccessLevel.PROTECTED)  \n@Entity  \n@Table(name = \"books\")  \npublic class Book {  \n  \n    @Id  \n    @GeneratedValue(strategy = GenerationType.IDENTITY)  \n    private Long id;  \n  \n    @Column(name = \"title\")  \n    private String title;  \n  \n    @Column(name = \"author\")  \n    private String author;  \n  \n    @Column(name = \"price\")  \n    private int price;  \n  \n    @Builder  \n    public Book(final Long id, final String title, final String author, final int price) {  \n        this.id = id;  \n        this.title = title;  \n        this.author = author;  \n        this.price = price;  \n    }  \n}\n```\n\n`BookService`\n```java\n@Service  \n@RequiredArgsConstructor  \npublic class BookService {  \n  \n    @Autowired  \n    private BookRepository bookRepository;  \n  \n\tpublic Book get(Long id) {  \n\t    return bookRepository.findById(id)  \n\t        .orElseThrow(IllegalArgumentException::new);  \n\t}\n}\n```\n\n`테스트 코드`\n```java\n@SpringBootTest  \nclass BookServiceTest {  \n  \n    @Autowired  \n    private BookService bookService;  \n  \n    @Test  \n    @Transactional    \n    @Sql(\"/sql/test-data.sql\")  \n    void transaction_test() {  \n        Long id = 1L;  \n        Book book = bookService.get(id);  \n        System.out.println(\"book.getId() = \" + book.getId());  \n        System.out.println(\"book.getTitle() = \" + book.getTitle());  \n        System.out.println(\"book.getAuthor() = \" + book.getAuthor());  \n        System.out.println(\"book.getPrice() = \" + book.getPrice());  \n        assertThat(book).isNotNull();  \n    }  \n}\n```\n\n`test-data.sql`\n```sql\ninsert into `books` (`id`, `title`, `author`, `price`)  \nvalues (1, 'The Great Gatsby', 'F. Scott Fitzgerald', 9.99),  \n       (2, 'The Catcher in the Rye', 'J.D. Salinger', 8.99);\n```\n\n![Pasted image 20240521170420](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/b47e5863-5f1f-4551-8ab2-d56528db2dc8)\n\n결과는 당연스럽게도(?) 통과입니다.\n\n## 멀티쓰레드 테스트\n\n그럼 멀티쓰레드를 사용하는 테스트를 작성해볼까요? \n\n```java\n@Test  \n@Transactional  \n@Sql(\"/sql/test-data.sql\")  \nvoid multi_thread_test() {  \n    boolean outerTransactionActive = TransactionSynchronizationManager.isActualTransactionActive();  \n    System.out.println(\"outerTransactionActive = \" + outerTransactionActive);  \n    Long id = 1L;  \n    Book book = bookService.get(id);  \n    System.out.println(\"book.getId() = \" + book.getId());  \n    System.out.println(\"book.getTitle() = \" + book.getTitle());  \n    System.out.println(\"book.getAuthor() = \" + book.getAuthor());  \n    System.out.println(\"book.getPrice() = \" + book.getPrice());  \n  \n    int count = 5;  \n    ExecutorService executorService = Executors.newFixedThreadPool(16);  \n    CountDownLatch latch = new CountDownLatch(5);  \n  \n    for (int i = 0; i < count; i++) {  \n        executorService.execute(() -> {  \n            boolean innerTransactionActive = TransactionSynchronizationManager.isActualTransactionActive();  \n            System.out.println(\"innerTransactionActive = \" + innerTransactionActive);  \n            Book book1 = bookService.get(id);  \n            System.out.println(\"book1.getId() = \" + book1.getId());  \n            System.out.println(\"book1.getTitle() = \" + book1.getTitle());  \n            System.out.println(\"book1.getAuthor() = \" + book1.getAuthor());  \n            System.out.println(\"book1.getPrice() = \" + book1.getPrice());  \n            latch.countDown();  \n        });  \n    }  \n}\n```\n\n생성된 테스트 데이터를 bookSerivce의 get으로 멀티쓰레드에서 읽어낼 수 있는지 확인하고\n@Transactional로 제공한 트랜잭션 환경이 멀티쓰레드에도 적용되는지 확인하기 위해 `TransactionSynchronizationManager`를 사용했습니다.\n\n테스트를 돌려보면\n![Pasted image 20240521172946](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/9e69852f-5117-47dc-9c09-1cc0b76d2cbd)\n멀티쓰레드가 생성되기 이전에 이미 book을 찾을 수 있지만\n\n![Pasted image 20240521172216](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/58600676-6cb1-404e-9002-1ee3ff05ae1a)\n\n보시는 것처럼 멀티쓰레드 내에서는 `orElseThrow()` 에 의해 예외가 일어나는 것을 알 수 있습니다.\n\n또한 멀티쓰레드에는 Transaction이 적용되지 않는 것을 확인할 수 있습니다.\n![Pasted image 20240521173005](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/10430003-336e-4f2d-a6f7-d70aa25b3fc1)\n![Pasted image 20240521173017](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/38b0edf3-b269-42d8-a097-e3c1edddaf8f)\n\n따라서 테스트 코드에서 @Transactional로 트랜잭션 환경을 제공해도 테스트내에서 생성된 쓰레드들은 이에 영향을 받지 않고 독자적으로 트랜잭션을 생성한다는 것을 확인할 수 있습니다.\n\n이 사실을 @SqlConfig의 전파 기본속성과 같이 묶어서 생각해본다면\n\n1. @Transactional을 선언한 테스트를 실행하는 main쓰레드가 트랜잭션을 시작함\n2. @Sql는 이미 열려 있는 main쓰레드의 트랜잭션에 참여함. 즉 main쓰레드의 영속성 컨텍스트의 1차캐시와 쓰기 지연 저장소에 insert쿼리가 저장됨. \n3. 테스트 내에서 ExecutorService로 생성된 쓰레드들은 main과 독자적인 쓰레드들이므로 main쓰레드의 영속성 컨텍스트를 공유하지 않음\n4. 따라서 멀티쓰레드들은 테스트 데이터를 읽을 수 없음\n\n즉, @Sql의 기본 전파 속성을 간과하고 @Transactional을 적용했기 때문에 발생한 문제였습니다.\n@Sql이 main쓰레드의 트랜잭션에 참여하게 되어 원래라면 바로 commit되었을텐데 이게 테스트가 끝나는 시점으로 미루어졌기 때문에 다른 스레드들이 데이터를 읽을 수 없게 된 것이었습니다. \n\n이렇게 문제를 분석하고, 이를 해결할 수 있는 방법을 찾아보았습니다.\n\n## 해결방법 1. @Transactional을 테스트에서 사용하지 않는다\n\n간단하게 @Transactioanl을 사용하지 않으면 @Sql은 제가 원래 생각했던 대로 작동할 것입니다. \n\n@Transactional을 테스트에 사용하는 이유는 테스트가 끝날 때마다 자동으로 데이터를 롤백해주기 때문인데, 이러한 처리를 수동으로 한다면 다른 테스트에 영향을 미치지 않을 것입니다.\n\n(참고로, 테스트에 @Transactional을 사용하는 것에 대해 의견이 다른 경우가 있는데 자세한 내용은 향로님의 [블로그](https://jojoldu.tistory.com/761)에 적혀있습니다.)\n\n따라서 모든 테이블을 truncate하는 쿼리를 실행한다면 테스트 데이터가 다른 테스트에 영향을 주지 않을 것입니다. 해당 부분을 적용하는 방법은 저의 이전 [테스트 격리](https://jinkshower.github.io/sprintboottest_isolation/)글에서 코드와 함께 설명했습니다.\n\n## 해결방법 2. @Sql의 트랜잭션 전파 수준을 변경한다\n\n@Sql의 공식문서는 @SqlConfig에서 TransactionMode를 명시함으로써 해당 sql 스크립트의 전파수준을 바꿀 수 있다고 말합니다. \n\n이 TransactionMode를 ISOLATED로 설정하면 즉시 커밋되는 새롭고 고립된 트랜잭션 내에서 실행될 수 있다고 합니다.\n\n```java\n@Sql(scripts = \"/sql/test-data.sql\",   \n    config = @SqlConfig(transactionMode = TransactionMode.ISOLATED))\n```\n이를 적용하고 테스트를 돌려보면\n\n![Pasted image 20240521195056](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/cbb9cc9e-0a8d-45db-a82f-edd8e1bc1d60)\n\n생성된 쓰레드들에서도 db의 테스트 데이터를 읽음을 확인할 수 있습니다. \n\n"},{"excerpt":"학습계기 운영중 서버의 상태와 비정상 수치를 추적하고 이를 시각화하여 팀원과 공유할 필요성을 인지하게 되어 모니터링 시스템을 구축하게 되었습니다. 이 중 Spring Boot 프로젝트와 쉽게 연동할 수 있고, 레퍼런스가 많고 무엇보다 인 Actuator-Prometheus-Grafana를 선택하게 되었습니다. 모니터링 과정 Pasted image 2024…","fields":{"slug":"/monitoring/"},"frontmatter":{"date":"May 13, 2024","title":"Actuator, Prometheus, Grafana로 모니터링 환경을 구축해보자","tags":["actuator","prometheus","grafana"]},"rawMarkdownBody":"\n## 학습계기\n\n운영중 서버의 상태와 비정상 수치를 추적하고 이를 시각화하여 팀원과 공유할 필요성을 인지하게 되어 모니터링 시스템을 구축하게 되었습니다.\n\n이 중 Spring Boot 프로젝트와 쉽게 연동할 수 있고, 레퍼런스가 많고 무엇보다 `무료`인 Actuator-Prometheus-Grafana를 선택하게 되었습니다.\n\n## 모니터링 과정\n\n![Pasted image 20240425111334](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/1b2879f0-b7ec-4865-8121-07dffea1cf3c)\n(출처: https://wildeveloperetrain.tistory.com/309)\n\n1. SpringBoot의 Actuator가 실행중인 애플리케이션의 다양한 내부 정보들을 Micrometer라는 라이브러리를 통해 Prometheus가 쓸 수 있는 메트릭으로 수집하고 \n2. Prometheus가 이를 주기적으로 pull하여 쿼리할 수 있는 시계열 데이터(시간에 따라 저장된 데이터)로 가공하고 \n3. Grafana는 사용자가 쉽게 볼 수 있게 시각화하는 역할을 합니다.\n\n`localhost:8080을 기준으로 진행되는 기록입니다.`\n\n## Actuator\n\nActuator는 Spring Boot의 서브 프로젝트로 간단하게 빌드에 의존성을 추가하는 것만으로도 활성화 할 수 있습니다.\n\n앞서 말씀드린 Micrometer라이브러리도 메트릭 수집에 필요하니 의존성을 추가해주어야 합니다.\n\n```java\nimplementation 'org.springframework.boot:spring-boot-starter-actuator'  \nruntimeOnly 'io.micrometer:micrometer-registry-prometheus'\n```\n\n해당 의존성을 추가하고 빌드를 하고 `localhost:8080/actuator`로 접속하면 Actuator가 제공하는 기본 엔드포인트를 확인할 수 있습니다.\n\n![Pasted image 20240425113607](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/3751e1ee-f591-4db9-b977-9881447becad)\n\n`actuator/health`로 접속해보면\n\n![Pasted image 20240425114118](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/925eb0be-788b-42f7-b6b8-e49f00ff9293)\n\n으로 현재 애플리케이션이 동작 중이라는 정보를 확인할 수 있습니다. \n\n### 주의점\n\n운영 서버에서 Actuator를 사용할 시 불필요한 엔드포인트를 활성화하면 중요 환경변수, 메모리 정보가 노출될 수 있고 Shutdown Endpoint는 애플리케이션을 중지시킬 수 있기 때문에 기본적으로 모든 엔드포인트를 disabled로 두고 필요한 엔드포인트만 화이트리스트로 운영하는 것이 추천됩니다.\n\n따라서 모든 엔드포인트를 disabled하고 prometheus에 대한 엔드포인트만 열도록 설정해주었습니다.\n\n```java\nmanagement.endpoints.enabled-by-default = false //모든 엔드포인트 disabled\nmanagement.endpoint.prometheus.enabled=true //prometheus enable\nmanagement.endpoints.web.exposure.include = prometheus //노출할 endpoint 명시\n```\n\n스프링 시큐리티를 사용하신다면 `/actuator/prometheus`라는 엔드포인트가 노출되었으니 관리자 권한으로 해당 엔드포인트에 인가 처리를 하시면 좀 더 보안성이 올라갈 것이라고 생각됩니다.\n\n```java\n.antMatchers(\"/actuator/**\").hasRole(\"ADMIN\")\n```\n\n이제 `/actuator/prometheus`로 접속을 하면\n\n![Pasted image 20240425120718](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/8420b5ba-bab2-4a1e-9b74-7da5a9c704a1)\n이렇게 애플리케이션의 내부정보들이 메트릭으로 수집된 것을 확인할 수 있습니다.\n\n## Prometheus\n\n이제 Prometheus가 위의 엔드포인트에 접속하여 메트릭을 pull할 수 있게 하면 됩니다.\n\n`docker로 진행됩니다`\n\n### docker로 Prometheus 설치\n\nPrometheus를 docker로 설치할 때 Prometheus가 어느 엔드포인트에서 어느 주기로 메트릭을 수집(scrape)할지 미리 설정해주어야 합니다.\n\ndocker로 Prometheus를 설치할 디렉토리를 만들고 해당 폴더에 `prometheus.yml`파일을 작성합니다.\n\n```yaml\nglobal:\n  scrape_interval:     15s //15초 간격으로 수집\n\nscrape_configs:\n  - job_name: 'prometheus'\n    metrics_path: '/actuator/prometheus' //메트릭 수집 경로\n    static_configs:\n      - targets: [ '<host>:<port>' ] //메트릭 수집 호스트 정보\n```\n\n15초 간격으로 수집하고 `/actuator/prometheus`로 메트릭 수집경로를 명시하고 `targets`에 호스트와 포트 정보를 명시해줍니다.\n\n이제 해당 폴더에서 docker 컨테이너로 Prometheus를 띄우면 됩니다.\n\n```bash\ndocker run --name prometheus -d -p 9090:9090 -v <prometheus.yml이 있는 경로>:/etc/prometheus/prometheus.yml prom/prometheus\n```\n\n저는 해당 폴더에서 실행했기 때문에 경로 부분을 `$(pwd)`로 했지만 환경에 맞춰서 사용하시면 됩니다.\n\n이제 9090포트로 접속하면!\n\n![Pasted image 20240425125109](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/4f2603b2-57fb-42dd-8683-564a95abdcdc)\nPrometheus가 실행되는 것을 확인할 수 있습니다.\n\n - Status-> Targets에서 endpoint와의 연결이 `up`인 것을 확인 해주셔야 합니다.\n\n - 트러블 슈팅\n-> prometheus.yml 파일의 호스트를 localhost로 명시하면 Prometheus가 연결에 실패하여 `host.docker.internal`로 바꾸어 주었습니다. [참고](https://www.inflearn.com/questions/1030769/docker%EB%A1%9C-prometheus-grafana-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EA%B2%BD%EC%9A%B0-%EC%84%A4%EC%A0%95-%EA%B0%80%EC%9D%B4%EB%93%9C)\n\n## Grafana\n\n이제 Prometheus가 수집하고 가공한 데이터를 Grafana에서 시각화할 차례입니다.\n\nGrafana를 일단 docker에서 컨테이너로 실행해야겠죠?\n\n````bash\ndocker run --name grafana -d -p 80:3000 grafana/grafana\n````\n\nGrafana 이미지를 80포트의 docker 컨테이너로 띄우는 명령입니다.\n\nGrafana\n\n80포트로 접속하면 Grafana의 로그인 화면이 보이는데요,\n기본 id는 admin, password도 admin으로 되어 있습니다.(password는 환경에 맞게 변경할 수 있습니다.)\n\n이제 Grafana에 접속하여 Prometheus 연결 설정을 해주어야 합니다.\n\n왼쪽 패널의 Connections에서 Data source를 클릭하고\n![Pasted image 20240425142203](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/3f41091e-e8a1-423f-a3d9-450afa3af239)\n\nAdd new Datasource를 클릭하고 type으로 Prometheus를 선택합니다.\n![Pasted image 20240425151724](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/e1d45327-e877-4c08-b7c3-d1e613b39b4e)\n\nConnection 부분에 Promethus의 접속 url을 입력합니다.\n![Pasted image 20240425151746](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/2a1bb74d-7452-4dc1-b410-f5afc51234fd)\n- 여기서도 docker로 사용할 시 `host.docker.internal`로 호스트를 명시해줘야 합니다.\n\nSave & Test를 클릭하여 접속이 되는지 확인합니다.\n![Pasted image 20240425151803](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/f36e8047-bf7b-4f6f-9012-e292de7f1d5d)\n\n이제 연결한 데이터를 시각화할 DashBoard를 생성하면 됩니다.\n\n왼쪽 패널의 Dashboards를 선택하고 New를 클릭합니다\n![Pasted image 20240425151841](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/22f89528-723c-4e08-b617-15c5c62ec711)\n\n이미 만들어진 Dashboard를 편리하게 사용하기 위해 Import Dashboard를 클릭하고\n![Pasted image 20240425151925](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/b9caa617-3c3c-496f-b611-57e082eb4430)\n\n가장 다운로드 수가 많은 JVM Dashboard를 사용하기 위해 4701을 Load합니다\n-> 잘 만들어진 다른 Dashboard를 사용하고 싶다면 바로 위의 링크로 들어가셔서 Dashboard를 선택할 수 있습니다.\n\n![Pasted image 20240425152009](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/76ba4f80-f3d6-460b-aea1-e8249e48e676)\n\n그리고 Dashboard에 연결할 Datasource를 설정하고, Import를 누르면!\n![Pasted image 20240425152043](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/5b71a463-3690-41e9-94f5-b899365f034d)\n\n\n![Pasted image 20240425143900](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/73edf3db-1da5-469e-b16f-d5d9974264f5)\n\n이렇게 Prometheus가 가공한 데이터를 시각화하여 볼 수 있습니다.\n\n---\n참고\n\nhttps://techblog.woowahan.com/9232/"},{"excerpt":"프로젝트에 적용한 Pull Request 링크 학습 계기 팀원분이 ci과정에서 계속 빌드가 실패한다고 해당 오류에 대한 이슈를 제기해주셨습니다. 로그를 살펴보았는데요, 테스트에서 Redis가 없어서 일어난 오류임을 인지하고 CI 과정에서 Redis설치하는 flow를 추가해서 문제를 해결했습니다. 간단한 오류였고 간단한 해결이었지만 찝찝했는데요, Redis…","fields":{"slug":"/technology_free_code/"},"frontmatter":{"date":"May 07, 2024","title":"기술변경에 확장성을 가진 리팩토링","tags":["refactoring","interface","test"]},"rawMarkdownBody":"\n## 프로젝트에 적용한 Pull Request\n\n[링크](https://github.com/lay-down-coding/tickitecking/pull/45)\n\n## 학습 계기\n\n팀원분이 ci과정에서 계속 빌드가 실패한다고 해당 오류에 대한 이슈를 제기해주셨습니다.\n\n로그를 살펴보았는데요, 테스트에서 Redis가 없어서 일어난 오류임을 인지하고 CI 과정에서 Redis설치하는 flow를 추가해서 문제를 해결했습니다.\n\n간단한 오류였고 간단한 해결이었지만 찝찝했는데요, Redis가 아닌 다른 기술을 사용하게 된다면? 그 땐 Redis를 설치하는 ci과정을 삭제하고 다른 기술 환경을 마련해야 할까요? 거기서 에러가 발생하면 다시 디버깅을 하고요? \n\n해당 오류를 해결하면서 좀 더 근본적인 문제는 기술에 의존적인 코드을 작성하고 있었기 때문임을 인지했습니다.\n\n따라서 리팩토링을 통해 기술 의존적인 코드를 개선한 기록을 공유하고자 합니다.\n\n## 구현체에 의존하는 코드\n\n(코드가 갑자기 나와 이해가 어려우신 분은 [이전 글](https://jinkshower.github.io/ticket_reservation_concurrency/) 을 참고하시면 더 이해가 쉬울 것 같습니다!)\n```java\npublic class ReservationService {\n\n\tprivate final RedisTemplate<String, String> redisTemplate;\n\n\tprivate Boolean isTaken(Long concertId, String horizontal, String vertical) {  \n    String key = concertId + horizontal + vertical;  \n    return Boolean.FALSE.equals(  \n        redisTemplate.opsForValue().setIfAbsent(key, \"reserved\"));  \n\t}\n}\n\n```\n예매에서 Redis를 이용하여 중복된 요청을 막는 메서드입니다. \nRedis를 spring에서 이용하기 위해 빈으로 등록된 RedisTemplate를 주입받아서 사용하고 있습니다.\n\n해당 코드의 문제점은 무엇일까요? \n비즈니스 로직이 RedisTemplate라는 구체적인 구현체와 강하게 결합하고 있다는 점입니다. \n\n중복 예매 생성을 막는 기술이 Redis가 아니라 MongoDB가 된다면 이에 따라 도메인 내 코드를 수정해야 합니다. 만약 RedisTemplate를 쓰는 곳이 1000곳이라면 1000곳의 코드를 모두 수정해야 하겠죠.\n\n즉, 객체지향 5대 원칙 중 \n\nOCP(Open-Closed Principle) 확장에는 열려있고 수정에는 닫혀 있는 코드 \n(Redis -> MongoDb의 경우 수정이 엄청 일어나야 함)\n\nDIP(Dependency Inversion Principle) 고수준 모듈은 저수준 모듈의 구현에 의존해서는 안된다 \n(고수준의 Service모듈이 구체적인 RedisTemplate에 의존하고 있음)\n\n는 원칙을 위배하고 있는 코드입니다.\n\n## 기술 의존적인 메서드를 추상화하기 \n\n로직을 수행하는 메서드를 다시 살펴보고 추상화 할 수 있는 부분을 짚어보았습니다.\n\n1. 예매 생성 시 중복된 key, value이면 다른 리턴값을 보내야 한다 \n2. 예매 취소 시 key-value는 삭제 되어야 한다.\n\n이를 interface로 적용해보았습니다. \n\n```java\npublic interface DuplicatedReservationCheck {  \n  \n    Boolean isDuplicated(String key, String value);  \n  \n    void delete(String key, String value);  \n}\n```\n\n해당 인터페이스의 구현체에서 구체적인 로직을 작성하면 됩니다.\n\n```java\npublic class DuplicatedReservationCheckImpl implements DuplicatedReservationCheck {  \n  \n    private final RedisTemplate<String, Object> redisTemplate;  \n  \n    @Override  \n    public Boolean isDuplicated(String key, String value) {  \n\t    return Boolean.FALSE.equals(  \n\t        redisTemplate.opsForValue().setIfAbsent(key, \"reserved\")); \n    }  \n  \n    @Override  \n    public void delete(String key, String value) {  \n        redisTemplate.opsForSet().remove(key, value);  \n    }  \n}\n```\n\n이제 서비스는 RedisTemplate라는 구체적인 구현체를 모르게 됩니다. \n\n```java\npublic class ReservationService {\n\n\tprivate final DuplicatedReservationCheck duplicatedReservationCheck;\n\t\n    @Override  \n    public Boolean isDuplicated(String key, String value) {  \n\t    return duplicatedReservationCheck.isDuplicated(key, value); \n    }  \n  \n    @Override  \n    public void deleteValue(String key, String value) {  \n        duplicatedReservationCheck.delete(key, value);  \n    }\n}\n```\n\n이 리팩토링이 가지는 장점은 무엇일까요?\n\n이제 Service는 중복체크가 어떻게 실행되는지 구체적으로 알 필요가 없어집니다. \n즉, 다른 구현체가 와도 리턴 값만 확인해주면 되기 때문에 Redis가 MongoDb가 되든, Memcached가 되든 Service의 코드에 변경점이 생기지 않습니다. \n\n## 테스트에 적용하기\n\n중복 예매 체크가 interface를 구현한 구현체라면 어떤 것이든 가능해졌기 때문에 테스트에서도 Redis를 실행해야할 필요가 없어졌습니다. 저희가 원하는 테스트 더블을 사용하는 것도 가능해졌습니다.\n\n```java\npublic class MockDuplicatedReservationCheck implements DuplicatedReservationCheck {  \n  \n    private final Map<String, Set<String>> store = new ConcurrentHashMap<>();  \n  \n    @Override  \n    public synchronized Boolean isDuplicated(String key, String value) {  \n        store.putIfAbsent(key, new HashSet<>());  \n        return !store.get(key).add(value);  \n    }  \n  \n    @Override  \n    public void deleteValue(String key, String value) {  \n        Set<String> values = store.get(key);  \n        if (values == null) {  \n            throw new IllegalArgumentException();  \n        }  \n        values.remove(value);  \n        if (values.isEmpty()) {  \n            store.remove(key);  \n        }  \n    }  \n}\n```\n\n해당 객체를 여러 테스트에서도 쉽게 사용할 수 있게 테스트용 Bean으로 주입하는 Configuration클래스를 만들었습니다.\n\n```java\n@TestConfiguration  \npublic abstract class TestConfiguration {  \n  \n    @Bean  \n    public DuplicatedReservationCheck duplicatedReservationCheck() {  \n        return new MockDuplicatedReservationCheck();  \n    }  \n}\n```\n\n이제 원하는 통합테스트에 @Import문으로 해당 Configuration을 사용하게 해줄 수 있습니다.\n\n## CI 빌드 속도 개선\n\n이제 CI 빌드 과정에서 Redis를 설치할 필요가 없어졌습니다.\n\n![Pasted image 20240415174857](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/0a40aa1d-b6f9-4b58-b96f-2d1d8b154da9)\n\n해당 부분을 삭제하고 빌드 속도가 40초 정도 개선되는 효과를 볼 수 있었습니다. \n\n## 마치며\n\n간단한 오류로 시작된 리팩토링이었지만 오류를 해결하면서 성능 개선까지 얻게 된 값진 경험이었습니다.\n하지만 interface가 만능은 아니라고 생각합니다. 무분별한 추상화는 복잡한 코드와 다량의 클래스를 낳을 수 있기 때문입니다.\n\n또한 테스트 부분은 고민이 있는데요, 현재 프로덕션 코드에서 Redis를 사용하고 있기 때문에 이를 더블로 대체하는 것이 좋은 방법이었는가는 물음표가 띄워집니다. 해당 부분은 좀 더 기술을 사용하는 학습을 진행하며 천천히 고민을 즐겨보려 합니다.\n\n---\n\n참고 \n\nhttps://tecoble.techcourse.co.kr/post/2021-11-21-dip/\n"},{"excerpt":"학습 계기 부하테스트를 진행하던 중 읽기와 쓰기 요청이 동시에 수행될 때 읽기 속도가 현저히 줄어드는 현상 발견하게 되었습니다.  쿼리 최적화를 수행하고 인덱스를 조정해보았지만 결국 DB에서의 병목 현상을 해결할 필요가 있음을 인지하게 되었습니다.  이에 따라 데이터베이스 레플리케이션을 학습하고 적용한 기록을 공유하고자 합니다. 데이터베이스 레플리케이션 …","fields":{"slug":"/db_replication/"},"frontmatter":{"date":"May 03, 2024","title":"알아보고 적용하는 DB Replication","tags":["MySQL","replication"]},"rawMarkdownBody":"\n## 학습 계기\n\n부하테스트를 진행하던 중 읽기와 쓰기 요청이 동시에 수행될 때 읽기 속도가 현저히 줄어드는 현상 발견하게 되었습니다. \n\n쿼리 최적화를 수행하고 인덱스를 조정해보았지만 결국 DB에서의 병목 현상을 해결할 필요가 있음을 인지하게 되었습니다. \n\n이에 따라 데이터베이스 레플리케이션을 학습하고 적용한 기록을 공유하고자 합니다.\n\n## 데이터베이스 레플리케이션\n\n데이터베이스가 하나뿐이라면 해당 데이터베이스에서 장애가 발생했을 시 데이터가 유실되고 사용자에게 장애 시간동안 서비스를 제공할 수 없게 됩니다.\n\n따라서 Fail Over(장애 대응)을 위한 대비책으로 Scale-up이나 Scale-out을 선택하게 됩니다.\n\n하지만 Scale up 만으로는 단일 장애지점(SPOF)이 하나의 데이터베이스에 그대로 있으므로 자연스레 Scale-out으로 SPOF를 분산하는 방법이 추천됩니다.\n(물론 엄청 좋은 데이터 베이스가 엄청 많이 있으면 더 좋겠죠 )\n\n보통의 애플리케이션에서 사용자 요청은 읽기가 많은 비율을 차지하기 때문에 데이터베이스를 2개 이상으로 만든 상태에서 하나의 서버는 Write만을 담당하고 나머지 서버는 Read를 담당하게 한다면 성능 향상과 읽기 요청의 분산을 꾀할 수 있습니다.\n\n즉, Replication(복제)를 통해 데이터베이스를 Scale-out(여러대 두기)하고 복제된 서버들에 읽기/쓰기 역할을 담당하게 하는 것이 데이터베이스 레플리케이션입니다.\n\n## 데이터 동기화\n\n복제라는 이름에서 알 수 있듯이 레플리케이션에서 중요한 부분은 데이터의 동기화입니다. 쓰기 요청이 이루어지지 않은 데이터베이스를 복제하면 당연히 읽기 데이터베이스도 같은 데이터를 가지겠지만 웹 애플리케이션은 실시간으로 읽기와 쓰기가 반복되기 때문입니다.\n\n제가 공부하고 있는 MySQL에서 어떻게 레플리케이션이 이루어지는지 학습해봤습니다.\n\n### MySQL의 레플리케이션\n\n`원본 데이터 서버는 소스 서버, 복제된 데이터를 가진 서버는 레플리카 서버라고 명칭합니다`\n\n`MySQL에서 발생하는 모든 변경사항(이벤트)은 바이너리 로그에 순서대로 기록되고 이 바이너리 로그를 레플리카 서버가 받아서 데이터에 반영합니다(동기화)`\n\n![Pasted image 20240424175342](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/ef209c51-a550-44db-b005-3121e2d14561)\n\n 해당 그림에 나온 요소들을 간략히 설명하려 합니다.\n\n- 트랜잭션 처리 스레드\nSQL쿼리를 실행하고 소스 서버에 데이터를 적용합니다. 작업한 내용을 바이너리 로그에 기록합니다.\n\n- 바이너리 로그\nMySQL 서버에서 일어난 모든 사항을 기록하는 로그 파일\n\n- 바이너리 로그 덤프 쓰레드\n바이너리 로그 -> 레플리카 서버로 전송하는 역할\n\n- 레플리케이션 I/O스레드\n복제가 시작되면 생성되어 바이너리 로그 덤프 쓰레드에서 바이너리 로그를 받아 릴레이 로그에 저장하고 사라집니다.\n\n- 릴레이로그\n소스 서버로부터 읽어온 바이너리 로그를 저장하는 로그 파일\n\n- 레플리케이션 SQL 쓰레드\n릴레이 로그의 이벤트를 실행하여 레플리카 서버 데이터에 반영\n\n이에 따라 소스 서버에 write쿼리가 오면\n\n1. 트랜잭션 처리 스레드가 쿼리를 실행, 소스 서버 데이터에 반영하고 \n2. 이를 바이너리 로그에 기록하고\n3. 바이너리 덤프 쓰레드가 이를 레플리카 서버로 전송하고\n4. 레플리케이션 I/O쓰레드가 바이너리 로그를 받아 릴레이로그에 저장하고\n5. 레플리케이션 SQL 쓰레드가 릴레이로그의 이벤트를 실행합니다\n\n## SpringBoot 프로젝트에 적용\n\n레플리케이션의 원리를 학습했으니, 프로젝트에 적용하기 위해 AWS RDS의 Read Replica를 생성하였습니다.\n\n[해당 글](https://velog.io/@skyjoon34/MySQL-RDS-%EC%9D%BD%EA%B8%B0%EC%A0%84%EC%9A%A9-replica-%EC%A0%81%EC%9A%A9)을 참고하여 DataSource를 @Transactional이 readOnly인지에 따라 분기처리하는 코드를 작성해봤습니다.\n\n```java\n@Configuration\npublic abstract class DataSourceConfiguration {\n\n\tprivate static final String SOURCE_SERVER = \"\bSOURCE\";\n\tprivate static final String REPLICA_SERVER = \"REPLICA\";\n\n\t@Bean\n\t@Qualifier(SOURCE_SERVER)\n\t@ConfigurationProperties(prefix = \"spring.datasource.source\")//(1)\n\tpublic DataSource sourceDataSource() {\n\t\treturn DataSourceBuilder.create()\n\t\t\t.build();\n\t}\n\n\t@Bean\n\t@Qualifier(REPLICA_SERVER)\n\t@ConfigurationProperties(prefix = \"spring.datasource.replica\")//(2)\n\tpublic DataSource replicaDataSource() {\n\t\treturn DataSourceBuilder.create()\n\t\t\t.build();\n\t}\n\n\t@Bean\n\tpublic DataSource routingDataSource(\n\t\t@Qualifier(SOURCE_SERVER) DataSource sourceDataSource, \n\t\t@Qualifier(REPLICA_SERVER) DataSource replicaDataSource //(3)\n\t) {\n\t\tRoutingDataSource routingDataSource = new RoutingDataSource(); \n\n\t\tHashMap<Object, Object> dataSourceMap = new HashMap<>(); \n\t\tdataSourceMap.put(\"source\", sourceDataSource);\n\t\tdataSourceMap.put(\"replica\", replicaDataSource);\n\n\t\troutingDataSource.setTargetDataSources(dataSourceMap); \n\t\troutingDataSource.setDefaultTargetDataSource(sourceDataSource); \n\n\t\treturn routingDataSource;\n\t}\n\n\t@Bean\n\t@Primary\n\tpublic DataSource dataSource() {//(4)\n\t\tDataSource determinedDataSource = routingDataSource(sourceDataSource(), replicaDataSource());\n\t\treturn new LazyConnectionDataSourceProxy(determinedDataSource);\n\t}\n}\n\n@Slf4j\npublic class RoutingDataSource extends AbstractRoutingDataSource { //(5)\n\n\t@Override\n\tprotected Object determineCurrentLookupKey() {\n\t\tString lookupKey = TransactionSynchronizationManager.isCurrentTransactionReadOnly() ? \"replica\" : \"source\";\n\t\tlog.info(\"Current DataSource is {}\", lookupKey);\n\t\treturn lookupKey;\n\t}\n}\n```\n\n\n코드를 설명하겠습니다.\n\n(1) & (2) : DataSource를 직접 2개로 만들어 주기 위해 `@ConfigurationProperties`를 사용하여 application.properties의 prefix(Source, Replica)에 명시한 정보로 Datasource를 빈으로 등록합니다.\nDatasource가 2개 이므로 spring에게 어떠한 빈을 사용할 지 명시하기 위해 `@Quailifier` 를 사용했습니다.\n\n(5) : Spring의 데이터 베이스 라우팅을 위해 사용되는 `AbstractRoutingDataSource`를 상속받아 커스텀한 구현체 입니다. `TransactionSynchronizationManager`는 현재 트랜잭션이 읽기 전용인지 구분하고 읽기 전용이면 replica, 아니면 source를 반환합니다.\n\n(3) : (5)의 구현체를 생성하고, (1),(2)에서 생성한 각각의 DataSource를 파라미터로 받아 사용합니다. \nMap의 구현체에 저희가 (5)에서 정한 key와 파라미터의 DataSource를 넣고 `setTargetDataSources()`의 인자로 만든 Map을 넣어 줍니다.\n\n이 메서드는 내부적으로 `determineTargetDataSource()`를 호출합니다.\n![Pasted image 20240429165757](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/09d6de71-f5d4-4dd2-b0ef-4112557e56b7)\n\n`determineTargetDataSource()`는 저희가 오버라이딩한 `determineCurrentLookupKey()` 를 사용하고 `lookupKey` 로 사용할 DataSource를 get하고 반환하는 것을 확인할 수 있습니다. \n\n(4) : Spring은 트랜잭션에 진입하는 순간 DB Connection을 가져옵니다. 이때 Ehcache같은 캐시를 사용하거나 영속성 컨텍스트의 1차캐시에 있는 정보를 가지고 올 때 불필요한 데이터베이스 풀의 커넥션을 점유할 수 있고, 트랜잭션에 진입한 이후 DataSource를 결정해야 할때 (저희의 경우 입니다) 미리 DataSource를 결정해버리면 분기를 나눌수가 없습니다.\n\n따라서 실제로 커넥션이 필요한 경우에만 커넥션을 점유할 수 있게 프록시 객체를 @Primary로 먼저 반환하고  `getConnection()`으로 실제 DataSource를 가져올 때 사용될 수 있도록 하는 설정입니다.\n\n## 속도 측정\n\n이제 레플리케이션의 효과를 측정해야겠죠?\n\n(100명이 5초간격으로 2분간 조회와 쓰기 요청을 동시에 보낸 경우)\nDB replication 적용 전의 읽기 성능\n![Pasted image 20240412194411](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/e42383b6-81a3-4e75-9d58-16ee5bc21b79)\n\n적용 후 읽기 성능\n![Pasted image 20240412193344](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/8d9d7d62-90e3-4d7b-be97-d6302d8f1a3d)\n\n응답시간과 TPS에서 60프로 정도의 성능 개선이 일어남을 확인할 수 있었습니다.\n\n---\n참고\n\nReal MySQL 8.0 - 백은빈, 이성욱\n\nhttps://sup2is.github.io/2021/07/08/lazy-connection-datasource-proxy.html"},{"excerpt":"프로젝트에 적용한 Pull Request 링크 학습계기 프로젝트를 진행하며 api마다 부하테스트로 성능을 측정하고 있었습니다. 이 중 가장 조회가 많을 것이라 예상되는 좌석 조회 api가 너무나 성능이 떨어지는 충격적인 결과를 보게 되었는데요.. (2분동안 100명이 10초 간격으로 조회요청시)\nPasted image 20240417170414\n오류율 2…","fields":{"slug":"/api_performance_improvment/"},"frontmatter":{"date":"April 27, 2024","title":"조회 API 성능 개선","tags":["performanceimprovement","index","caching","querytuning"]},"rawMarkdownBody":"\n## 프로젝트에 적용한 Pull Request\n\n[링크](https://github.com/lay-down-coding/tickitecking/pull/48)\n\n## 학습계기\n\n프로젝트를 진행하며 api마다 부하테스트로 성능을 측정하고 있었습니다. 이 중 가장 조회가 많을 것이라 예상되는 좌석 조회 api가 너무나 성능이 떨어지는 충격적인 결과를 보게 되었는데요..\n\n(2분동안 100명이 10초 간격으로 조회요청시)\n![Pasted image 20240417170414](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/69471f59-4e86-4ea5-9345-926f8232f240)\n오류율 25%, tps 2.9, 평균 응답시간 3만대의 처참한 결과를 받게 되었습니다.\n\n이번 기록은 해당 api를 차근차근 개선하면서 배운 점과 성능 기록을 다루려고 합니다.\n\n## 원인 찾기\n\n좌석 조회 api가 이렇게 느린 이유를 찾는 것이 우선이라고 생각했습니다.\n해당 메서드를 따라가면서 for문이 돌고 있는 건 아닌지, 불필요한 객체 생성을 하는지 점검 하고 이내 쿼리를 찾게 되었습니다.\n\n```sql\nselect  \n\ts1_0.horizontal,  \n\ts1_0.vertical  \nfrom  \n\tseats s1_0  \nwhere  \n\ts1_0.concert_id=?  \n  and s1_0.reserved=?;\n\nselect\n\ts1_0.horizontal,\n\ts1_0.vertical\nfrom\n\tseats s1_0\nwhere\n\ts1_0.concert_id=?\n and s1_0.availability=?;\n```\n\n예약이 된 좌석의 행열, 사용불가능한 좌석의 행열을 찾는 2번의 쿼리를 실행중이었는데요, 해당 쿼리의 실행계획을 확인했습니다.\n\n(seats 테이블에 데이터가 10만개 있을 때)\n![Pasted image 20240417121808](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/0f790ca2-1d64-4169-b0fd-ba409e438588)\n테이블 풀 스캔을 한 것을 확인할 수 있습니다.\n\n즉, 10만개의 데이터를 풀스캔하는 쿼리 2개가 처참한 성능의 원인임을 확인했습니다.\n\n## 1차 개선 - 쿼리개선\n\n가장 먼저 두개의 쿼리를 하나로 합치는 작업부터 진행했습니다.\n\n구현할때는 생각도 못했는데 왜 리팩토링 할때는 이렇게 문제점이 잘보일까요? 당연히 or을 사용하면 1개의 쿼리로 해결 할 수 있는 문제였습니다.\n\n```sql\nselect  \n    s1_0.horizontal,  \n    s1_0.vertical,  \n    s1_0.locked,  \n    s1_0.reserved  \nfrom  \n    seats s1_0  \nwhere  \n    s1_0.concert_id=?  \n  and (  \n    s1_0.reserved=?  \n        or s1_0.availability=?  \n    );\n```\n\n(1차 개선 후 측정결과)\n![Pasted image 20240417203050](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/0226aaaa-89ea-4e81-aec3-0b9f1654755c)\n\n풀테이블 스캔을 하는 쿼리가 하나 줄었을 뿐인데 오류가 발생하지 않고 tps,응답시간에 향상이 있었음을 확인했습니다.\n\n## 2차  개선 - 인덱스\n\n해당 테이블에 인덱스 작업을 하지 않았기 때문에 인덱스를 적용하면 성능을 더 개선할 수 있을거라 생각했는데요.\n인덱스의 개념과 적용방법은 [이전글](https://jinkshower.github.io/database_index/)을 참고하시면 좋을 것 같습니다.\n\n인덱스를 적용하기 위해 컬럼들을 확인 할까요? \n![Pasted image 20240424095115](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/867dd6b7-7404-4ca2-a567-ebc65776ed54)\n\nwhere 절에 조건이 명시 되지 않으면 인덱스를 타지 않기 때문에 availability, reserved 두 컬럼을 후보로 두었습니다.\n\n이에 availability, reserved를 복합 인덱스로 두고 인덱스를 타게 하기 위하여 쿼리를 변경해보았습니다.\n\n```sql\nSELECT  \n    s1_0.horizontal,  \n    s1_0.vertical,  \n    s1_0.availability,  \n    s1_0.reserved  \nFROM  \n    seats s1_0  \nWHERE  \n    (  \n        s1_0.reserved = ? OR s1_0.availability = ?  \n        )  \n  AND s1_0.concert_id = ?;\n```\n\n하지만 실행계획은 그대로였습니다.\n\n![Pasted image 20240424124427](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/1547d9e0-2730-4d3f-8d0b-27d2511189b8)\n\n왜 인덱스를 타지 않았을까요?\n\n### 인덱스를 타지 않는 조건 \n\n이에 대해 인덱스를 타지 않는 조건에 대해 학습하게 되었습니다.\n\n1.  인덱스 컬럼을 변환하는 쿼리\n2.  NULL 조건을 사용하는 쿼리\n3. LIKE 문에서 와일드 카드를 앞에 두는 쿼리\n4. OR 조건에서 모든 컬럼에 인덱스 처리가 되지 않았을 때\n5. 읽어야 할 레코드가 전체 테이블의 20%를 상회하는 쿼리\n6. 조건문에 인덱스 컬럼을 명시하지 않는 쿼리\n\n해당 조건에서 4번이 저의 상황과 일치하다고 판단되었습니다. \n\n복합 인덱스를 사용할 시 각 컬럼에 단일 인덱스를 지정하여 합치는 것과 달리 하나의 인덱스가 생성되므로 or조건의 모든 컬럼에 인덱스 처리를 했다고 판단하지 않은 것이라고 예상했습니다.(해당 부분은 좀 더 공부가 필요한 것 같습니다.)\n\n### 인덱스 수정 후 측정 결과\n\n인덱스를 타지 않는 쿼리에 대해 학습하고 복합 인덱스가 아닌, reserved와 availability 각각의 컬럼에 인덱스 처리를 하면 되겠다고 판단하게 되었습니다.\n\n(각 컬럼 인덱스 처리 후 실행계획)\n![Pasted image 20240424141207](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/14481d19-6374-4f8c-ba97-8dd6d3bf25a7)\n\n(2차 개선 후 측정결과)\n![Pasted image 20240417170033](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/87fa645b-f502-488f-8ebc-ecd2e79bbe08)\n드디어 눈에 띄는 개선 결과가 측정되었습니다\n\n## 3차 개선 - 캐싱\n\n해당 api는 예약 불가능한 좌석의 정보만 보내주고 있는게 아니라 콘서트에 대한 정보도 같이 보내주고 있었는데요, \n실시간으로 예약 상태가 변경되는 좌석과 달리 콘서트 정보는 변경이 일어날 확률이 낮다고 판단했습니다.\n\n따라서 콘서트 정보는 캐싱을 적용할 수 있을 거라 판단되었고, 간단하게 적용할 수 있는 스프링의 로컬캐시를 적용해보기로 했습니다.\n\n```java\n@Configuration  \n@EnableCaching  \npublic class CacheConfig {  \n  \n    public static final String CONCERT_CACHE = \"concertCache\";  \n    \n    @Bean  \n    public CacheManager cacheManager() {  \n        return new ConcurrentMapCacheManager(CONCERT_CACHE);  \n    }  \n  \n    //매 6시간마다 콘서트 캐시 제거  \n    @CacheEvict(allEntries = true, value = {CONCERT_CACHE})  \n    @Scheduled(cron = \"0 0 */6 * * *\")  \n    public void cacheEvict() {  \n    }  \n}\n```\n\n로컬 캐시는 ttl을 줄 수 없기 때문에 6시간 간격으로 캐시를 비워주는 스케쥴러 기능을 같이 사용했습니다.\n\n```java\n@Cacheable(value = CacheConfig.CONCERT_CACHE, key = \"#concertId\")\n```\n해당 어노테이션을 캐싱이 필요한 메서드에 추가해주었습니다.\n\n(3차 개선 후 측정결과)\n![Pasted image 20240417202422](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/e71dd8e9-7fde-4e26-a607-386ed1dafd99)\n\n\n2차 개선에서 소폭의 성능상승이 있음을 확인했습니다.\n\n## 4차 개선 - 테이블 설계 변경\n\n3차 개선까지 한 후 인덱스와 실행계획을 다시 살펴봤습니다. \n그리고 제 고정관념에 대해서 깨닫게 되었는데요.\n\n저는 `콘서트의 예약 불가능한 좌석`을 찾기 위해서 당연스럽게 where 절에 concertId를 첫번째 조건으로 명시했었습니다.\n\n하지만 2차개선에서 인덱스를 타기 위해 조건절을 변경하면서 이러한 생각이 깨지게 되었는데요, 제가 당연하게 생각했던 것이 db입장에서는 비효율적인 쿼리를 만들고 있었습니다.\n\n제가 원했던 좌석의 행열정보는\n`concertId -> availability or reserved`의 순서로 서치를 해도,\n`availabilty or reserved -> concertId` 의 순서로 서치를 해도 같은 결과가 나옴을 깨달았습니다. \n\n또한 두개의 컬럼에 각각 인덱스를 거는 것보다 좌석의 상태를 하나의 필드로 관리하고, 이에 인덱스를 걸면 추가 인덱스에 드는 저장공간 소모를 막을 수 있고, 좌석 상태에 대한 관리점을 하나로 모을 수 있겠다는 생각이 들었습니다.\n\n따라서 availability, reserved를 status라는 enum으로 모으고 status 필드 하나에만 인덱스를 걸어 보았습니다.\n\n(바뀐 쿼리)\n```sql\nselect  \n    s1_0.horizontal,  \n    s1_0.vertical,  \n    s1_0.status  \nfrom  \n    seats s1_0  \nwhere  \n    (  \n        s1_0.status=?  \n            or s1_0.status=? \n        )  \n  and s1_0.concert_id=?;\n```\n\n(실행계획이 길어져 markdown으로 대체 합니다)\n```\n-> Filter: (s1_0.concert_id = 2)  (cost=4.56 rows=4.5) (actual time=0.13..0.136 rows=8 loops=1)  \n    -> Index range scan on s1_0 using idx_status over (status = 'RESERVED') OR (status = 'LOCKED'), with index condition: ((s1_0.`status` = 'RESERVED') or (s1_0.`status` = 'LOCKED'))  (cost=4.56 rows=9) (actual time=0.128..0.133 rows=8 loops=1)\n```\n실행 계획을 통해 한 번의 인덱스 스캔으로 먼저 8개의 로우로 데이터를 좁히고 그 안에서 concertId를 필터했음을 알 수 있습니다.\n\n(4차 개선 후 측정결과)\n![Pasted image 20240418165800](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/14134eaf-59a2-4fd5-8b3c-5b38923297c7)\n처음의 결과와 비교해보면 30배 정도의 성능 차이가 나는 것을 확인할 수 있습니다.\n\n## 마치며\n\n이렇게 api의 성능을 차례차례 개선한 기록을 작성해보았습니다.\n\n해당 방법은 status의 수가 적을때 유효하다는 단점이 있습니다. \n\nstatus가 비슷한 데이터가 많아질수록 풀스캔에 가까운 쿼리가 발생할 것이라고 예상되는데요, 하지만 예매가 거의 되지 않은 상황에서는 속도 차이가 많이 나기 때문에 의미있는 결과를 냈다고 생각합니다.\n\n또한 로컬 캐시를 사용했기 때문에 스케쥴러가 분산 서버에서는 여러개 발생해 의도치 않은 캐시 삭제가 일어날 가능성도 있으니 다른 환경에서는 변경이 필요한 방법이라고 생각됩니다.\n\n직접 인덱스를 걸고 실행계획을 확인하고 부하테스트를 진행하면서 제가 작성한 코드나 쿼리에 대해 다시 한번 돌아보는 계기가 되었습니다. 하나의 쿼리를 작성할 때 이 데이터가 10만개, 100만개가 된다면 어떻게 될까를 생각하게 해준 좋은 경험이었다고 생각합니다."},{"excerpt":"학습 계기 저번글에서 프로젝트의 예매 로직에서의 동시성 제어를 Redis로 해결하기로 결정했습니다. redis가 프로젝트의 핵심기능에서 중요한 역할을 하는 만큼 좀 더 자세히 학습할 필요가 있다고 생각했습니다. 또한 프로젝트에서 Redis의 관리가 중요한 대목으로 떠올랐는데요! 이번 글에서는 Redis를 자세히 알아보고 저희 프로젝트에서 어떻게 Redis…","fields":{"slug":"/redis_deeper/"},"frontmatter":{"date":"April 22, 2024","title":"Redis, 좀 더 자세히 알아볼까?","tags":["redis"]},"rawMarkdownBody":"\n## 학습 계기\n\n[저번글](https://jinkshower.github.io/ticket_reservation_concurrency/)에서 프로젝트의 예매 로직에서의 동시성 제어를 Redis로 해결하기로 결정했습니다. redis가 프로젝트의 핵심기능에서 중요한 역할을 하는 만큼 좀 더 자세히 학습할 필요가 있다고 생각했습니다.\n\n또한 프로젝트에서 Redis의 관리가 중요한 대목으로 떠올랐는데요! 이번 글에서는 Redis를 자세히 알아보고 저희 프로젝트에서 어떻게 Redis를 적용했는지 다루어보려고 합니다.\n\n## Redis\n\nRedis는 인메모리 기반의 데이터 저장소로서, 빠른 속도와 간편한 사용성으로 널리 알려져 있습니다. \n\n주로 캐싱, 세션 관리, 메시지 큐, 실시간 분석 등 다양한 용도로 활용됩니다. Redis는 다양한 자료구조를 지원하며, 복제, 클러스터링, 트랜잭션 등의 기능을 제공하여 안정적이고 확장 가능한 시스템을 구축할 수 있습니다.\n\n`저희 프로젝트에서 레디스를 캐시보다는 db로 사용하니 redis의 자료구조에 집중하려 합니다.`\n\n레디스를 캐시로 사용하는 전략은 [해당 글](https://inpa.tistory.com/entry/REDIS-%F0%9F%93%9A-%EC%BA%90%EC%8B%9CCache-%EC%84%A4%EA%B3%84-%EC%A0%84%EB%9E%B5-%EC%A7%80%EC%B9%A8-%EC%B4%9D%EC%A0%95%EB%A6%AC)을 참조하면 좋을 것 같습니다.\n\n## Redis 자료구조\n\nRedis는 다양한 형태의 자료구조를 제공합니다.\n![Pasted image 20240411174307](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/52d38e0f-cb40-40cd-94f6-e8ca8b5d03ce)\n\n### 1. String\n\n문자열은 가장 간단한 형태의 자료구조로서 키와 값을 가지고 있습니다. \n단순 증감 연산에 좋다고 합니다.\n\n```\nSET key value \nGET key\n```\n\n### 2. Hash\n\n해시는 키와 여러 개의 필드와 값으로 구성되어 있습니다. \n\n```\nHSET key field value \nHGET key field\n```\n\n### 3. List\n\n리스트는 여러 개의 요소를 순서대로 저장하는 자료구조입니다. \nBlocking 기능을 통해 Event Queue로도 활용이 가능하다고 합니다.\n\n```\nLPUSH key value1 value2 ... \nLRANGE key start stop\n```\n\n### 4. Set\n\n셋은 중복되지 않는 여러 개의 멤버를 저장하는 자료구조입니다. \n\n```\nSADD key member1 member2 ... \nSMEMBERS key\n```\n\n### 5. Sorted Set\n\n정렬 집합은 셋과 비슷하지만 각 멤버에 대해 순서를 지정하여 저장합니다. \n\n```\nZADD key score1 member1 score2 member2 ... \nZRANGE key start stop WITHSCORES\n```\n\n### 6. Bitmaps\n\n0 또는 1의 값을 가진 이진 데이터를 저장하는 자료구조입니다.\n정수로 된 데이터만 카운팅 가능합니다.\n\n```\nSETBIT key offset value \nGETBIT key offset\n```\n### 7. HyperLogLogs\n\n고유한 요소의 개수를 근사치로 추정하는 확률적 자료구조입니다\n대용량 데이터를 카운팅할 때 적절하며 12kb고정으로 용량을 매우 적게 사용합니다.\n\n```\nPFADD key element1 element2 ... \nPFCOUNT key\n```\n### 8. Streams\n\n타임스탬프와 함께 연결된 메시지 시퀀스를 저장하는 자료구조입니다\n로그를 저장하기 가장 적절한 자료구조입니다.\n\n```\nXADD key ID field1 value1 field2 value2 ... \nXREAD COUNT count STREAMS key ID\n```\n\n## 어떤 자료구조를 써야할까?\n\n자료구조를 결정하기 전에 어떠한 자료구조가 필요한지 먼저 파악해야겠죠?\n\n이전 글에서는 간단하게 key-value값, 즉 String으로 저장하고 key가 있는지 없는지만 체크하는 방식을 썼습니다.\n\n하지만 Redis는 인메모리 DB구조, 즉 RAM을 사용하기 때문에 속도가 빠르지만 그만큼 용량이 작기 때문에 메모리 관리가 필수적입니다.\n\n따라서 비즈니스에서 요구하는 로직에 맞는 자료구조를 적절히 선택하는게 Redis를 제대로 사용하는 첫걸음입니다.\n\n다시 한 번 저희 예매 로직을 살펴볼까요 ?\n\n```java\n@Override  \npublic void createReservation(Long userId, Long concertId,  \n    ReservationRequestDto requestDto) {  \n  \n  \tif (isTaken(concertId, requestDto.getHorizontal(), requestDto.getVertical())) {  \n\t    throw new CustomRuntimeException(\"이미 예약된 좌석입니다.\");  \n\t}\n\n    Seat seat = seatRepository.findSeatForReservation(concertId,  \n        requestDto.getHorizontal(), requestDto.getVertical());  \n  \n    seat.validateAvailability();\n    seat.reserve();  \n  \n    Reservation reservation = new Reservation(userId, concertId, seat.getId());\n    reservationRepository.save(reservation);\n}\n\nprivate Boolean isTaken(Long concertId, String horizontal, String vertical) {  \n    String key = concertId + horizontal + vertical;  \n    return Boolean.FALSE.equals(  \n        redisUtil.add(key, \"reserved\"));  \n}\n```\n\n여기서 Redis에 저장해야 될 정보만을 보겠습니다.\n\n`concertId, horizontal + vertical`\n즉 1과 A1을 저장해야하고 이 두 값이 같은 요청에는 다른 응답을 줘야 합니다.\n\n지금은 \"1A1\"과 같은 형태로 key를 저장하는데요, 좌석 행열 정보는 콘서트마다 몇 천개 정도는 생길 수 있고 콘서트도 얼마든지 생길 수 있기 때문에 key가 너무 많아져 메모리에 문제가 생길 수 있습니다.\n\n따라서 현재 필요한 자료구조로는 Set, Hash, Sorted Set정도를 생각할 수 있겠습니다.\n\nconcertId를 key값으로 두고 행열을 set의 value, hash의 field로 둔다면 콘서트 개수만큼만 key가 생기는 거니 메모리 효율성 측면에서 훨씬 낫다고 판단됩니다.\n\n## Set을 선택한 이유 \n\n저희가 선택한 자료구조는 Set이었습니다.\n\n현재 상황에서 Redis는 1A1라는 요청이 이미 있나, 없나만 판단하는 역할을 하면 됩니다.\n\n콘서트와 콘서트의 좌석 수가 많아질 수 있는 만큼 순서(sorted set)나 더 많은 정보(hash) 관리를 Redis가 담당하면 메모리 관리 지점이 더 늘어날 여지가 크다고 판단했습니다.\n\nSet의 최대의 장점은 속도인데요, value간의 순서를 보장할 필요가 없으므로 추가, 삭제, 조회가 훨씬 더 빠릅니다.\n예매요청이 동시에 많이 일어날 수 있으니 현재 상황에서 가장 적합한 자료구조로 생각됐습니다.\n\n## Set 적용하기\n\nRedis Set의 명령어를 다시 한번 확인할까요? \n```\n127.0.0.1:6379> sadd 1 A1 A2 A3\n(integer) 3\n127.0.0.1:6379> smembers 1\n1) \"A1\"\n2) \"A2\"\n3) \"A3\"\n127.0.0.1:6379> sadd 1 A1\n(integer) 0\n```\n\n`sadd key value`는 value값이 추가된 만큼의 integer를 반환합니다.\n또한 value가 이미 존재할 경우에는 0을 반환하는 것을 확인했습니다.\n\n해당 명령어를 spring boot 프로젝트에 적용해봤습니다.\n\n```java\npublic Long addSet(String key, String value) {  \n    return redisTemplate.opsForSet().add(key, value);  \n}\n```\n\n해당 메서드를 isTaken()에 적용하면 되겠군요.\n\n## Redis 메모리, 삭제 정책 적용하기\n\n하지만 이대로 끝인걸까요? 좀 더 메모리를 효율적으로 쓸 수 있는 방법은 없을까요?\n\n있습니다. 바로 redis의 메모리 휘발성을 이용하는 겁니다.\n\nRedis는 기본적으로 TTL(Time To Live)가 무한대로 설정되는데요, `expire` 명령을 통해 해당 값의 만료시간을 설정할 수 있습니다.\n\n현재 저희 프로젝트에서 콘서트가 시작되면 예매 정보를 따로 저장하는 테이블이 있기 때문에 예매를 막는 Redis의 자료들은 모두 쓸모가 없어집니다.\n\n이를 이용해 저장된 값들의 TTL을 현재시각과 콘서트 시작시각의 차이로 지정하면 메모리를 더 효율적으로 사용할 수 있다고 생각했습니다.\n\n하지만 문제가 있었습니다. Redis에 `sadd` 명령과 `expire` 이 하나의 메서드로 작용하면  value가 추가될 때마다 key의 만료시간이 갱신되버립니다. 따라서 콘서트 시작 직전의 예매 하나 때문에 만료시간이 다시 갱신될 수도 있습니다.\n\n따라서 concertId라는 key값이 처음 생성될 때 만료시간이 설정되고 이후의 value들은 `expire`명령을 실행해서는 안되는 상황입니다.\n\n## Lua 스크립트로 Redis 명령을 커스텀하자\n\n사용자 정의 명령이 필요하다면 Lua 스크립트를 작성해야 합니다.\n\n1. Lua 스크립트 작성\n2. RedisTemplate의 execute로 작성된 스크립트 실행 \n\n의 구조로 되어있는데요, 간단하게 코드로 볼까요 ?\n\n```java\npublic String customCommand(String key, String value) {\n    String script = \"return redis.call('set', KEYS[1], ARGV[1])\";\n    DefaultRedisScript<String> luaScript = new DefaultRedisScript<>(script, String.class);\n    List<String> keys = Collections.singletonList(key);\n    return redisTemplate.execute(luaScript, keys, value);\n}\n```\n\nString으로 스크립트를 저장하고 이를 통해 DefaultRedisScript를 생성하고 execute에 생성된 스크립트, 키값, value를 전달하여 실행합니다.\n\n저희에게 필요한 lua script를 작성해보았습니다.\n```lua\nlocal keyExists = redis.call('exists', KEYS[1])\nlocal isAdded\nif keyExists == 0 then\n    redis.call('sadd', KEYS[1], ARGV[1])\n    redis.call('expire', KEYS[1], ARGV[2])\n    isAdded = 1\nelse\n    isAdded = redis.call('sadd', KEYS[1], ARGV[1])\nend\nreturn tostring(isAdded)\n```\n\nkey가 존재하는지 확인하고, 존재하지 않는다면 `sadd`와 `expire`을 실행하고 존재한다면 `sadd`의 리턴값을 반환하는 스크립트입니다.\n\n```java\npublic String addSet(String key, String value, Long expiredTime) {  \n    StringBuffer stringBuffer = new StringBuffer();  \n    stringBuffer.append(\"local keyExists = redis.call('exists', KEYS[1]) \");  \n    stringBuffer.append(\"local isAdded \");  \n    stringBuffer.append(\"if keyExists == 0 then \");  \n    stringBuffer.append(\"    redis.call('sadd', KEYS[1], ARGV[1]) \");  \n    stringBuffer.append(\"    redis.call('expire', KEYS[1], ARGV[2]) \");  \n    stringBuffer.append(\"    isAdded = 1 \");  \n    stringBuffer.append(\"else \");  \n    stringBuffer.append(\"    isAdded = redis.call('sadd', KEYS[1], ARGV[1]) \");  \n    stringBuffer.append(\"end \");  \n    stringBuffer.append(\"return tostring(isAdded)\");  \n    String script = stringBuffer.toString();  \n    DefaultRedisScript<String> luaScript = new DefaultRedisScript<>(script, String.class);  \n    List<String> keys = Collections.singletonList(key);  \n  \n    return redisTemplate.execute(luaScript, keys, value, expiredTime.toString());  \n}\n```\n스크립트가 추가된 `addSet`메서드 입니다.\n\n이제 해당 메서드로 동시성 테스트를 실행하면\n![Pasted image 20240410164944](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/94b48661-7883-4dc0-a323-4b80f612d156)\n![Pasted image 20240410165002](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/76e1b413-2477-4589-9f15-a283bbe549ea)\n테스트는 통과하고\n\n```\n127.0.0.1:6379> ttl 1\n(integer) 5115\n```\nredis의 ttl도 정상적으로 세팅된 것을 확인할 수 있습니다.\n\n## Redis 데이터 영구 저장하기\n\n처음 부분에서 말씀드린것처럼 Redis는 인메모리 기반으로 작동하기 때문에 서버 재시작시 모든 데이터가 사라집니다\n\n따라서 Redis를 캐시 이외의 용도로 사용할시에는 적절한 데이터 백업이 필요한데요.\n이에는 두가지 방법이 있습니다.\n\n1. RDB\n스냅샷 저장 방식으로 당시의 메모리 그대로 파일로 저장.\n특정 조건이 만족되면 스냅샷을 찍는 방식이므로 조건 전에 Redis가 종료되면 그 사이 데이터는 유실됩니다.\n\n2. AOF\n데이터 변경 커맨드를 모두 저장\n모든 쓰기 명령에 대한 로그를 남기기 때문에 장애 상황 직전까지 모든 데이터가 보장됩니다.\n\nAOF 설정은 redis.conf파일의 `appendonly`를 `yes`로 변경하면 AOF가 적용되고 서버 시작시 aof파일을 읽어서 db에 그대로 다시 저장하게 됩니다.\n\n## Redis 영속화의 주의점\n\n`RDB`\n\nRDB의 기본 설정은 `save <기준초> <쓰기개수>`입니다. 예를 들어 `save 90 1`이라면 90초 이후 1개의 쓰기가 발생했을 때 스냅샷을 찍어 저장하라는 의미입니다.\n\n`save`는 blocking 방식으로 이루어지기 때문에 redis의 동작이 멈추고 디스크에 스냅샷이 저장됩니다. 따라서 백그라운드에서 자식 프로세스를 띄운 후 non-blocking 방식으로 스냅샷을 저장하는 `bgsave`가 권장됩니다.\n\n`bgsave`시 주의해야 할 점은 Copy-On-Write 방식을 사용하기 때문에 자식 프로세스를 fork()할 때 부모 프로세스의 write가 많아 실제로 복사하게 되는 페이지가 많아진다면 메모리 사용량이 많아질 수 있고 이는 swap현상으로 이어져 성능에 영향이 갈 수도 있습니다.\n\n`AOF`\n\nAOF는 변경된 모든 명령을 기록하기 때문에 시간이 지남에 따라 파일 크기가 지나치게 늘어날 수 있습니다. 또한 이렇게 늘어난 파일을 이용해 복구를 수행하는 시간이 늘어날 수 있습니다.\n\n### 그래서 어떻게?\n\nRedis의 공식문서는 데이터가 유실되지 않기 위함이 목적일 때 RDB와 AOF를 같이 쓰는 것을 권장합니다.\n\n주기적으로 RDB로 스냅샷을 저장하고,다음 스냅샷까지의 write를 AOF로 수행한다면 서버가 재시작될 때 스냅샷을 읽어서 백업할 수 있고 비교적 적은 양의 로그만 읽을 수 있게 됩니다.\n\n## 마치며\n\n이렇게 Redis의 자료구조와 메모리 관리, 영속화에 대해 알아보고 저희 프로젝트에 적용, 주의점을 살펴봤습니다. Redis를 공부하며 제대로 쓰기 위해서는 좀 더 깊은 학습이 필요하다고 느껴졌고 이후에도 계속 학습하며 적용해보도록 하겠습니다.\n\n---\n참고\n\nhttps://www.youtube.com/watch?v=92NizoBL4uA&t=1411s \n\nhttps://redis.io/docs/latest/operate/oss_and_stack/management/persistence/"},{"excerpt":"프로젝트에 적용한 Pull Request 링크 학습 계기 프로젝트를 진행하며 쿼리문을 짜고 있는데 너무나 많은 join을 사용하고 있다고 느껴졌습니다.  물론 테이블 개수가 많으면 여러개의 join문을 사용하는 것은 빈번하지만 해당 프로젝트는 테이블의 개수가 그렇게 많지 않은데도(6개) 3~4중 조인문을 작성하며 구현이 진행되니 도메인 설계와 비즈니스 로…","fields":{"slug":"/domain_refactoring/"},"frontmatter":{"date":"April 16, 2024","title":"리팩토링을 통해 유연한 도메인을 만들자!","tags":["java","refactoring"]},"rawMarkdownBody":"\n## 프로젝트에 적용한 Pull Request\n\n[링크](https://github.com/lay-down-coding/tickitecking/pull/31)\n\n## 학습 계기\n\n프로젝트를 진행하며 쿼리문을 짜고 있는데 너무나 많은 join을 사용하고 있다고 느껴졌습니다. \n\n물론 테이블 개수가 많으면 여러개의 join문을 사용하는 것은 빈번하지만 해당 프로젝트는 테이블의 개수가 그렇게 많지 않은데도(6개) 3~4중 조인문을 작성하며 구현이 진행되니 도메인 설계와 비즈니스 로직에 대한 재점검이 필요하다고 느껴졌습니다.\n\n회의를 통해 도메인 설계와 비즈니스 로직을 수정했고 이에 따라 리팩토링을 진행한 기록을 남기고자 합니다.\n\n## 리팩토링 전 설계 살펴보기\n\n![Pasted image 20240429151801](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/cb452356-4387-4bc6-8ff7-af762dab77a5)\n초기의 비즈니스 로직 설계는 이러했습니다.\n\n1. 공연장 생성시 3개로 고정된 등급을 가진 좌석들을 생성한다\n2. 콘서트 생성시 3개의 등급에 맞는 가격을 결정한다.\n3. 예매시 좌석이 예약되었는지 확인한다.\n4. 예약되지 않았다면 예매를 생성한다.\n\n공연장이 좌석을 제공하고 콘서트는 해당 공연장의 좌석을 이용하는 것이 좀 더 현실적이라고 생각해서 내렸던 결정이었습니다.\n\n이에 따라 좌석은 공연장의 id를 가지고 좌석의 가격은 콘서트의 id를 가지게 되었습니다.\n\n예매는 예매한 사용자의 id, 콘서트의 id, 좌석의 id를 가지게 하면 1번 좌석의 1번 콘서트에 1번 유저가 예매한 정보를 저장할 수 있으니 예매정보를 콘서트의 좌석마다 가지는 것도 문제없을거라는 생각이었죠. \n\n## 다중 조인을 작성하며\n\n해당 설계의 문제점은 코드를 구현하면서 드러났습니다.\n\n콘서트의 예약된 좌석의 행열 정보를 찾는 쿼리문입니다. \n```sql\nselect\n        s1_0.horizontal,\n        s1_0.vertical \n    from\n        concerts c1_0 \n    join\n        seats s1_0 \n            on c1_0.auditorium_id=s1_0.auditorium_id \n    join\n        reservations r1_0 \n            on r1_0.seat_id=s1_0.id \n    where\n        (\n            c1_0.deleted_at is NULL\n        ) \n        and c1_0.id=? \n        and r1_0.concert_id=? \n        and r1_0.seat_id=s1_0.id \n        and r1_0.status=?\n```\n\n1. 콘서트 id로 콘서트를 찾습니다\n2. 찾은 콘서트에서 공연장 id로 좌석테이블과 join 합니다\n3. 찾은 좌석에서 좌석 id로 예매테이블과 join합니다\n4. 찾은 예매에서 콘서트 id, status가 \"Y\"인 좌석을 찾습니다.\n\n예약된 좌석의 행열번호를 찾는다는 간단한 로직인데 쿼리문과 그 쿼리문을 수행하기 위한 로직은 그렇게 간단하지 않았습니다.\n\n사실 쿼리문만 작성하면 기능이 문제없이 작동되긴 하지만 해당 코드를 누군가 고칠 수 있을까?라는 생각에는 물음표가 띄워졌습니다. \n\n## 진짜 문제점 파악하기\n\n작성한 코드(QueryDsl)를 보며 도메인 설계에 대한 리팩토링이 필요하다는 생각이 들었습니다. \n\n저희 설계의 문제는 무엇이었을까요?\n\n![Pasted image 20240429152306](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/08efc576-090d-45c5-b32e-99aa67c8484a)\n\n바로 `좌석이 예약에 대한 정보를 가지지 않기 때문`입니다.\n좌석이 예약에 대한 정보를 가지고 있다면 좌석 테이블에 where하나로 해결할 수 있는 문제였습니다.\n\n좌석이 예약에 대한 정보를 가지지 못한 이유는 무엇이었을까요?\n\n`좌석과 콘서트의 생성주기가 달랐기 때문입니다.`\n\n이는 좌석의 등급별 가격에 대한 고민에서 비롯되었는데요, 가격의 등급을 3개로 제한하고 공연장이 생성될 때 고정된 등급을 좌석에 부여한데에서 문제가 생겼습니다.\n\n공연장이 좌석의 등급을 결정해서 생성하기 때문에 좌석 테이블은 콘서트나 예약에 관한 정보를 가지는게 불가능했습니다. \n\n그러다 보니 좌석의 예약 상태를 확인하기 위해서 콘서트-공연장-좌석-예매의 4개 테이블이 모두 쓰일 수 밖에 없었던 것이죠\n\n## 설계 리팩토링\n\n문제점을 알았으니 도메인 설계를 리팩토링하기로 결정했습니다.\n\n`공연장이 생성될 때 좌석이 생성된다는 현실에서는 자연스럽던 사실이 코드상에서는 오히려 부자연스럽고 복잡한 구현을 낳았습니다.`\n\n그래서 공연장이 좌석을 생성하는 게 아닌, 콘서트가 생성될때 콘서트가 좌석을 생성하게 바꾸기로 결정하였습니다.\n\n이에 따라 공연장 도메인에서 좌석에 대한 의존성을 모두 제거하였습니다.\n\n![Pasted image 20240404152113](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/bc8fe3cf-3bde-49d4-b5ae-6e7421d463d9)\n(깔끔해진 공연장 임포트문)\n\n이제 좌석은 concertId와 예약여부인 reserved 필드를 가질 수 있게 되었습니다.\n\n![Pasted image 20240429152547](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/ec83ec4f-f1b4-4904-a5aa-a53f42ae95b7)\n\n이에 따라 바뀐 설계입니다.\n\n![Pasted image 20240429152754](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/aca03a98-c63e-4fc1-b2e8-aac994111bc5)\n\n\n## 설계가 바뀐 후\n\n설계가 바뀐 후의 예약된 좌석 정보를 조회하는 쿼리문이 어떻게 바뀌었을까요 ?\n\n```sql\n\tselect\n        s1_0.horizontal,\n        s1_0.vertical \n\tfrom\n        seats s1_0 \n    where\n        s1_0.concert_id=? \n        and s1_0.reserved=?\n```\n\n4개의 테이블을 거쳐서 찾아야 했던 좌석정보가 단 하나의 테이블을 조회하는 쿼리문으로 바뀌었습니다.\n\n전과 비교했을 때 당연히 해당 쿼리를 보기도 쉽고, 고치기도 편하게 되었네요!\n\n### 바뀐 설계가 열어준 유연한 가격정책\n\n설계가 바뀌니 고정되어 있던 가격정책에도 눈이 갔습니다.\n\n가격이 3개로 고정되어 있었고 콘서트는 3개의 등급에 대한 가격을 결정하다보니 정해진 등급에 가격을 맞춰끼우는 코드를 작성해야 했었는데요.\n\n(G는 골드, S는 실버, B는 브론즈 등급입니다)\n```java\n@Override  \npublic void createSeatPrices(Long concertId, SeatPriceDto seatPriceDto) {  \n    List<SeatPrice> seatPrices = new ArrayList<>();  \n  \n    Map<String, Double> seatPricesMap = parseSeatPrices(seatPriceDto);  \n  \n    seatPricesMap.forEach((grade, price) -> {  \n        SeatPrice seatPrice = SeatPrice.builder()  \n            .price(price)  \n            .grade(grade)  \n            .concertId(concertId)  \n            .build();  \n        seatPrices.add(seatPrice);  \n    });  \n  \n    seatPriceRepository.saveAll(seatPrices);  \n}\n\nprivate Map<String, Double> parseSeatPrices(SeatPriceDto seatPriceDto) {  \n    return Map.of(  \n        \"G\", seatPriceDto.getGoldPrice(),  \n        \"S\", seatPriceDto.getSilverPrice(),  \n        \"B\", seatPriceDto.getBronzePrice()  \n    );  \n}\n```\n\n해당 코드는 슬쩍봐도 확장성이 꼭꼭 닫혀진 좋지 않은 코드였습니다.\n\n만약 가격정책에서 다이아몬드 등급이 추가된다면? Map을 사용하는 모든 코드에 \"D\"라는 키를 추가해야겠네요. 만약 Grand라는 등급이 추가되면 모든 키의 String값을 수정해야 겠네요.\n\n바뀐 설계에서는 콘서트가 좌석을 생성하기 때문에 가격정책도 유연하게 바뀔 수 있었습니다. \n\n```java\n@Override  \npublic void createSeatPrices(Long concertId, List<SeatPriceRequestDto> seatPriceRequestDtos) {  \n  SeatPrices seatPrices = SeatPrices.from(concertId, seatPriceRequestDtos);  \n  seatPriceRepository.saveAll(seatPrices.getSeatPrices());  \n}\n//SeatPrices 클래스\npublic class SeatPrices {  \n  \n    private final List<SeatPrice> seatPrices;  \n  \n    public SeatPrices(List<SeatPrice> seatPrices) {  \n        this.seatPrices = seatPrices;  \n    }  \n  \n    public static SeatPrices from(Long concertId, List<SeatPriceRequestDto> seatPriceRequestDtos) {  \n        return toEntity(concertId, seatPriceRequestDtos);  \n    }  \n  \n    public static SeatPrices toEntity(Long concertId,  \n        List<SeatPriceRequestDto> seatPriceRequestDtos) {  \n        List<SeatPrice> seatPrices = new ArrayList<>();  \n  \n        for (SeatPriceRequestDto requestDto : seatPriceRequestDtos) {  \n            SeatPrice seatPrice = SeatPrice.builder()  \n                .grade(requestDto.getGrade())  \n                .price(requestDto.getPrice())  \n                .concertId(concertId)  \n                .build();  \n            seatPrices.add(seatPrice);  \n        }  \n        return new SeatPrices(seatPrices);  \n    }  \n  \n    public List<SeatPrice> getSeatPrices() {  \n        return new ArrayList<>(seatPrices);  \n    }  \n}\n```\n\nSeatPrice를 일급컬렉션으로 만들고 해당 일급컬렉션내에서 entity로 만드는 로직을 갖게 했습니다. \n\n콘서트를 생성할때 콘서트가 원하는 좌석의 가격과 등급을 결정할 수 있게 되었고 이제 저희는 해당 부분의 변경이 필요할 때 SeatPrices의 로직을 바꾸면 됩니다.\n\n## 마치며\n\n해당 리팩토링 경험을 통해 도메인 설계의 중요성, 현실과 객체는 언제나 1대1로 매칭되는 정석적인 관계가 아니라는 사실을 다시 한번 깨닫게 되었습니다. \n\n\n\n\n\n\n"},{"excerpt":"프로젝트에 적용한 Pull Request 링크 학습 계기 콘서트 티켓 예매 프로젝트를 진행하는 중 동시에 많은 사용자가 한 자리의 좌석을 예매할 시 여러개의 같은 예약이 생성되는 문제를 발견했습니다. 콘서트 예매 상황을 생각해보면 굉장히 흔한 일인데요, '이미 선택된 좌석입니다'라는 메시지를 한번쯤은 보신 기억이 있을 거라 생각됩니다. 해당 문제를 해결하…","fields":{"slug":"/ticket_reservation_concurrency/"},"frontmatter":{"date":"April 11, 2024","title":"'이미 선택된 좌석입니다' 티켓 예매시 중복예매생성 문제","tags":["concurrency","lock","redis"]},"rawMarkdownBody":"\n## 프로젝트에 적용한 Pull Request\n\n[링크](https://github.com/lay-down-coding/tickitecking/pull/35)\n\n## 학습 계기\n\n콘서트 티켓 예매 프로젝트를 진행하는 중 동시에 많은 사용자가 한 자리의 좌석을 예매할 시 여러개의 같은 예약이 생성되는 문제를 발견했습니다.\n\n콘서트 예매 상황을 생각해보면 굉장히 흔한 일인데요, '이미 선택된 좌석입니다'라는 메시지를 한번쯤은 보신 기억이 있을 거라 생각됩니다.\n\n해당 문제를 해결하기 위해 jpa의 낙관적 락, 비관적락을 적용해보고 다른 방식으로 문제를 해결한 기록을 공유하고자 합니다.\n\n## 예매 코드와 테스트 코드\n\n예매 로직을 수행하는 코드를 살펴보고 갈까요?(실제 코드와 다를 수 있습니다.)\n\n```java\npublic void createReservation(Long userId, Long concertId,  \n    ReservationRequestDto requestDto) {  \n  \n    Seat seat = seatRepository.findSeatForReservation(concertId,  \n        requestDto.getHorizontal(), requestDto.getVertical());  \n  \n    seat.validateAvailability();\n    seat.reserve();  \n  \n    Reservation reservation = new Reservation(userId, concertId, seat.getId());\n    reservationRepository.save(reservation);\n}\n```\n\n저희는 콘서트의 좌석을 행과 열로 관리하고 있기때문에 findSeatForReservation()을 통해서 해당 콘서트의 id, 행열 정보로 seat를 찾고 seat가 예약 가능한지 살펴보고 예약이 가능하다면 seat의 예약 필드를 바꾸고 reservation을 만드는 로직을 사용하고 있습니다.\n\n1번 콘서트의 A-1이라는 좌석을 20명의 사용자가 동시에 예매하는 테스트 코드를 작성해보았습니다.\n\n```java\n@DisplayName(\"동시에 한자리 예매시 첫번째 요청만 예매성공한다.\")  \n@Test  \nvoid concurrency_test() throws InterruptedException {  \n    //given  \n    int tryCount = 20;  \n    long userId = 1L;  \n    Long concertId = 1L;  \n    ReservationRequestDto reservationRequestDto = ReservationRequestDto.builder()  \n        .horizontal(\"A\")  \n        .vertical(\"1\")  \n        .build();  \n    ExecutorService executor = Executors.newFixedThreadPool(10);  \n  \n    //when  \n    CountDownLatch latch = new CountDownLatch(tryCount);  \n    for (int i = 0; i < tryCount; i++) {  \n        int finalI = i;  \n        executor.submit(() -> {  \n            try {  \n                reservationService.createReservation(userId + finalI, concertId,  \n                    reservationRequestDto);  \n            } catch (Exception e) {  \n                log.error(e.getMessage());  \n            } finally {  \n                latch.countDown();  \n            }  \n        });  \n    }  \n    latch.await();  \n  \n    //then  \n    assertThat(reservationRepository.count()).isEqualTo(1);  \n}\n```\n\n테스트 결과는 실패였습니다.\n\n![Pasted image 20240407111517](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/3a81b97d-faa2-4c7c-ad5f-5afd2b0b3569)\n![Pasted image 20240407111531](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/b5088e14-d8e8-4eb0-b05a-a97c368552e1)\n\n왜 이런일이 일어난걸까요?\n\n## 동시성 문제\n\n동시성 제어를 하지 않은 현재의 경우 2개의 스레드만을 생각해보면, Thread1이 seat를 조회하는 동안 Thread2도 seat를 조회할 수 있습니다.\n\nThread1이 seat의 예약 상태를 바꾸기 전에 Thread2도 예약 상태를 조회할 수 있기 때문에 결국 update를 두개의 스레드들이 모두 수행할 수 있고 예약이 동시에 seat에 접근한 스레드의 수(10개)만큼 생성될 수 있습니다.\n\n이 문제를 해결하려면 먼저 들어온 요청이 끝나기 전까지 다른 스레드들은 seat의 정보를 읽어서는 안됩니다.\n\n즉 seat의 예약상태를 임계영역(Critical Section)으로 보고 스레드들의 경쟁상태(Race Condition)을 제어해줄 필요가 있습니다.\n\n## Synchronized\n\n자바는 `synchronized` 키워드를 사용하여 스레드 간의 임계 영역을 보호할 수 있습니다. `synchronized`를 사용하면 한 번에 하나의 스레드만이 해당 블록 또는 메서드에 진입할 수 있습니다. 이를 통해 동시성 문제를 해결할 수 있습니다.\n\n```java\npublic synchronized void createReservation(Long userId, Long concertId,  \n    ReservationRequestDto requestDto) {  \n  \n    Seat seat = seatRepository.findSeatForReservation(concertId,  \n        requestDto.getHorizontal(), requestDto.getVertical());  \n  \n    seat.validateAvailability();\n    seat.reserve();  \n  \n    Reservation reservation = new Reservation(userId, concertId, seat.getId());\n    reservationRepository.save(reservation);\n}\n```\n\n간단하게 예매 메서드에 synchronized 키워드를 추가하면 자바가 제공하는 동시성 제어를 사용할 수 있습니다.\n\n하지만 테스트는 실패합니다.\n\n![Pasted image 20240407111955](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/346b5654-6179-4ea8-a628-dc3d4c4c9316)\n![Pasted image 20240407112022](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/3303dd96-84fc-475c-8102-d0642513045a)\n\n### synchronized의 문제점\n\nsynchronized는 @Transactional과 함께 사용시 동시성을 제대로 제어할 수 없습니다.\n\n이를 위해서는 @Transactional에 대한 이해가 필요한데요.\n\n@Transactional은 해당 어노테이션이 붙은 메서드에 트랜잭션 환경을 제공하기 위해 프록시 메서드를 만들고 그 프록시 메서드에서 실제 트랜잭션을 시작하고 종료하는 작업을 처리합니다.\n\n이 때 실제 메서드에서 쓰이는 synchronized 키워드는 프록시 메서드에 적용되지 않습니다.\n\n(간단하게 재현해본 @Transactional의 프록시 메서드)\n```java\npublic class TransactionalProxy {\n\n    // 프록시 메서드\n    public void invokeTransactionalMethod(Runnable method) {\n            // 트랜잭션 시작\n            method.run();\n            // 트랜잭션 커밋\n    }\n}\n```\n\n즉 트랜잭션 시작과 커밋을 담당하는 프록시 메서드에 한 스레드만 접근하는 것을 보장하지 못하기 때문에 동시성 문제를 synchronized로는 해결 할 수 없습니다.\n\n## 락\n\n해당 문제를 해결하기 위해 JPA가 제공하는 락의 기능에 대해 알아보고 적용해봤습니다.\n\n## 낙관적 락\n\n낙관적 락은 여러 트랜잭션의 충돌이 적을 것을 낙관적으로 가정하고 JPA가 제공하는 버전 관리 기능을 사용하는 것입니다.\n\n낙관적 락을 적용하는 방법은 간단합니다.\n낙관적 락이 필요한 엔티티에 @Version 어노테이션을 추가해주면 됩니다.\n```java\n@Version\nprivate Integer version;\n```\n\n이제 해당 엔티티는 수정할때마다 버전이 하나씩 자동으로 증가하고 엔티티를 수정할 때 조회 시점 버전과 다르다면 예외를 발생시킵니다.\n\n![Pasted image 20240408113751](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/7cba894a-6068-49c0-bd3b-23e68c0d5119)\n따라서 낙관적 락을 사용하면 최초의 커밋만 인정되고 나머지 트랜잭션은 예외가 발생하기 때문에 동시성을 제어할 수 있게 됩니다.\n\n낙관적 락을 적용하고 테스트를 해보았습니다.\n\n![Pasted image 20240407115641](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/e187e197-959d-4937-89b3-b4c8a6743e4b)\nwhere절의 version이 보이시나요? 해당 쿼리는 seat.reserve()를 할때  발생하는 update 쿼리로 조회시의 버전과 update시점의 버전이 다르면 예외를 발생시킵니다.\n\n![Pasted image 20240407115601](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/2a600f34-5f40-4ce2-9db1-ebb0674f548f)\n\n테스트 결과는 통과입니다.\n![Pasted image 20240407115717](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/ab541c3e-948f-4a77-924b-14759d767db8)\n\n## 비관적 락\n\n비관적 락은 여러 트랜잭션의 충돌이 일어날 것을 비관적으로 가정하고 우선 데이터베이스 락 메커니즘을 사용하여 해당 row에 락을 거는 방법입니다.\n\n비관적 락의 적용방법도 간단한데요\n```java\n@Lock(LockModeType.PESSIMISTIC_WRITE)  \nSeat findSeatForReservation(Long concertId, String horizontal, String vertical);\n```\n\n데이터베이스에 PESSIMISTIC_WRITE로 쓰기 락을 걸 수 있습니다.\n\n비관적 락을 위와 같이 설정하면 위의 메서드를 사용하여 seat를 조회할때 그냥 select대신 `select for update`로 조회하고 해당 데이터에 배타적 lock을 걸어 lock을 획득한 트랜잭션의 update가 실행될 때까지 다른 트랜잭션의 데이터 조회를 막을 수 있습니다. \n\n![Pasted image 20240407121851](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/dd47c914-1d6d-4863-9d15-c90882fae4c1)\n\n![Pasted image 20240407123606](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/66fedf8e-cc45-4114-a7d5-4f8718e02fd3)\n\n테스트 또한 통과합니다.\n\n## 락을 꼭 써야할까?\n\n사실 락을 쓰지 않고 중복된 예매 생성을 막는 방법이 있습니다.\n\n바로 Reservation 테이블에 unique constraints를 걸어주는 방법인데요, \nconcert_id + seat_id를 복합unique키 설정을 해주면 중복된 예매가 생성되는 것을 막을 수 있습니다\n\njpa에서 두개 이상의 컬럼에 unique 설정을 해주려면 다음과 같이 @Table 어노테이션을 수정해주면 됩니다.\n```java\n@Table(name = \"reservations\", uniqueConstraints = {  \n    @UniqueConstraint(  \n        columnNames = {\"concertId\", \"seatId\"}  \n    )  \n})\n```\n테스트를 돌려보면!\n![Pasted image 20240415105417](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/a715aea6-1b3c-447d-a897-184f622f8c0b)\n\nunique key violation이 중복된 예매에서 발생하는 것을 확인할 수 있습니다.\n\n![Pasted image 20240415105618](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/e8742978-98c4-4bc4-992d-20558c8f1442)\n테스트는 통과입니다.\n\n## 그럼 무슨 방법을 써야할까?\n\n낙관적 락, 비관적 락 혹은 unique constraint 모두 예매 중복 생성 방지라는 목표를 달성했지만 프로젝트에 적용하기에는 무리가 있다고 판단했습니다. \n\n낙관적락은 update, 비관적 락은 select for update쿼리, unique constraint 모두 스레드 요청만큼 발생하게 됩니다. db에 요청 수만큼  쿼리가 날아가면 부하가 심해질 것입니다.\n\n## db에 부하를 주지 않는 방법은 없을까?\n\n두가지 방법을 생각해봤습니다.  \n\n첫 번째 방법은 첫번째 좌석을 예매하는 요청이외의 다른 스레드의 요청을 예외처리하면 db에 쿼리를 날리지 않을 수 있다고 생각했습니다.\n\n이를 위해 Java의 Map과 같은 구조로 좌석의 정보를 key, value로 설정하고 해당 key, value가 이미 있으면 예외로 처리하면 되지 않을까라는 생각을 하게 되었습니다.\n\n두 번째 방법은 메시지 큐를 사용하여 사용자의 요청을 순서대로 처리하는 버퍼(Buffer)시스템을 도입하는 방법입니다.\n\n## 첫 번째 방법, Redis를 비관계형 데이터베이스로 사용하기\n  \nRedis는 인메모리 데이터베이스로서 데이터를 메모리에 저장하므로 빠른 응답 속도를 제공합니다.\n\nredis를 활용하면 분산DB환경에서도 따로 동작하는 공통의 db가 생기는 것이기 때문에 예매 로직이 문제없이 실행될 것이라 생각했습니다.\n\n비관계형 데이터베이스는 MongoDB도 있지만 저희 로직에서 빠른 성능이 필요했기 때문에 Redis를 우선적으로 선택하여 적용해봤습니다.\n\n### redis 적용하기\n\n`redis 설정과 명령어는 좀더 학습한 후 다른 포스트에서 자세히 다룰 예정입니다.`\n\nredis에는 `SETNX`라는 명령어가 존재합니다. 'SET if Not eXits'의 줄임말로 특정 Key에 Value가 존재하지 않을 때만 값을 설정할수 있는 명령어입니다.\n\n```java\n127.0.0.1:6379> setnx 1A1 reserved\n(integer) 1\n127.0.0.1:6379> setnx 1A1 reserved\n(integer) 0\n```\n\n이를 이용하여 첫번째 요청시에 concertId + 행열정보로 key를 설정하고 다른 요청에서 같은 key로 요청시 응답이 다른 redis의 성질을 이용하면 될 것이라 생각했습니다.\n\n```java\nprivate final RedisTemplate<String, String> redisTemplate;\n//\nif (isTaken(concertId, requestDto.getHorizontal(), requestDto.getVertical())) {  \n    throw new CustomRuntimeException(\"이미 예약된 좌석입니다.\");  \n}\n//\nprivate Boolean isTaken(Long concertId, String horizontal, String vertical) {  \n    String key = concertId + horizontal + vertical;  \n    return Boolean.FALSE.equals(  \n        redisTemplate.opsForValue().setIfAbsent(key, \"reserved\"));  \n}\n```\n\nSpring환경에서 redis를 코드로 활용하기 위하여 RedisTemplate를 @Bean으로 등록해 사용했습니다. \n\n예매 로직이 실행되기전 isTaken()메서드를 호출하여 `opsForValue().setIfAbsent()`로 concertId +행열을 key로 설정합니다.\n\n당연히 첫 요청은 위에서 본 것처럼 1이 반환되고 이는 RedisTemplate에서 true로 반환됩니다. 같은 좌석을 예매하는 요청은 0이 반환되고 false가 반환되겠네요!\n\n해당 메서드를 테스트해보겠습니다.\n\n테스트는 통과하고\n![Pasted image 20240407193400](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/db7fce63-f06e-4db9-a8b0-76622d488ad0)\n\n애플리케이션에서 redis로 미리 예외처리를 모두 해줬기 때문에 db에 insert쿼리가 단 하나만 날아가는 모습입니다.\n![Pasted image 20240407193443](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/73a814d4-47eb-4e08-aca2-88224491cd33)\n\n## 두 번째 방법, 메시지 큐 이용하기\n\n두 번째 방법은 메시지 큐를 이용하는 방법입니다.\n\n메시지 큐는 기본적으로 큐(queue)라는 자료 구조를 사용하여 데이터를 저장하고, 이를 통해 발신자(sender)와 수신자(receiver) 간의 통신을 관리합니다.\n\n즉, 예매 요청은 기본적으로 메시지 큐에 발신하고, 메시지 큐를 순차적으로 중복이 제거되게 구현하면 저희는 큐에서 메시지를 수신해서 예약을 생성하면 될 것이라고 생각했습니다.\n\n이러한 아이디어를 가지고 적용할만한 메시지 큐 시스템을 찾게 되었습니다. \n\nRabbitMQ, Kafka, Redis Pub/Sub등 많은 방법이 있었지만 저희 프로젝트에 적용해 본 것은 SQS였습니다.\n\n### SQS FIFO Queue\n\nSqs를 적용하게 된 이유는  AWS의 공식문서를 읽어본 결과 저희 프로젝트에서 이미 원하는 Queue 시스템을 제공해주고 있었기 때문입니다.\n\n바로 Sqs FIFO Queue인데요, 동일한 요청은 자동으로 중복을 제거해주고 순서대로 요청을 처리해주는 Sqs의 Queue의 한 종류로 저희 프로젝트에서 딱 필요한 기능이라고 생각했습니다.\n\n또한 메시지 큐에 대한 이해도가 낮은 상태이기 때문에 다른 기술들의 높은 학습비용을 치루는 것보다 이미 잘 만들어진 시스템을 이용하고자 하는 마음도 있었습니다.\n\n### SQS 적용하기\n\n`Spring Boot 3.x 버전과 Spring Cloud Aws 3.x버전으로 진행했습니다.`\n\n```java\n@PostMapping  \npublic SendResult<String> sendReservationMessage(  \n    @AuthenticationPrincipal UserDetailsImpl userDetails,  \n    @PathVariable Long concertId,  \n    @RequestBody ReservationRequestDto requestDto) {  \n    String payload = sqsRequestParser.toPayload(userDetails.getUser().getId(), concertId,  \n        requestDto);  \n  \n    return sqsTemplate.send(to -> to  \n        .queue(queueName)  \n        .messageGroupId(String.valueOf(concertId))  \n        .messageDeduplicationId(requestDto.getHorizontal() + requestDto.getVertical())  \n        .payload(payload));  \n}  \n  \n  \n@SqsListener(\"${cloud.aws.sqs.queue.name}\")  \npublic void createReservation(String message) {  \n    SqsRequest sqsRequest = sqsRequestParser.toRequest(message);  \n  \n    ReservationRequestDto requestDto = ReservationRequestDto.builder()  \n        .horizontal(sqsRequest.getHorizontal())  \n        .vertical(sqsRequest.getVertical())  \n        .build();  \n  \n    reservationService.createReservation(  \n        sqsRequest.getUserId(), sqsRequest.getConcertId(), requestDto);  \n}\n```\n\n[공식문서](https://docs.awspring.io/spring-cloud-aws/docs/3.0.0/reference/html/index.html#sqs-integration)를 참고하여 저희가 원하는 기능을 Sqs로 구현해보았습니다. \n\n간단하게 설명드리자면 @PostMapping에서 받은 요청을 sqs로 보내는 메서드와 @SqsListener로 큐에 있는 메시지를 받은 메서드 두가지 입니다. \n\nSqs FIFO Queue는 GroupId로 메시지들을 구분하는데 DeduplicationId가 같은 요청은 자동으로 제거합니다. \n\n따라서 GroupId 는 콘서트 id, DeduplicationId는 행열로 설정해주었습니다. \n\n적용 후, 동시요청을 보내도록 하겠습니다.\n\n결과는 \n![Pasted image 20240429162111](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/e344220b-4b7b-42a8-b830-c7764e592221)\n하나의 예매만 생성되었습니다.\n\n## 둘 중에 뭘쓰지?\n\n두 방법 모두 예매 중복생성문제를 해결할 수 있으니 성능을 측정하여 비교하기로 하였습니다.\n\nSQS\n![Pasted image 20240421113403](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/06fbea41-f792-4ce8-976e-752de09cf3f5)\n(만개의 동시 요청시 Sqs는 3천개의 메시지 제한으로 일부가 실패했습니다)\n\nRedis\n![Pasted image 20240407165805](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/9a98b6e1-94cd-49c1-b8c2-c6b5bfa60814)\n\n(Redis의 오류율은 의도된 것입니다)\n\n응답속도는 Sqs가 우위를 가지지만 Redis는 tps가 우위에 있습니다. 그렇게 큰 성능 차이는 아니라고 판단했습니다.\n\n## Redis를 선택한 이유\n\n저희는 Redis를 사용하기로 결정하였습니다.\n\n이유는 다음과 같습니다.\n\n성능에 큰 차이가 나지 않고, Sqs의 메시지 3천개 제한으로 오류가 발생하는 것도 주요한 이유였습니다. \n\n하지만 무엇보다\n\n`Sqs FIFO Queue의 중복제거 5분 제약` 때문에 Sqs 적용이 기각되었습니다.\n\n[공식문서](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html)발췌\n```\nMessage deduplication ID is the token used for deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.\n```\n\n저희에게 중요한 부분만 보자면 Sqs FIFO Queue는 5분동안 같은 요청을 중복으로 보고 큐에 적재하지 않습니다. 그리고 이 5분이라는 제약은 고정입니다.\n\n예매 생성에는 해당 제약은 일견 문제가 없어보이지만 예매가 취소되면, 이를 실시간으로 반영할 수 없습니다. 취소되어도 5분 동안 해당 요청은 중복으로 취급됩니다. \n\n정책상으로 취소가 되면 5분동안 예매가 불가능 하다는 비즈니스 제약을 추가한다고 해도 해당 좌석은 취소되면 사용자에게도 예매가 불가능하게 보여야 합니다.\n\n따라서 해당 Sqs를 제대로 저희 프로젝트에 적용하려면 대거의 도메인 코드 수정이 일어나야 한다고 판단했고 이는 저희가 얻을 성능적 이점에 비해 너무 큰 리스크라고 판단했습니다.\n\n## 마치며\n\n이렇게 예매 상황에서 중복 예매 생성 방지를 위해 낙관적락, 비관적락, 락을 쓰지 않는 방법을 살펴보고 비즈니스 로직에 더 적절하고 db에 부하를 주지 않는 Redis를 활용한 기록을 적어보았습니다.\n\n하지만 Redis도 단점은 분명히 존재합니다. 핵심 비즈니스 로직인 예매 기능이 Redis와 강하게 결부된다는 점, 그리하여 Redis의 서버가 죽는다면 예매 로직이 제대로 기능하지 못한다는 점등이 그러합니다.\n\n현재 상황에서는 좋은 방안이라고 생각되지만 프로젝트가 진행되면서 해당 기능과 문제 해결점이 다시 변할 수도 있다고 생각됩니다!!"},{"excerpt":"이전 글에서 Github Actions로 CI환경을 만들고 적용한 글을 작성했던 적이 있습니다.  이 후 현재 참여하는 프로젝트에서 배포 자동화의 필요성을 느끼고 학습, 적용한 기록을 적어봅니다. 배포 자동화의 필요성 이전에 참여한 프로젝트에서 AWS의 EC2 인스턴스로 배포를 할 때 신기하면서도 힘들었던 기억이 있습니다.  코드의 수정이 있어 머지가 될…","fields":{"slug":"/docker_githubactions/"},"frontmatter":{"date":"April 06, 2024","title":"Github Actions, Docker와 함께하는 배포 자동화","tags":["docker","continuous_depolyment","github_actions"]},"rawMarkdownBody":"\n[이전 글](https://jinkshower.github.io/githubaction_automated_test/)에서 Github Actions로 CI환경을 만들고 적용한 글을 작성했던 적이 있습니다. \n\n이 후 현재 참여하는 프로젝트에서 배포 자동화의 필요성을 느끼고 학습, 적용한 기록을 적어봅니다.\n\n## 배포 자동화의 필요성\n\n이전에 참여한 프로젝트에서 AWS의 EC2 인스턴스로 배포를 할 때 신기하면서도 힘들었던 기억이 있습니다. \n\n코드의 수정이 있어 머지가 될 때마다 jar파일을 직접 build하고 해당 파일을 ec2에 업로드하고 다시 nohup으로 jar파일을 실행해야 했습니다.\n\n이후 쉘 스크립트를 작성해 위 과정에서 많은 부분을 생략하는 경험도 해보았지만 여전히 배포가 다시 필요할때마다 인스턴스에 접속해 쉘 스크립트를 실행하며 CI/CD툴을 이용해 배포 자동화를 이루고 말리라는 결심을 하게 되었습니다.\n\n배포 자동화에 사용되는 툴은 여러가지가 있지만 이미 워크플로우 작성 경험이 있는 Github Actions를 사용하는 것이 러닝 커브가 낮을 것이라 예상되어 배포 자동화 또한 Github Actions로 진행하게 되었습니다.\n\n## Docker?\n\n배포 자동화 과정에서 왜 Docker가 나왔을까요?\n\nDocker는 애플리케이션을 컨테이너화하여 환경을 표준화하고 이식성을 높여줍니다. 이는 다양한 환경에서 애플리케이션을 실행할 때 발생하는 호환성 문제를 해결해줍니다.\n\n자바 애플리케이션을 실행시킨다고 생각해볼 때 서버가 하나 이상되면 OS환경, JVM버전도 모두 같다고 보장할 수 없기 때문입니다. \n\n서버가 수십개가 넘어가면 이에 대한 환경설정을 하는 것만해도 정말 많은 시간이 걸릴것입니다. 그리고 어렵게 설정한 환경설정이 충돌한다면? ...환경설정에 쏟을 시간이 너무 아깝습니다.\n\n현재 진행하고 있는 프로젝트는 서버환경을 아직 상세하게 결정하지 않은 상태이기 때문에 Docker를 이용하여 어떤 환경에도 이식 가능한 배포환경을 구축하기로 결정했습니다! \n\n## Docker 알아보기\n\n어떻게 Docker를 사용했는지 기술하기 전에 Docker를 이해하고 넘어가겠습니다.\n\n`Docker 설치 과정은 생략합니다`\n\n### Docker Image와 Docker Container\n\n![Pasted image 20240406211131](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/637acd7d-62a7-4ad0-a4cb-a46f3d1e04f3)\nDocker 이미지는 애플리케이션을 실행하는 데 필요한 모든 것을 포함하는 패키지입니다. 이는 애플리케이션의 코드, 런타임, 시스템 도구, 시스템 라이브러리 및 설정과 같은 모든 것을 포함할 수 있습니다. \n\nDocker 컨테이너는 Docker 이미지의 인스턴스로, 실행 가능한 가상화된 환경입니다. 컨테이너는 격리되어 있으며, 호스트 시스템과 독립적으로 실행됩니다. 이는 애플리케이션을 다양한 환경에서 일관되게 실행할 수 있음을 의미합니다.\n\n즉, Docker 이미지는 애플리케이션의 빌드정보를 담은 파일이며, Docker 컨테이너는 해당 이미지를 실행하여 애플리케이션을 실행하는 인스턴스입니다.\n\n간단하게 Docker로 nginx를 이미지->컨테이너로 실행하는 예제를 볼까요?\n\n`docker image pull nginx`\n\n`docker run -d -p 80:80 --name my-nginx nginx`\n\n어떠한 곳(Public Registry)에서 `nginx의 정보(이미지)를 pull`하고, \n\n이 이미지를 `백그라운드`(-d)에서, `포트를 연결`하여 (-p 80:80), \n\nmy-nginx라는 이름의(--name) `컨테이너로 실행`시키는 명령입니다.\n\n저희의 상황에서 저희가 열심히 작성하고 빌드한  jar파일을 이미지화하여 이를 컨테이너로 실행할 수 있게 하면 어떠한 환경에도 저희의 애플리케이션을 실행할 수 있겠네요!\n\n### 우리만의 이미지 만들기, Dockerfile\n\n![Pasted image 20240406210517](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/39fb0682-ca05-40cf-ae2f-940f7749d6bb)\n그럼 어떻게 우리의 jar파일을 이미지로 만들까요?\n\n개인화된 이미지를 만들기위해서는 Dockerfile로 스크립트를 작성해주고 이를 이미지화하는 과정이 필요합니다.\n\n저희의 jar파일을 이미지화하기 위한 Dockerfile을 작성해봤습니다.\n\n```Dockerfile\n# open jdk 17 버전의 환경을 구성  \nFROM openjdk:17-alpine  \n  \n# 빌드 실행파일을 복사  \nCOPY ./build/libs/tickitecking-0.0.1-SNAPSHOT.jar app.jar  \n  \n# 빌드 파일 실행 \nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n```\n\nDocker 이미지는 레이어 형태로 층을 쌓아가며 이루어지는데\n\n\n`FROM`\n\n으로 베이스 이미지를 jdk17버전으로 설정하고,\n\n`COPY` \n\n명령어로 jar 파일을 이미지 내부에 app.jar라는 이름으로 복사하고, \n\n`ENTRYPOINT`\n\n를 사용하여 컨테이너가 시작될 때 실행될 명령을 지정합니다.\n\n따라서 jdk17 버전으로 jar파일을 실행하는 명령어들이라고 보면 될 것같습니다.\n\n더 자세한 Dockerfile의 명령어는 [공식 레퍼런스](https://docs.docker.com/reference/dockerfile/)를 참조하면 좋을것 같습니다.\n\n### 커스텀한 Image를 어떠한 환경에서도 pull 할 수 있게, Docker Hub\n\n![Pasted image 20240406214028](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/69388e96-56e5-4099-8f6c-a9bd93aac754)\n(출처 : https://class101.net/products/5fc4a3b4fc231b000d85661b)\n\n위의 nginx예시에서 Public Registry를 어떠한 곳이라고 표현했는데요, pull 명령에서 어디서 이미지를 받아올지 특정하지 않으면 기본으로 Docker Hub에서 이미지를 받아 옵니다.\n\n저희는 저희의 Dockerfile을 이미지화 할건데, 당연히 Docker Hub에는 저희 이미지가 존재하지 않을 겁니다. 따라서 저희가 따로 만든 Docker Hub Repository에 이미지를 올리고, 이를 EC2와 같은 서버 환경에서 pull하는 과정이 필요합니다.\n\nDocker Hub에 가입하고 배포환경에 사용할 Repository를 설정합니다. \n이 때 id, password, username는 Github Actions의 스크립트에서 사용되므로 기억하고 있어야 합니다.\n\n## Github Actions 워크플로우 작성\n\n이렇게 작성한 Dockerfile을 파일 디렉토리의 최상단에 위치시키고 .github/workflows 디렉토리에 배포 자동화 과정에서 사용될 github actions 워크플로우 파일을 작성했습니다.\n\n```\nname: CD github Actions & Docker\n\non:\n  push:\n    branches: [ \"\bdev\" ] \n\njobs:\n  CI-CD:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up JDK 17     # JDK setting \n      uses: actions/setup-java@v3\n      with:\n        java-version: '17'\n        distribution: 'temurin'\n\n    # gradle chmod\n    - name: Grant execute permission for gradlew\n      run: chmod +x gradlew\n\n    # gradle build\n    - name: Build with Gradle\n      run: ./gradlew bootJar\n\n      # docker login\n    - name: Docker Hub Login\n      uses: docker/login-action@v2\n      with:\n        username: ${{ secrets.DOCKERHUB_ID }}\n        password: ${{ secrets.DOCKERHUB_PASSWORD }}\n\n    # docker build & push to production\n    - name: Build Docker image\n      run: |\n        docker build -t {dockerhub_username}/{image_name} .\n\t    docker push {dockerhub_username}/{image_name}\n\n    ## deploy\n    - name: Deploy\n      uses: appleboy/ssh-action@master\n      with:\n        host: ${{ secrets.HOST_DEV }} # EC2 퍼블릭 IPv4 DNS\n        username: ${{ secrets.USERNAME }} # ubuntu\n        port: 22\n        key: ${{ secrets.PRIVATE_KEY }} #pem\n        script: |\n          sudo docker stop $(docker ps -a -q)\n          sudo docker rm $(docker ps -a -q)\n          sudo docker pull {dockerhub_username}/{image_name}\n          sudo docker run -d -p 8080:8080 --name {container_name} {dockerhub_username}/{image_name}\n          sudo docker image prune -f\n```\n\n각 job의 step들을 살펴볼까요?\nsecrets.로 표현된 중괄호는 github의 secrets에 저장된 값이고 없는 중괄호는 각자의 환경에 맞게 설정해줘야 합니다. \n\n`Set up JDK 17`\n\ngithub actions의 runner서버에 jdk17을 설치합니다.\n\n`Grant execute permission for gradlew`\n\ngradle의 실행권한을 부여합니다.\n\n`Build with Gradle`\n\ngradle 빌드를 수행합니다. jar파일이 생성될 것입니다.\n\n`Docker Hub Login`\n\ngithub의 secrets에 저장된 Docker Hub의 id, password로 로그인합니다.\n\n`Build Docker image`\n\nDockerfile을 이미지화 합니다.  Docker Hub의 username을 이미지에 명시해줘야 하기 때문에 `-t`명령어로 이미지에 태그를 부여합고, Docker Hub에 해당 이미지를 push 합니다.\n\n`Deploy`\n\n예시로 EC2 환경을 설정하고 해당 [액션](https://github.com/appleboy/ssh-action)을 사용했습니다.\nscript를 간단하게 설명하면 \n\n`sudo docker stop $(docker ps -a -q)`, `sudo docker rm $(docker ps -a -q)`\n\nEC2에서 실행중인 컨테이너들을 모두 중지, 삭제 하고\n\n`sudo docker pull {dockerhub_username}/{image_name}`\n\nDocker Hub에서 새로이 빌드된 이미지를 pull하고 \n\n`sudo docker run -d -p 8080:8080 --name {container_name} {dockerhub_username}/{image_name}`\n\n새롭게 pull한 이미지를 컨테이너로 가동시킵니다. \n\n`sudo docker image prune -f`\n\n쓰이지 않는 이미지를 삭제합니다.\n\n이제 dev 브랜치에 push가 일어나면 자동으로 저희의 애플리케이션 EC2서버에 배포되고 실행될 것입니다.\n\n이렇게 docker와 github actions를 이용하여 배포 자동화를 이루어보았습니다. \n\n---\n참고\n\nhttps://velog.io/@leeeeeyeon/Github-Actions%EA%B3%BC-Docker%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-CICD-%EA%B5%AC%EC%B6%95\n\n"},{"excerpt":"학습계기 팀 프로젝트 중 테스트를 해보며 쿼리를 살펴보고 있는데 이상하게 delete 쿼리가 많이 나가는 현상을 발견했습니다. Pasted image 20240321160655 문제가 되는 repository의 코드입니다. 카드에 할당자가 여러명 존재할 수 있기 때문에 카드를 삭제할때 해당 카드에 할당된 사용자를 모두 삭제해줘야 했는데,  처음 생각은 d…","fields":{"slug":"/jpa_deleteAll/"},"frontmatter":{"date":"March 31, 2024","title":"JPA deleteAll()을 사용할 시 문제점","tags":["jpa"]},"rawMarkdownBody":"\n## 학습계기\n\n팀 프로젝트 중 테스트를 해보며 쿼리를 살펴보고 있는데 이상하게 delete 쿼리가 많이 나가는 현상을 발견했습니다.\n\n![Pasted image 20240321160655](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/da7e70e0-3700-4c46-b0d8-389a35a193b1)\n\n문제가 되는 repository의 코드입니다.\n```java\n@Override  \npublic void deleteCard(Long cardId) {  \n    cardJpaRepository.deleteById(cardId);  \n    assignJpaRepository.deleteAllByCardId(cardId);  \n}\n```\n\n카드에 할당자가 여러명 존재할 수 있기 때문에 카드를 삭제할때 해당 카드에 할당된 사용자를 모두 삭제해줘야 했는데, \n\n처음 생각은 deleteAll이면 쿼리로 `delete from assigns where card_id = ?`로 하나의 쿼리가 나갈 줄 알았는데 아니었습니다.\n\n## 분석\n\n이유를 알아보기 위해 deleteAll()을 수행하는 jpaRepository가 상속하는 ListCrudRepository를 살펴보았습니다. ListCrudRepository는 CrudRepository를 상속받고 있었는데요, 해당 인터페이스에서 \n\n```java\nvoid deleteAll(Iterable<? extends T> entities);\n```\n엔티티 목록을 삭제하는 메서드를 찾을 수 있었고  \n해당 인터페이스의 메서드는 SimpleJpaRepository에서 구현하고 있었습니다.\n\n(SimpleJpaRepository의 deleteAll 구현 메서드)\n```java\n@Transactional  \npublic void deleteAll(Iterable<? extends T> entities) {  \n    Assert.notNull(entities, \"Entities must not be null\");  \n    Iterator var3 = entities.iterator();  \n  \n    while(var3.hasNext()) {  \n        T entity = (Object)var3.next();  \n        this.delete(entity);  \n    }  \n  \n}\n//\n@Transactional  \npublic void delete(T entity) {  \n    Assert.notNull(entity, \"Entity must not be null\");  \n    if (!this.entityInformation.isNew(entity)) {  \n        Class<?> type = ProxyUtils.getUserClass(entity);  \n        T existing = this.entityManager.find(type, this.entityInformation.getId(entity));  \n        if (existing != null) {  \n            this.entityManager.remove(this.entityManager.contains(entity) ? entity : this.entityManager.merge(entity));  \n        }  \n    }  \n}\n```\ndeleteAll이 iterator를 통해 파라미터로 전달된 엔티티 목록을 순회하며 this.delete메서드를 호출하고 있는 것을 확인할 수 있었습니다.\n\n또한 this.delete(entity)에서 호출하는 delete메서드를 살펴보면 isNew()를 통해 파라미터의 엔티티가 1차 캐시에 존재하는지 확인하고 그렇지 않다면 entityManager.find()를 통해 데이터베이스에 select 쿼리를 보내는 구조라는 것을 알 수 있습니다.\n\n따라서 이 deleteAll()이라는 쿼리 메서드를 사용하면 저의 의도와는 다르게 select 쿼리 + n개의 delete쿼리가 다량 발생하는 문제가 발생했습니다.\n\n## 해결 \n\n해당 쿼리 메서드의 문제점을 파악하고 jpql로 벌크연산 쿼리를 작성하였습니다. \n```java\n@Modifying(clearAutomatically = true)\n@Query(\"delete from AssignEntity a where a.cardId = :cardId\") \nvoid deleteAllByCardId(@Param(\"cardId\") Long cardId);\n```\n\n벌크성 연산임을 알리기 위해 @Modifying 어노테이션을 추가해주었고, 영속성 컨텍스트를 거치는 것이 아니라 데이터베이스에 바로 쿼리를 날리기 때문에 영속성 컨텍스트에 있는 데이터와 정합성을 해칠 수 있기 때문에 (삭제 했는데 조회가 된다던가) clearAutomatically = true로 해당 메서드 수행 후 영속성을 초기화해주게 설정했습니다.\n\njpql로 메서드 변경후 하나의 쿼리만 나가는 것을 확인했습니다.\n![Pasted image 20240321160245](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/01b4a536-2749-41c4-9892-b390b154ac4d)\n\n## deleteAllinBatch() 잠깐 살펴보기\n\n이렇게 문제를 해결했지만 SimpleJpaRepository의 메서드들을 살펴보며deleteAllInBatch(), deleteAllByIdInBatch()가 눈에 띄었습니다.\n\n```java\n@Transactional  \npublic void deleteAllInBatch(Iterable<T> entities) {  \n    Assert.notNull(entities, \"Entities must not be null\");  \n    if (entities.iterator().hasNext()) {  \n        QueryUtils.applyAndBind(QueryUtils.getQueryString(\"delete from %s x\", this.entityInformation.getEntityName()), entities, this.entityManager).executeUpdate();  \n    }  \n}\n\n@Transactional  \npublic void deleteAllByIdInBatch(Iterable<ID> ids) {  \n    Assert.notNull(ids, \"Ids must not be null\");  \n    if (ids.iterator().hasNext()) {  \n        if (this.entityInformation.hasCompositeId()) {  \n            List<T> entities = new ArrayList();  \n            ids.forEach((id) -> {  \n                entities.add(this.getReferenceById(id));  \n            });  \n            this.deleteAllInBatch(entities);  \n        } else {  \n            String queryString = String.format(\"delete from %s x where %s in :ids\", this.entityInformation.getEntityName(), this.entityInformation.getIdAttribute().getName());  \n            Query query = this.entityManager.createQuery(queryString);  \n            if (Collection.class.isInstance(ids)) {  \n                query.setParameter(\"ids\", ids);  \n            } else {  \n                Collection<ID> idsCollection = (Collection)StreamSupport.stream(ids.spliterator(), false).collect(Collectors.toCollection(ArrayList::new));  \n                query.setParameter(\"ids\", idsCollection);  \n            }  \n  \n            this.applyQueryHints(query);  \n            query.executeUpdate();  \n        }  \n  \n    }  \n}\n```\ndeleteAllInBatch() 메서드의 내부를 살펴보니 QueryUtils로 제가 원하던 `delete from` 쿼리를 생성하고 executeUpdate()로 실행하는 메서드로 파악했습니다.\n\ndeleteAllByIdInBatch()는 일단 복합키인지를 확인하고 복합키라면 id에 해당하는 엔티티를 찾아서 deleteAllInBatch()를 사용하고 아니라면 바로 `delete from`쿼리를 실행하는 메서드로 보입니다!\n\nid리스트로 삭제가 필요하던가, 엔티티 리스트를 삭제할때는 deleteAllInBatch()를 사용하면 좋을 것 같습니다.\n\n\n"},{"excerpt":"MySQL InnoDB스토리지 엔진으로 진행된 글입니다. 인덱스란? 인덱스는 데이터베이스에서 검색 속도를 향상시키기 위해 사용되는 데이터 구조다. 지정한 컬럼들을 기준으로 메모리 영역에서 일종의 목차를 생성하는 것과 비슷하다. 이렇게 생성된 목차를 통해 검색시 전체 테이블을 스캔하는 대신 목차를 사용하여 원하는 결과를 빠르게 찾을 수 있다. 왜 인덱스를 …","fields":{"slug":"/database_index/"},"frontmatter":{"date":"March 24, 2024","title":"인덱스와 인덱스 적용기","tags":["database","index"]},"rawMarkdownBody":"\n\nMySQL InnoDB스토리지 엔진으로 진행된 글입니다.\n\n## 인덱스란? \n\n인덱스는 데이터베이스에서 검색 속도를 향상시키기 위해 사용되는 데이터 구조다.\n\n지정한 컬럼들을 기준으로 메모리 영역에서 일종의 목차를 생성하는 것과 비슷하다.\n\n이렇게 생성된 목차를 통해 검색시 전체 테이블을 스캔하는 대신 목차를 사용하여 원하는 결과를 빠르게 찾을 수 있다.\n\n### 왜 인덱스를 사용해야할까?\n\n인덱스는 처음 생성하는데 시간이 많이 소요될수도 있고, 새로운 목차를 생성하는 것이기 때문에 추가 저장공간이 필요하다. \n\n보통은 데이터베이스의 10% 정도의 추가 공간이 필요하다고 한다. \n또한 insert, update, delete와 같이 데이터 변경작업이 자주 일어날 경우 오히려 성능이 나빠질수도 있다.\n\n하지만 일반적인 애플리케이션에서 select는 insert delete보다 훨씬 더 많이 발생한다. \n\n즉 조회를 얼마나 빨리 할수 있느냐가 전체 애플리케이션 성능에 지배적인 영향을 끼칠수 밖에 없다. 따라서 위의 단점을 고려한 적절한 인덱스 설정이 쿼리 최적화의 첫걸음이라고 할 수 있다.\n\n## 이미 인덱스를 사용하고 있다?\n\n다음과 같은 테이블이 있다\n\n(실험을 위해 데이터를 10만개 정도 넣은 상태이다)\n![Pasted image 20240324141013](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/6e1c1612-4e3b-44d2-bff4-14f275650441)\n\n생성된 테이블에 `SHOW INDEX FROM {tableName}`명령을 통해 테이블에 생성된 인덱스를 확인할 수 있다.\n\n`show index from cards;`\n\n![Pasted image 20240324141045](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/b463b66d-a2ac-4f4a-9068-b9cc98ab979c)\n\n인덱스를 설정하는 추가적인 작업이 없었는데 왜 인덱스가 card테이블에 있는걸까?\n\n## B-Tree\n\n![Pasted image 20240324172257](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/7c04c5a3-f31d-430e-809d-59ed1a642605)\n(출처 :https://builtin.com/data-science/b-tree-index)\n\n데이터베이스는 일반적으로 많은 데이터를 저장하기 위해 B-Tree구조로 이루어져 있다.\n\nB-Tree는 이진트리와 유사하지만 다른 점을 가지는데 자식 노드를 2개 이상 가질 수 있다는 점이다. 노드의 개수가 늘어나면서 자연스레 트리의 높이가 낮아지고 빠른 탐색 속도를 보장한다.\n\n또한 B-Tree에서 말단 노드를 리프노드라 하는데 리프노드가 같은 레벨을 유지함으로써 편향된 트리가 선형탐색시간으로 치중되는 단점을 보완한다.\n\n### 인덱스와 B-Tree\n\nMySQL은 B-Tree의 노드에 해당하는 개념을 페이지로 구현한다. \n페이지는 키와 다음 페이지를 가리키는 주소로 이루어져 있고 키는 정렬되어 있다.\n\n![Pasted image 20240324173414](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/4073d433-1802-464b-b900-681529a2911a)\n\nHHH라는 데이터를 찾을때 B-Tree구조로 이루어져 있다면 AAA->FFF->(페이지 이동) -> FFF -> HHH 5번의 검색으로 찾을 수 있기 때문에 선형탐색보다 훨씬 검색 성능이 우월하다.\n\n하지만 insert작업시 리프 페이지가 꽉찼다면 새로운 페이지가 생성되어야 하고 이는 주소값을 가지는 중간 노드- 루트 노드의 페이지 증가로 연쇄되기 때문에 많은 비용이 발생하며 이를 '페이지 분할 작업'이라고 한다. \n\n###  클러스터형 인덱스과 논클러스터형 인덱스\n\n- 클러스터형 인덱스 \n\n테이블 전체가 정렬된 인덱스가 되는 방식의 인덱스 종류이다. \n테이블에 Primary Key를 지정하게 되면 클러스터링 인덱스를 형성하게 된다. \n정렬 되어 있지 않았던 테이블이 Primary Key를 기준으로 정렬된다.\n\n![Pasted image 20240324191104](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/8835a7ae-9fbb-41dc-a7c2-792950c99e9b)\n클러스터형 인덱스는 리프페이지가 실제 데이터 페이지이다. \n\n Primary Key를 지정하지 않는다면 Unique + Not null 컬럼을 Primary Key처럼 사용하게 된다.\n\n즉, Cards테이블을 만들 때 id를 Primary Key로 설정해줬기 때문에 자동으로 클러스터형 인덱스를 생성하게 된 것이다.\n\n클러스터형 인덱스는 테이블 당 하나만 가질 수 있다.(물리적 정렬기준이 하나여야 하므로)\n\n- 논클러스터형 인덱스\n\n물리적으로 테이블을 정렬하지 않고 대신 정렬된 별도의 인덱스 페이지를 생성한다.\n클러스터형 인덱스와 달리 테이블 하나에 여러개를 가질 수 있다. \n\n![Pasted image 20240324192858](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/354ec545-68ea-4f66-9c62-587f57106ba3)\n논클러스터형 인덱스의 리프 페이지는 실제 데이터의 주소 페이지 이다. \n실제 데이터 페이지의 주솟값 + 오프셋으로 구성되어 있다.\n\n테이블 생성시 Unique 제약을 걸거나 `create index`등의 명령어로 논클러스터형 인덱스를 생성할 수 있다.\n\n- 클러스터형 인덱스와 논클러스터형 인덱스의 조합\n\n대개의 경우에는 한 테이블에 PK + Unique or 별도의 인덱스를 생성하므로 클러스터형 인덱스와 논클러스터형 인덱스가 혼합되어 있다.\n\n![Pasted image 20240324193515](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/ec9e636b-4854-45c1-8f8c-0d0f9ebfa2b0)\n\n이 경우 조회시 `논클러스터형 인덱스` -> `클러스터형 인덱스` 의 순서로 조회가 발생한다. \n이 때 논클러스터형 인덱스는 실제 데이터의 주소와 오프셋을 가지는 대신 클러스터형 인덱스가 적용된 컬럼의 값을 가지게 된다.\n\n테이블에 데이터가 변경될 시 논클러스터형 인덱스가 테이블의 페이지번호와 오프셋을 가지고 있게 되면 논클러스터형 인덱스를 모두 수정해야 하기 때문이다. \n\n## 인덱스 적용하기\n\n다시 Card테이블로 돌아와서 인덱스를 적용해보자.\ncard는 board_id를 가지고 보드에 있는 카드를 조회하는 경우가 많으므로 \nwhere 절에 board_id를 가지고 조회하는 쿼리가 많이 발생하게 된다.\n\n`where board_id = ?`의 쿼리를 실행했을 때 테이블 풀스캔으로 10만개의 데이터를 모두 찾는 실행계획을 볼 수 있다.\n![Pasted image 20240321144818](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/9bbab62e-4754-4f8f-b75f-8520b21ce670)\n\n간단한 테스트로 조회속도를 측정해보았다.\n```java\n@DisplayName(\"조회 속도 검사\")  \n@Test  \nvoid count_search_time() {  \n    // 현재 시간 기록  \n    LocalDateTime before = LocalDateTime.now();  \n  \n    // 조회할 쿼리 실행  \n    Query query = em.createQuery(\"select c from CardEntity c where c.boardId = 9999\");  \n    List<CardEntity> resultList = query.getResultList();  \n  \n    // 쿼리 실행 후 현재 시간 기록  \n    LocalDateTime after = LocalDateTime.now();  \n  \n    // 쿼리 실행 시간 계산  \n    Duration duration = Duration.between(before, after);  \n    long seconds = duration.getSeconds();  \n    long milliseconds = duration.toMillis();  \n  \n    // 결과 출력  \n    System.out.println(\"카드를 조회하는데 걸린 시간: \" + seconds + \"초 \" + milliseconds + \"밀리초\");  \n}\n```\n\n![Pasted image 20240321143628](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/41b95df0-b02f-489b-a9d8-eed2810772fb)\n\n그럼 card테이블에 board_id를 인덱스로 적용해보자 \n`CREATE INDEX idx_board_id ON cards(board_id)`로 인덱스를 등록할 수 있다. \n\n(인덱스가 생성된 모습)\n![Pasted image 20240324165228](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/05093475-87ca-47d0-b12e-b45d349e4505)\n\n그럼 인덱스 적용 후의 실행계획을 살펴보자 \n\n(인덱스 적용 후 실행계획)\n![Pasted image 20240321144741](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/8d564e70-fcc1-4f7e-a377-a0dbb7980c6e)\nusing idx_board_id, 즉 인덱스를 타고 조회를 했음을 알 수 있다.\n실행 계획의 시간이 적용 전보다 훨씬 많이 줄었음을 확인할 수 있다.\n\n![Pasted image 20240321144941](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/dccbc1b3-485a-4690-a89d-877b5784c3fc)\n조회 테스트의 시간도 절반 가량으로 줄어들었음을 확인 할 수 있다.\n\n이를 클러스터 인덱스와 논클러스터 인덱스로 보자면 논클러스터 인덱스인 `idx_board_id`를 통해 클러스터형 인덱스인 `card_id`의 pk값을 찾고 이에 해당하는 데이터의 주솟값으로 이동해 실제 데이터를 읽는 작업이라고 할 수 있겠다.\n\n---\n참고\n\n이것이 MySQL이다 - 우재남 저\n\nhttps://builtin.com/data-science/b-tree-index"},{"excerpt":"문제를 해결한 코드링크 페이징 처리의 필요성 성능: 대량의 데이터를 한 번에 로드하면 메모리 부족이나 느린 쿼리 실행으로 인해 성능이 저하될 수 있다. 페이징 처리를 사용하면 사용자가 필요로 하는 작은 일부 데이터만 로드하여 성능을 향상시킬 수 있다. 사용자 경험: 사용자가 대량의 데이터를 한 번에 볼 필요는 없으며, 보통은 페이지별로, 특히 최근기록 위…","fields":{"slug":"/pagination_fetchjoin/"},"frontmatter":{"date":"March 14, 2024","title":"Spring Data JPA의 페이징처리와 fetch join시의 문제점","tags":["jpa","querydsl","pagination"]},"rawMarkdownBody":"\n\n*문제를 해결한 [코드](https://github.com/jinkshower/Todo-management)링크*\n\n## 페이징 처리의 필요성\n\n1. **성능**: 대량의 데이터를 한 번에 로드하면 메모리 부족이나 느린 쿼리 실행으로 인해 성능이 저하될 수 있다. 페이징 처리를 사용하면 사용자가 필요로 하는 작은 일부 데이터만 로드하여 성능을 향상시킬 수 있다.\n    \n2. **사용자 경험**: 사용자가 대량의 데이터를 한 번에 볼 필요는 없으며, 보통은 페이지별로, 특히 최근기록 위주로 조회한다.\n    \n3. **네트워크 부하 감소**: 대량의 데이터를 한 번에 전송하면 네트워크 부하가 증가할 수 있다. 페이징 처리를 사용하여 각 페이지마다 필요한 데이터만 전송해 네트워크 오버헤드를 감소 시킬 수 있다.\n\n## Spring Data Jpa에서 페이징 처리하기\n\nJpaRepository는 PagingAndSortingRepository를 상속받고 있는데, 이를 사용하면 간편하게 페이징된 데이터를 조회할 수 있다.\n\n간단한 페이징 예제코드\n```java\npublic interface UserRepository extends JpaRepository<User, Long> {  \n\tPage<User> findAll(Pageable pageable);\n}\n\n@Service  \npublic class UserService {  \n  \n    @Autowired  \n    private UserRepository userRepository;  \n  \n    public Page<User> getUsers(int page, int size) {  \n        Pageable pageable = PageRequest.of(page, size, Sort.by(\"createdAt\").descending());  \n        return userRepository.findAll(pageable);  \n    }  \n}\n```\n\n 현재 페이지와 페이지에 들어갈 데이터의 양, 정렬기준을 정해서 PageRequest의 스태틱 메서드를 사용해 Pageable을 구현할 수 있다. 이를 JpaRepository의 파라미터로 넘겨줄 시 `Page<T>`로 반환결과를 받을 수 있다\n\n위 코드 실행시 \n`SELECT * FROM user ORDER BY created_at DESC OFFSET ? LIMIT ?`\n라는 쿼리로 offset과 limit을 실행하는 것을 확인할 수 있다. \n\n### `Slice<T> 와 Page<T>의 차이점`\n\n위 코드는 반환타입으로 Page를 받았지만 `Slice<T>`로도 페이징 처리된 객체를 받을 수도 있다.\n`Page<T>`와 `Slice<T>`의 가장 큰 차이점은 count 쿼리가 날아가냐, 아니냐의 차이이다.\n\n`Page<T>`는 전체 페이지의 수를 포함한 페이징된 데이터를 반환한다.\n즉 totalCount를 함께 조회하는 쿼리를 실행하여 결과에 포함시킨다.\n게시판 형태의 페이징에 적합하다.\n\n`Slice<T>`는 limit+1을 조회하여 다음 페이지의 존재여부만 확인한다.\n따라서 totalCount를 함께 조회하는 쿼리를 실행하지 않는다.\n더보기, 무한 스크롤 형태의 페이징에 적합하다. \n\n## QueryDSL에서 페이징 적용하기\n\n동적쿼리 작성을 위해 QueryDSL을 사용할 경우 쿼리 메서드 체이닝에 `.offset()` 과 `.limit()` 을 추가하여 페이징 처리를 할 수 있다.\n\n```java\npublic Page<Todo> findAll(Pageable pageable) {  \n    // 페이징 정보를 적용하여 쿼리 실행  \n    List<Todo> todos = queryFactory  \n        .select(todo)  \n        .from(todo)  \n        .offset(pageable.getOffset())  \n        .limit(pageable.getPageSize())  \n        .fetch();  \n  \n    // 전체 개수를 조회  \n    long totalSize = queryFactory  \n        .select(todo.count())  \n        .from(todo)  \n        .fetchFirst();  \n  \n    // 페이징 처리된 결과를 Page 객체로 변환하여 반환  \n    return new PageImpl<>(todos, pageable, totalSize);  \n}\n```\n\nreturn 부분은 \n`return PageableExecutionUtils.getPage(todos, pageable, () -> totalSize);`로 유틸리티 메서드를 사용해도 된다. (내부에서 PageImpl을 생성하므로 실제 동작은 같다)\n\n## 일대다 관계의 fetch join\n\n[이전글](https://jinkshower.github.io/querydsl_nplusone/) 에서 N+1 문제를 해결하기 위해 queryDSL의 fetchjoin()을 사용한 기록을 남긴 적이 있다. \n자세한 설명없이 코드에 distinct()를 추가했는데 이는 일대다 관계에서 fetch join할 시의 문제점 때문이었다.\n\n- 데이터 중복\n일대다 관계에서 fetch join을 사용하면 일의 엔티티와 다의 엔티티들과 조인된다. 이 때 일의 엔티티가 다의 엔티티 수만큼 중복되어 반환된다.\n\n예를 들어 team(1)와 members(다)의 관계가 있을때 \n```java\nselect(team)\n.from(team)\n.leftJoin(team.members).fetchJoin()\n.where(team.name.eq(\"팀A\"))\n```\n을 할 경우 \n![Pasted image 20240314170134](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/69d8a341-e79e-49b8-8d04-274902180bad)\n위의 그림 처럼 일(팀)에 맞는 다의 member를 조회하기 때문에 팀이름이 중복되어 나타난다.\n\n따라서 \n```java\nList<Team> teams = queryFactory \n.select(team) \n.distinct() // 중복된 결과를 제거\n.from(team) \n.leftJoin(team.members, member).fetchJoin() \n.where(team.name.eq(\"팀A\")) \n.fetch();\n```\n처럼 distinct()를 사용하여 중복을 제거해 줘야 한다.\n\n## 일대다 fetchJoin과 페이징 처리\n\n그렇다면 distinct()를 사용하면 일대다 관계에서 페이징처리를 사용할 수 있을까? \n\ndistinct()는 애플리케이션단에서 중복을 제거하는 것이기 때문에 데이터베이스로 날아가는 쿼리에는 영향을 주지 못한다. \n\n실제로 일대다 fetch join과 페이징 처리를 같이 사용해보자.\n```java\n    @Override  \n    public Page<Todo> findAllByOrderByCreatedAtDesc(Pageable pageable) {  \n        List<Todo> fetch = queryFactory.select(todo)\n\t        .distinct()  \n            .from(todo)  \n            .join(todo.user, user).fetchJoin()  \n            .leftJoin(todo.comments, comment).fetchJoin()  \n            .orderBy(todo.createdAt.desc())  \n            .offset(pageable.getOffset())  \n            .limit(pageable.getPageSize())  \n            .fetch();  \n        long totalSize = queryFactory  \n            .select(todo.count())  \n            .from(todo)  \n            .fetchFirst();  \n        return PageableExecutionUtils.getPage(fetch, pageable, () -> totalSize);  \n    }\n```\n\n(테스트 코드)\n```java\n    @Test  \n    void pageTest() {  \n        for (int i = 0; i < 15; i++) {  \n            todoRepository.save(Todo.builder()  \n                .title(TEST_TODO_TITLE)  \n                .content(TEST_TODO_CONTENT + 1)  \n                .user(TEST_USER)  \n                .likeCount(0L)  \n                .build());  \n        }  \n  \n        System.out.println(todoRepository.findAll().size());  \n        Page<Todo> found = todoRepository.findAllByOrderByCreatedAtDesc(  \n            PAGE_DTO.toPageable());  \n        Pageable pageable = found.getPageable();  \n        Sort sort = pageable.getSort();  \n        // 페이지 정보  \n        System.out.println(\"Sort (Sorted): \" + sort.isSorted());  \n        System.out.println(\"Sort (Unsorted): \" + sort.isUnsorted());  \n        System.out.println(\"Sort (Empty): \" + sort.isEmpty());  \n        System.out.println(\"Page Size: \" + pageable.getPageSize());  \n        System.out.println(\"Page Number: \" + pageable.getPageNumber());  \n        System.out.println(\"Offset: \" + pageable.getOffset());  \n        System.out.println(\"Is Paged: \" + pageable.isPaged());  \n        System.out.println(\"Is Unpaged: \" + pageable.isUnpaged());  \n  \n        // 전체 페이지 정보  \n        System.out.println(\"Total Pages: \" + found.getTotalPages());  \n        System.out.println(\"Total Elements: \" + found.getTotalElements());  \n        System.out.println(\"Is Last Page: \" + found.isLast());  \n        System.out.println(\"Current Page Number: \" + found.getNumber() + 1);  \n        System.out.println(\"Is First Page: \" + found.isFirst());  \n        System.out.println(\"Is Empty: \" + found.isEmpty());  \n        System.out.println(\"Size: \" + found.getSize());  \n        System.out.println(\"Number Of Elements: \" + found.getNumberOfElements());  \n    }\n```\n\n![Pasted image 20240314172138](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/ec87d6f5-5b59-46ad-b098-7e8c86037b03)\n결과는 Pageable에서 명시한대로 페이징처리가 되지만\n\n![Pasted image 20240314172411](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/fcafb7e9-b520-42ec-8102-fe1a513f5c64)\nlimit 이나 offset이 없는 쿼리가 날아감을 볼수 있고 \n`firstResult/maxResults specified with collection fetch; applying in memory`라는 경고 메시지가 나오는 것을 확인할 수 있다. \n\n즉, 일대다에서 다에 해당하는 컬렉션을 모두 메모리에 적재해서 가져오고 그 다음에 페이징처리를 하기 때문에 메모리에 과부하가 갈 수 있다는 뜻이다.\n\n## 해결책1. 페이지네이션 쿼리와 fetch join쿼리를 나눈다\n\nfetch join과 페이지네이션을 동시에 사용할 수 없기 때문에 fetch join을 사용하지 않은 쿼리로 페이지네이션을 적용한 todo의 id를 쿼리하고 그 id리스트를 fetchjoin에서 in절에 사용한다면 위와 같은 문제가 일어나지 않을 거라고 생각했다. \n\n```java\npublic Page<Todo> findAllByOrderByCreatedAtDesc(Pageable pageable) {  \n    JPAQuery<Long> idQuery = queryFactory.select(todo.id) //id조회\n        .from(todo)  \n        .orderBy(todo.createdAt.desc())  \n        .offset(pageable.getOffset())  \n        .limit(pageable.getPageSize());  \n    List<Long> ids = idQuery.fetch();  \n    JPAQuery<Todo> query = queryFactory.select(todo)  \n        .from(todo)  \n        .join(todo.user, user).fetchJoin()  \n        .leftJoin(todo.comments, comment).fetchJoin()  \n        .orderBy(todo.createdAt.desc())  \n        .where(todo.id.in(ids));  //id로 조회\n    List<Todo> fetch = query.fetch();  \n    long totalSize = queryFactory.select(todo.id.countDistinct())  \n        .from(todo)  \n        .fetch().get(0);  \n    return PageableExecutionUtils.getPage(fetch, pageable, () -> totalSize);  \n}\n```\n\n![Pasted image 20240314173731](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/a14436e5-36f7-4f31-80d7-fe4c0d5ae833)\nid쿼리에서 페이지네이션이 적용되고\n\n![Pasted image 20240314173817](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/a6361d53-555b-431f-aaf3-909f7ef58aff)\nid로 fetch join을 수행할때의 where 절\n\n하지만 해당 방법은 다른 메서드에서도 fetch join을 사용할 경우 id를 조회하는 쿼리를 따로 작성해줘야 하는 불편함이 있다\n\n## 해결책2. BatchSize적용\n\nBatchSize는 부모 엔티티를 조회할때 연관된 자식 엔티티의 수를 제한하는 기능이다.\n부모에서 자식엔티티 그래프를 탐색할때 N+1처럼 select가 부모엔티티의 수만큼 나가는게 아니라 이미 조회한 엔티티의 식별자 값을 모아서 where절에 자식 엔티티 조회 쿼리를 하나로 처리한다.\n따라서 쿼리를 하나 추가함으로써 BatchSize에 명시한 만큼의 자식 엔티티를 한번에 조회할 수 있다.\n\nBatchSize는 일대다 관계의 컬렉션에 어노테이션을 직접 명시하거나\n```java\n@BatchSize(size = 100)  \n@OneToMany(mappedBy = \"todo\", cascade = {CascadeType.PERSIST,  \n    CascadeType.REMOVE})  \nprivate List<Comment> comments = new ArrayList<>();\n```\n\n설정 파일에 해당 설정을 추가해서 어플리케이션 전체에서 사이즈를 제한할 수도 있다\n```java\nspring.jpa.properties.hibernate.default_batch_fetch_size=100\n```\n\n(배치 사이즈 적용 후 쿼리)\n```java\n    public Page<Todo> findAllByOrderByCreatedAtDesc(Pageable pageable) {  \n        List<Todo> fetch = queryFactory.select(todo)  \n            .from(todo)  \n            .orderBy(todo.createdAt.desc())  \n            .offset(pageable.getOffset())  \n            .limit(pageable.getPageSize())  \n            .fetch();  \n        long totalSize = queryFactory  \n            .select(todo.count())  \n            .from(todo)  \n            .fetchFirst();  \n        return PageableExecutionUtils.getPage(fetch, pageable, () -> totalSize);  \n    }\n```\nfetch join을 사용할 필요가 없어졌다. 이미 지연로딩에 대한 대비책으로 batchsize를 두었기 때문이다. \n\n![Pasted image 20240314180406](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/b62e6d86-0a7d-4ddb-aef4-958582fe47f7)\n\n배치 사이즈 추가로 나가는 in 쿼리\n![Pasted image 20240314175431](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/d424779d-0804-4cdd-a227-df13995e8c17)\n\n## 마치며\n\n이렇게 Spring Data Jpa에서 페이징 처리와 일대다 관계에서 페이징 처리시 주의점과 해결책을 알아보았다. \n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n참고\n\n자바 ORM 표준 JPA 프로그래밍 - 김영한 저"},{"excerpt":"리팩토링 과제 중 검색기능을 Querydsl로 개선하면서 만난 N+1문제를 해결한 기록 코드링크 검색기능 jpa에 대한 학습이 부족하던 떄, 과제에서 검색기능을 만든 경험이 있다. (서비스 호출메서드 생략) SearchFilter를 전략패턴과 어댑터패턴을 활용하여 필터 기능을 구현한 코드였다.\n그렇게 나쁜 코드는 아니지만 단점이 많은 코드라는 생각이 들었…","fields":{"slug":"/querydsl_nplusone/"},"frontmatter":{"date":"March 10, 2024","title":"Querydsl과 JPA에서의 N+1문제","tags":["jpa","querydsl"]},"rawMarkdownBody":"\n\n리팩토링 과제 중 검색기능을 Querydsl로 개선하면서 만난 N+1문제를 해결한 기록 [코드링크](https://github.com/jinkshower/Todo-management)\n\n## 검색기능\n\njpa에 대한 학습이 부족하던 떄, 과제에서 검색기능을 만든 경험이 있다.\n```java\n    @GetMapping(\"/todos/filter\")\n    public ResponseEntity<CommonResponse<List<TodoResponseDto>>> getFilteredTodos(\n            @RequestParam(defaultValue = \"false\") Boolean completed,\n            @RequestParam(required = false) Long userId,\n            @RequestParam(required = false) String title,\n            @Login UserDto userDto) {\n        List<TodoResponseDto> todoResponseDtos = todoService.getFilteredTodos(completed, userId, title, userDto);\n        return ResponseEntity.ok().body(CommonResponse.<List<TodoResponseDto>>builder()\n                        .statusCode(HttpStatus.OK.value())\n                        .message(\"검색 결과가 조회되었습니다.\")\n                        .data(todoResponseDtos).build());\n    }\n```\n(서비스 호출메서드 생략)\n```java\npublic class Todos {\n\n    private final List<Todo> todos;\n    private final List<SearchFilter> filters = new ArrayList<>();\n\n    public Todos(List<Todo> todos) {\n        this.todos = new ArrayList<>(todos);\n        initializeFilters();\n    }\n\n    private void initializeFilters() {\n        filters.add(new AuthorSearchFilter());\n        filters.add(new TitleSearchFilter());\n        filters.add(new StatusSearchFilter());\n    }\n\n    public List<Todo> filter(Object ...parameters) {\n        List<Todo> filtered = new ArrayList<>(todos);\n\n        for (Object object: parameters) {\n            if (object == null) {\n                continue;\n            }\n            SearchFilter searchFilter = findFilter(object);\n            filtered = searchFilter.apply(filtered, object);\n        }\n\n        return filtered.stream()\n                .sorted(Comparator.comparing(Timestamped::getCreatedAt).reversed())\n                .toList();\n    }\n\n    private SearchFilter findFilter(Object object) {\n        return filters.stream()\n                .filter(filter -> filter.supports(object))\n                .findFirst()\n                .orElseThrow(() -> new InvalidInputException(\"유효한 입력이 아닙니다.\"));\n    }\n}\n```\n\nSearchFilter를 전략패턴과 어댑터패턴을 활용하여 필터 기능을 구현한 코드였다.\n그렇게 나쁜 코드는 아니지만 단점이 많은 코드라는 생각이 들었다.\n\n### 문제점\n\n1. Controller에서 @RequestParam으로 검색 조건을 받고 있기 때문에 검색조건이 늘어나면 파라미터가 계속 늘어날 수 있다.\n2. 가변인자 Object로 검색조건들을 받고 이에 맞는 filter를 선택해야하는데 지금은 모두 타입이 다르지만 같은 타입의 파라미터가 추가되어야하면 이를 다른 클래스로 구현해야한다.\n3. 검색조건이 늘어나면 그만큼 필터의 구현클래스도 늘어나야한다.\n\n즉, 확장하는데 불편함이 많은 코드다.\n\n그렇게 아쉬운 마음을 가진채로 공부를 이어가다 jpa에서 동적쿼리를 사용할 수 있는 방법을 알게 되었고, 해당 기능을 개선하기로 했다.\n\nJpql자체로 혹은 Criteria, Specification을 사용하는 방법이 있는데 이에 대한 자세한 설명은 [링크](https://tecoble.techcourse.co.kr/post/2022-10-11-jpa-dynamic-query/)에 잘 설명이 되어있다. \n\n나는 가장 가독성이 좋다고 느껴진 Querydsl을 활용했다.\n\n## Querydsl 적용하기\n\nQuerydsl을 적용하는데 다양한 방법이 있지만, 나는 JpaRepository 인터페이스가 제공하는 기본 CRUD는 사용하되 동적쿼리가 필요한 메서드는 직접 구현하고 싶었다.\n\n그래서 TodoQueryRepository라는 인터페이스에 동적쿼리가 필요한 메서드를 작성하고 이를 TodoRepositoryImpl에서 구현하였다.\n\n레포지토리+Impl이라는 명명 규칙을 지킬경우 Spring Data Jpa가 자동으로 빈으로 등록해주기 때문에 기본 인터페이스 메서드 + 구현의 동적쿼리를 함께 주입받아 하나의 객체로 사용할 수 있다.\n\nJPAQueryFactory는 [동욱님의 글](https://jojoldu.tistory.com/372)을 참고하도록 하자\n\n(작성한 Impl 코드)\n```java\n@RequiredArgsConstructor  \npublic class TodoRepositoryImpl implements TodoQueryRepository {  \n  \n    private final JPAQueryFactory queryFactory;  \n  \n    public List<Todo> findAllByOrderByCreatedAtDescc() {  \n        return queryFactory  \n            .select(todo)  \n            .from(todo)   \n            .orderBy(todo.createdAt.desc())  \n            .fetch();  \n    }  \n  \n    public List<Todo> searchByFilter(TodoSearchFilter todoSearchFilter) {  \n        return queryFactory  \n            .select(todo)  \n            .from(todo)  \n            .where(  \n                eqAuthor(todoSearchFilter.getUserId()),  \n                eqTitle(todoSearchFilter.getTitle()),  \n                eqStatus(todoSearchFilter.getTodoStatus())  \n            )  \n            .fetch();  \n    }  \n  \n    private BooleanExpression eqTitle(String title) {  \n        if (StringUtils.isEmpty(title)) {  \n            return null;  \n        }  \n        return todo.title.eq(title);  \n    }  \n  \n    private BooleanExpression eqAuthor(Long userId) {  \n        if (userId == null) {  \n            return null;  \n        }  \n        return todo.id.eq(userId);  \n    }  \n  \n    private BooleanExpression eqStatus(TodoStatus status) {  \n        return todo.todoStatus.eq(status);  \n    }  \n}\n```\n\nBooleanExpression은 null반환시 자동으로 조건절에서 제거된다.\n다만 모든 조건이 null일 경우 전체 결과가 그냥 반환되버리니 주의해야 한다.\n\n나는 검색조건을 클래스로 만들어서 @ModelAttribute로 컨트롤러에서 받고 이를 사용했다. \n\n![Pasted image 20240310220132](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/6c6f31c6-a0fe-4de3-9645-7f3a8e9287d6)\n생성된 쿼리에서 내가 원하던 조건이 where절에 생성된 쿼리를 확인할 수 있었다. \n\n### 그런데 Select가 마구잡이로 나간다? \n\n쿼리를 다시 보는 중 모든 할일목록을 조회하는 기능에서 select쿼리가 잔뜩 나가는 모습을 확인되었다.\n\n![Pasted image 20240310144552](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/934aab65-eefb-415a-a317-2c87f4d34c1c)\n(엄청난 스크롤의 쿼리 중 일부)\n\n## N+1문제\n\nJPA에서 N+1문제란 조회한 엔티티와 연관된 엔티티를 가져올때마다 N(조회한 엔티티수)만큼 추가적인 쿼리가 발생하는 상황을 말한다. \n\n나의 경우에 Todo는 User, Comment와 연관관계를 맺고 있기 때문에 todo를 조회할때 조회한 개수만큼 3배의 쿼리가 발생한 것이다.\n\n### 즉시로딩과 지연로딩\n\nJPA에서는 연관관계를 설정할 때, 즉시로딩(EAGER loading)과 지연로딩(LAZY loading) 두 가지 방식을 선택할 수 있다.\n\n1. 즉시로딩 (EAGER loading): 연관된 엔티티를 즉시 조회하는 방식으로, 연관된 엔티티를 함께 가져와서 메모리에 로드합니다. 즉시로딩은 `@ManyToOne` 또는 `@OneToOne` 관계에서 기본 설정이다.\n\n2. 지연로딩 (LAZY loading): 연관된 엔티티를 실제로 사용할 때 조회하는 방식으로, 연관된 엔티티를 사용할 때에는 해당 엔티티에 대한 쿼리가 실행되어 로드된다. `@OneToMany` 또는 `@ManyToMany` 관계에서 기본 설정이다.\n\n대부분의 경우 성능을 위해서 글로벌 페치전략을 지연로딩으로 설정한다. (즉시 로딩은 필요없는 연관관계의 엔티티도 즉시 조회하므로)\n\n### 프록시\n\n지연로딩으로 객체 그래프를 탐색할때 JPA는 실제객체가 사용될때(ex: getName()) 실제 엔티티를 조회하는데 그 전에는 프록시 객체를 제공한다.\n\n```java\nTodo todo = em.find(Todo.class, id);\ntodo.getComments()// 프록시 객체, sql은 todo 관련 한번만 나간다\n```\n\n이때\n```java\nfor (Comment comment: comments) {\n\t//객체 사용 로직.. sql 다량 발생\n}\n```\n와 같이 컬렉션을 초기화하면 그 수만큼 연관관계가 맺어진 엔티티를 조회하는 쿼리가 나가게 된다.\n\n나는 조회한 할일목록을 스트림으로 dto로 반환하고 있었다.\n```java\n//service method\n@Transactional(readOnly = true)  \n@Override  \npublic List<TodoResponseDto> getAllTodos() {  \n    return todoRepository.findAllByOrderByCreatedAtDesc().stream()  \n        .map(TodoResponseDto::new)  \n        .toList();  \n}\n//dto\npublic TodoResponseDto(Todo todo) {  \n    this.id = todo.getId();  \n    this.title = todo.getTitle();  \n    this.content = todo.getContent();  \n    this.author = todo.getUser().getName();//초기화\n    this.status = todo.getTodoStatus();  \n    this.createdAt = todo.getCreatedAt();  \n  \n    for (Comment comment : todo.getComments()) {  \n        comments.add(new CommentResponseDto(comment.getContent()));//초기화\n    }  \n}\n```\n\b즉 지연로딩된 연관객체들이 스트림의 반복 + foreach의 반복에서 초기화되면서 엄청난 양의 쿼리가 발생하게 된것이다. \n\n## N+1문제 해결\n\n### fetch join\n\nN+1의 해결책은 여러가지 방법이 있는데 가장 일반적인 방법은 페치조인을 사용하는 방법이다. \n\n페치 조인은 일반조인과 조금 다른데, 일반 조인은 조인된 엔티티들의 필드를 가져오지만 페치 조인은 연관된 엔티티를 함께 조회하여 결과로 가져온다. \n\n페치조인은 쿼리를 실행하는 시점에서 연관된 엔티티들을 함께 조회하기 때문에 한번의 쿼리로 모든 데이터를 가져올 수 있다. 이에 반해 일반 조인은 연관된 엔티티들을 조회하기 위해 추가적인 쿼리가 발생할 수 있다. \n\n```java\npublic List<Todo> findAllByOrderByCreatedAtDescc() {  \n    System.out.println(\"called\");  \n    return queryFactory  \n        .select(todo)\n        .distinct()  \n        .from(todo)  \n        .join(todo.user, user).fetchJoin()  \n        .leftJoin(todo.comments, comment).fetchJoin()  \n        .orderBy(todo.createdAt.desc())  \n        .fetch();  \n}  \n  \npublic List<Todo> searchByFilter(TodoSearchFilter todoSearchFilter) {  \n    return queryFactory  \n        .select(todo)  \n        .distinct()\n        .from(todo)  \n        .join(todo.user, user).fetchJoin()  \n        .leftJoin(todo.comments, comment).fetchJoin()  \n        .where(  \n            eqAuthor(todoSearchFilter.getUserId()),  \n            eqTitle(todoSearchFilter.getTitle()),  \n            eqStatus(todoSearchFilter.getTodoStatus())  \n        )  \n        .fetch();  \n}\n```\nuser는 필수 포함이기 때문에 inner join(join시 생략 가능)을 사용했고 comments는 없을 수도 있는 필드이므로 left join을 사용했다. (없을 수도 있는 필드에 inner join을 해버리면 todo 자체가 포함되지 않기 때문에 무결성이 깨진다)\n\n(실행 결과)\n![Pasted image 20240310164055](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/c20bbb82-3291-4704-bd49-1ba2ddfce63d)\n\n### EntityGraph사용\n\n@EntityGraph도 마찬가지로 EntityGraph 상에 있는 Entity들의 연관관계 속에서 필요한 엔티티와 컬렉션을 함께 조회하려고 할때 사용한다. \n어노테이션으로 fetch join을 사용할 수 있게 해준다고 생각하면 쉽다.\n\n```java\npublic interface TodoRepository extends JpaRepository<Todo, Long> {  \n  \n    @EntityGraph(attributePaths = {\"comments\", \"user\"}, type = EntityGraphType.LOAD)  \n    List<Todo> findAllByOrderByCreatedAtDesc();  \n}\n```\n\n`attributePaths`에 같이 조회할 연관엔티티를 작성하면 된다.\n\n`type`은 `EntityGraphType.LOAD`, `EntityGraphType.FETCH` 2가지가 있다\n\n- `LOAD` \nattributePaths에 정의한 엔티티들은 EAGER, 나머지는 글로벌 패치 전략에 따라 패치한다\n일단 attributePaths 는 EAGER, 나머지는 매핑 설정 따라서\n\n- `FETCH` \nattributePaths에 정의한 엔티티들은 EAGER, 나머지는 LAZY로 패치한다 \n\n(실행결과)\n![Pasted image 20240310200026](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/7b5e472b-196a-4011-b888-40d2c78ac513)\n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n참고\n\n자바 ORM 표준 JPA 프로그래밍 - 김영한 저\n"},{"excerpt":"완성된 코드는 링크에서 볼 수 있습니다. (글의 코드와 조금 다른 면이 있을 수 있습니다) @SpringBootTest와 @Transactional @Transactional을 어노테이션 자체에서 포함하고 있는 @DataJpaTest와 달리 @SpringBootTest는 @Transactional을 가지고 있지 않다. 따라서 트랜잭션-롤백 환경을 @Spr…","fields":{"slug":"/sprintboottest_isolation/"},"frontmatter":{"date":"March 04, 2024","title":"@SpringBootTest와 테스트격리","tags":["spring","testing","testisolation"]},"rawMarkdownBody":"\n완성된 코드는 [링크](https://github.com/jinkshower/Todo-management)에서 볼 수 있습니다. (글의 코드와 조금 다른 면이 있을 수 있습니다)\n\n## @SpringBootTest와 @Transactional\n\n@Transactional을 어노테이션 자체에서 포함하고 있는 @DataJpaTest와 달리 @SpringBootTest는 @Transactional을 가지고 있지 않다.\n\n따라서 트랜잭션-롤백 환경을 @SprintBootTest에서 만들기 위해서는 \n```java\n@SpringBootTest  \n@Transactional  \n@Rollback  \npublic class ControllerTest {   \n}\n```\n위 코드처럼 test 클래스에 @Transactional과 @Rollback을 명시해줘야 한다\n\n## RandomPort를 사용할때의 @SpringBootTest\n\n하지만 RestAssured와 같은 프레임워크를 사용하는 인수테스트에서는 어노테이션으로 Port를 지정하게 되는데 이 때 HTTP 클라이언트와 서버는 각각 다른 스레드에서 실행된다.\n\n즉 @Transactional로 트랜잭션 설정을 해도 다른 스레드에서 커밋을 해버리기 때문에 테스트 격리가 되지 않는다. \n\n![Pasted image 20240304114747](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/b3265dbe-be08-44fb-892a-555a71f56e72)\n\n각 테스트는 따로 실행했을 때 제대로 통과하는 테스트지만 같이 실행되었을 때 줄줄이 실패하는 모습을 볼 수 있다.\n\n## 해결책 1. @DirtiesContext\n\n가장 간단하게 테스트를 격리할 수 있는 방법이 있다.\n\n`@DirtiesContext(classMode = ClassMode.BEFORE_EACH_TEST_METHOD)`\n를 테스트 클래스 어노테이션으로 선언하여  각 테스트 메서드가 실행될 때마다 컨텍스트를 새로 로드함을 명시한다.\n\n즉, 테스트 메서드마다 다른 컨텍스트를 사용하기 때문에 트랜잭션 외부에서 데이터를 커밋하여 변경하는 테스트에 영향받지 않고 각 메서드마다 db의 테이블을 새롭게 만들 수 있다.\n\n![Pasted image 20240304115424](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/a32fed4a-4ded-4428-a449-cc9a7e093af3)\n\n적용 이후 모든 테스트가 통과함을 확인할 수 있다.\n\n### @DirtiesContext의 문제점\n이렇게 편한 어노테이션이지만 치명적인 단점이 있으니 바로 속도가 많이 느리다는 것이다. \n\n원래 @SpringBootTest는 테스트 클래스에서 컨텍스트를 한 번 로드하고 이미 컨텍스트가 있다면 캐싱해서 사용하기 때문에 매번 컨텍스트를 다시 로드하는 @DirtiesContext는 필연적으로 속도가 느릴 수 밖에 없다.\n\n## 해결책2. 매 테스트마다 테이블을 Truncate하기\n\n위의 문제를 좀더 빠르게 해결할 수 없는 지 방법을 계속 찾아봤다.\n\n일단 가장 먼저 @AfterEach에 테스트에서 사용하는 repository의 deleteAll() 메서드를 호출하는 방식을 적용해봤다. \n\n하지만 deleteAll()은 특정 엔티티에 대한 레코드를 삭제하므로 연관관계가 맺어져 있는 엔티티에 대한 삭제가 제대로 이루어지지 않았다.\n\n따라서 테이블 데이터를 모두 삭제하는 truncate를 사용하게 되었다.\n\n### h2의 truncate\n\n테스트 db로 h2를 사용하고 있기 때문에 truncate 쿼리를 사용하기 위해서는 테이블의 제약조건을 무효화하고 실행해야 한다\n\n따라서 \n```java\nSET REFERENTIAL_INTEGRITY FALSE\nTRUNCATE ..\nSET REFERENTIAL_INTEGRITY TRUE\n```\n로 truncate 이후 다시 제약조건을 걸어주는 쿼리를 작성해야 했다. \n\n```java\n@AfterEach  \nvoid clear() {  \n    jdbcTemplate.execute(\"SET REFERENTIAL_INTEGRITY FALSE\");  \n    jdbcTemplate.execute(\"TRUNCATE TABLE users\");  \n    jdbcTemplate.execute(\"TRUNCATE TABLE todos\");  \n    jdbcTemplate.execute(\"SET REFERENTIAL_INTEGRITY TRUE\");  \n}\n```\n테스트 클래스 내에서 jdbcTemplate를 사용해 쿼리를 작성해서 실행 시켜 줬다.\n\n![Pasted image 20240304122429](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/8d152e82-5e91-4699-afa0-7a5bade9685a)\n\n@DirtiesContext사용 보다 총 테스트 시간이 절반으로 줄어든 모습\n\n### 아쉬운 점\n\n이렇게 빠른 테스트 속도와 격리를 얻어냈지만 아쉬움이 남았다.\n매번 @AfterEach에 위의 코드를 작성해야 한다는 점, 테이블이 늘어나면 그만큼 쿼리문도 늘어나는 단점을 여전히 가지고 있다.\n\n## 관심사 분리\n\n관심사를 분리하여 @AfterEach에 있는 데이터베이스 초기화 코드를 다른 클래스로 분리하면 매번 테이블을 확인할 필요도 없고, 다른 테스트 클래스에도 재사용할 수 있지 않을까?\n\n테스트마다 생성된 테이블의 이름을 얻어내 각각을 truncate쿼리를 자동으로 만든다면 가능할 것 같다.\n\n### 테이블 이름 얻기\n\n대부분의 관계형 데이터베이스는 생성된 테이블들의 메타정보를 가지고 있는 정보성 테이블이 존재한다. \n\nh2는 information_schema라는 테이블에 메타정보를 저장하는데, 해당 테이블에 있는 모든 메타정보는 [공식문서](https://www.h2database.com/html/systemtables.html)를 참고하면 될 것같다.\n\n내가 관심있는건 테이블이름들이니\n```java\nSELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'PUBLIC'\n```\n라는 쿼리를 작성하면 될 것같다. \n\n### truncate 쿼리 작성\n\n이렇게 얻은 테이블 이름을 자료구조에 저장하고 각 테이블이름마다 반복하여\n```java\nTRUNCATE TABLE \" + tableName + \" RESTART IDENTITY\n```\n를 실행하면 직접 일일히 입력해주지 않아도 쿼리문이 실행될 것이다.\n\n작성한 코드\n```java\npublic class TableCleaner {  \n  \n    private List<String> tableNames;  \n    private JdbcTemplate jdbcTemplate;  \n  \n    @Autowired  \n    public TableCleaner(DataSource dataSource) {  \n        this.jdbcTemplate = new JdbcTemplate(dataSource);  \n    }  \n  \n    @Transactional  \n    public void tableClear() {  \n        String query = \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'PUBLIC'\";  \n        tableNames = jdbcTemplate.queryForList(query, String.class);  \n  \n        jdbcTemplate.execute(\"SET REFERENTIAL_INTEGRITY FALSE\");  \n        for (String tableName : tableNames) {  \n            jdbcTemplate.execute(\"TRUNCATE TABLE \" + tableName + \" RESTART IDENTITY \");  \n        }  \n        jdbcTemplate.execute(\"SET REFERENTIAL_INTEGRITY TRUE\");  \n    }  \n}\n```\n\n이렇게 작성한 클래스를 컴포넌트로 등록해줘서 테스트 실행시 컴포넌트 스캔에 잡히게 해주면 해당 클래스를 빈으로 사용할 수 있고, \n\n```java\n@Component  \npublic abstract class TestSupporter {  \n  \n    @Autowired  \n    private TableCleaner tableCleaner;  \n  \n    @AfterEach  \n    void clear() {  \n        tableCleaner.tableClear();  \n    }  \n}\n```\n해당 클래스로 주입받은 뒤 데이터베이스 초기화가 필요한 테스트에 상속받아 사용하면 테스트 생명주기에 맞춰 @AfterEach가 실행되며 자동으로 데이터베이스가 초기화되어, 테스트 격리가 될 것이다.\n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n참고\n\nhttps://tecoble.techcourse.co.kr/post/2020-09-15-test-isolation/\n\n\n\n\n\n\n\n"},{"excerpt":"팀 프로젝트를 진행하면서 Github Actions를 통해 테스트 자동화 환경을 구축한 기록 코드 보러가기 학습 계기 팀프로젝트를 진행하면서 팀원들끼리 테스트 코드를 작성했지만, 이를 PR에서 확인할 수 있는 방법이 없었다.  물론 팀원들을 믿고 있지만,  나조차도 급할때는 테스트를 돌리는 걸 까먹고 push를 한 기억도 있기 때문이다..  Github …","fields":{"slug":"/githubaction_automated_test/"},"frontmatter":{"date":"March 01, 2024","title":"Github Actions 를 통한 테스트 자동화 구축","tags":["ci/cd","githubactions"]},"rawMarkdownBody":"\n\n팀 프로젝트를 진행하면서 Github Actions를 통해 테스트 자동화 환경을 구축한 기록\n\n[코드](https://github.com/jinkshower/please-praise) 보러가기\n\n## 학습 계기\n\n팀프로젝트를 진행하면서 팀원들끼리 테스트 코드를 작성했지만, 이를 PR에서 확인할 수 있는 방법이 없었다. \n\n물론 팀원들을 믿고 있지만,  나조차도 급할때는 테스트를 돌리는 걸 까먹고 push를 한 기억도 있기 때문이다.. \n\nGithub Actions를 통하면 PR마다 혹은 push마다 테스트를 자동화할 수 있는 workflow를 설정할 수 있다.\n\n## 테스트 자동화의 필요성\n\n`제 로컬에서는 잘 돌아가는데요..?`\n\n1. 환경 일관성 보장\n\n각자의 로컬이 아닌 독립된 (Github Actions의 경우 Runner 서버) 환경에서 테스트가 실행되기 때문에 환경 관련 문제를 사전에 감지 할 수 있다.\n즉, 배포 환경에 맞는 빌드/테스트 환경을 구축하는데 큰 도움이 된다\n\n2. 커밋 이후 안전성 확인 가능\n\n위에서도 말했지만 PR에 `테스트 다 통과해요~` 라는 말로는 PR에 올라온 코드는 믿을 수 없다. 독립된 환경에서 모든 테스트가 통과하는 코드라면, 안심하고 Approve를 누를 수 있게 도와준다. \n\n3. 테스트 실패를 팀 모두가 알 수 있음\n\n테스트 자동화 환경이 구축된 협업에서는 참여하는 모두가 테스트 실패시 에러로그를 공유할 수 있기 때문에 디버깅을 모두가 빠르게 할 수 있고, 공통적인 문제점을 캐치하기 쉬워진다. \n\n## Github Workflow 파일 작성\n\n.github/workflows/{name}.yml로 workflow 설정 파일을 추가해줬다.\n\n```yml\nname: PR Test  \n  \non:  \n  pull_request:  \n    branches: [ dev ]  \n  \njobs:  \n  test:  \n    runs-on: ubuntu-latest  \n    steps:  \n      - name: Set up JDK 17  \n        uses: actions/setup-java@v1  \n        with:  \n          java-version: 17    \n  \n      - name: Grant execute permission for gradlew  \n        run: chmod +x gradlew  \n  \n      - name: Test with Gradle  \n        run: ./gradlew --info test  \n  \n      - name: Publish Unit Test Results  \n        uses: EnricoMi/publish-unit-test-result-action@v1  \n        if: ${{ always() }}  \n        with:  \n          files: build/test-results/**/*.xml\n```\n\n한 부분씩 살펴보도록 하자.\n\n`Set up JDK 17`\n\nRunner 서버에 JDK를 설치한다. \n\n`Grant execute permission for gradlew`\n\n명령어를 통해 gradlew 스크립트에 대한 실행권한을 부여한다.\n\n`Test with Gradle`\n\n실행 권한을 받아 테스트를 실행한다\n\n`Publish Unit Test Results`\n\n다른 사람이 만든 workflow설정도 쓸 수 있다. [링크](https://github.com/EnricoMi/publish-unit-test-result-action)\n테스트 결과를 읽기 쉬운 로그로 게시해준다.\n\n하지만... \n![Pasted image 20240219161225](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/1f76a31c-2e77-4ce2-bf41-818fdf1fda07)\n\n## Fail to load ApplicationContext\n\n앞에서도 말했듯이 Github Actions는 별개의 Runner서버에서 실행되기 때문에 로컬에서만 적용되는 환경설정은 오류가 발생한다\n\n나의 경우 설정 파일에 JWT 시크릿 키나 관리자 토큰 값이 있었기 때문에 기본적으로 .gitignore에 설정파일을 추가하고 팀원들도 마찬가지로 각자 로컬환경에서 설정파일을 작성했기 때문에 해당 값을 가져다 쓰는 빈을 생성하는데 문제가 발생했다.\n\n가장 빠른 해결 방법은 물론 설정파일을 깃에 업로드하는 거지만, AWS 키와 같은 절대 공개되어서는 안\n되는 정보는 깃에 올릴 수 없다.\n\n이에 대한 해결방법으로 \nGithub Secrets에 모든 설정을 등록하고 workflow에서 직접 설정파일을 작성하는 job을 추가하는 방법을 찾아봤지만 value를 암호화해야하고, 수정할때마다 secret을 삭제, 추가하는 일이 너무 번거로워 보였다.\n\n## Git Submodule로 설정파일 관리하기\n\nGit Submodule를 이용하면 리포지토리 안에 다른 리포지토리를 분리해서 넣어서 사용할 수 있다고 한다.\n\n별도의 private repository에 설정에 필요한 파일을 두고 메인 리포지토리에서 이를 연동해 사용한다면 팀원들과 설정을 공유하기도 쉽고 이를 workflow에서 사용할 수도 있다.\n\nprivate repository에 설정파일을 업로드 한 후 메인 리포지토리의 깃에서 \n`git submodule add {private-repository-url} config`\n를 입력해 서브모듈 리포지토리를 등록해준다.\n\n>[config]\n>\n>스프링은 설정파일을 가장 먼저 `src/main/resources`에서 찾지만 해당 디렉토리에 설정파일이 없을 경우 `config`디렉토리를 찾아서 적용하기 때문이다. \n>\n>나는 설정파일은 깃에 업로드하고 있지 않기 때문에 서브모듈에서 가져올 설정파일을 `config` 경로에 연동했다. \n\n>[서브모듈 최신화]\n>\n>서브 모듈은 커밋 후 푸시를 해도 메인 리포지토리에서 자동으로 추적하여 업데이트 하지 않는다.\n>\n>메인 리포지토리에서 서브모듈의 commit hash값만 참고하는 방식이기 때문에 서브모듈을 업데이트 했다면 `git submodule update --remote`로 최신화 해줘야 한다\n\n.gitmodules 파일에 서브모듈 경로가 잘 적용된 것을 확인했다면 workflow에 해당 서브모듈을 연동함을 명시해 줘야 한다. \nsecrets에 다른 리포지토리 접근을 허용하는 access token을 추가한 후\n\nworkflow 파일에 해당 작업을 추가해줬다.\n```yml\n- name: Checkout  \n  uses: actions/checkout@v2  \n  with:  \n    token: ${{ secrets.ACTION_TOKEN }}  \n    submodules: true\n```\n\n## 마치며\n\n![Pasted image 20240301233610](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/701cd629-2d9a-4185-8157-803e3696b32c)\n\n이렇게 여러 시간의 시도를 통해 PR마다 테스트를 자동화해주었다.\n\n테스트 실패시 PR에 실패한 라인에 코멘트를 달아주는 workflow, 빌드/테스트 실패시 slack에 알림을 보내는 workflow도 있으니 각자의 협업 방식에 맞는 workflow를 추가해주면 된다! \n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n"},{"excerpt":"인수테스트 에서 이어지는 글입니다. Slice Test Slice Test는 레이어별로 잘라서 레이어를 하나의 단위로 보는 테스트이다. 왜 Slice Test를 해야 하는가? 개별 레이어의 검증\nSlice Test를 통해 각 레이어를 독립적으로 테스트할 수 있다. \n즉, 테스트가 깨진다면 어디를 고쳐야할 지에 대해 빠른 피드백을 받을 수 있다. 레이어간 …","fields":{"slug":"/sprint_test_strategy/"},"frontmatter":{"date":"February 25, 2024","title":"Slice Test","tags":["spring","slicetest","testing"]},"rawMarkdownBody":"\n\n[인수테스트](https://jinkshower.github.io/acceptance_test/) 에서 이어지는 글입니다.\n\n## Slice Test\n\nSlice Test는 레이어별로 잘라서 레이어를 하나의 단위로 보는 테스트이다.\n\n### 왜 Slice Test를 해야 하는가? \n\n1. 개별 레이어의 검증\nSlice Test를 통해 각 레이어를 독립적으로 테스트할 수 있다. \n즉, 테스트가 깨진다면 어디를 고쳐야할 지에 대해 빠른 피드백을 받을 수 있다.\n\n2. 레이어간 의존성을 낮추는 리팩토링을 유도한다\n`단위 테스트`를 하다 보면 테스트하기 어려운 메인코드들이 존재한다. \n나의 경우는 다른 객체에 과도하게 의존하고 있는 메인코드들에서 테스트를 하기 어렵다는 느낌을 받은 경험이 많은데, 이를 spring의 레이어들에도 적용할 수 있다.\n\n3. @SpringBootTest는 무겁다\n인수테스트 글에서도 언급했지만 @SpringBootTest는 모든 Bean을 로드하기 때문에 속도가 느리다. \n\n이러한 이유에서 Slice Test를 개인 과제에서 적용한 기록을 적어보고자 한다.\n\n## @WebMvcTest\n\n@WebMvcTest는 웹 레이어 테스트를 하는데 필요한 빈들만 로드한다.\n즉, @Service @Repository @Component를 스캔하지 않기 때문에 수동으로 등록해주거나 Mock객체를 만들어서 주입시켜줘야 한다.\n\n작성한 코드 \n```java\n@WebMvcTest(TodoController.class)\n@ActiveProfiles(\"test\")  \n@MockBean(JpaMetamodelMappingContext.class)  \n@Import(ExternalConfig.class)  \npublic class ControllerTest {  \n  \n    @Autowired  \n    protected MockMvc mockMvc;  \n  \n    @Autowired  \n    protected ObjectMapper objectMapper;  \n\n}\n```\n\n`@WebMvcTest(TodoController.class)` \n해당 컨트롤러에 관련된 빈만 로드하게 설정해줬다.\n\n`@MockBean(JpaMetamodelMappingContext.class)`\nTodo 엔티티가 JpaAuditing을 사용하고 있기 때문에  충돌을 방지하기 위하여 로 Mock으로 대체해주었다.\n\n`@Import(ExternalConfig.class)` \n수동으로 등록한 @Component는 앞서 말했듯이 @WebMvcTest에서 컴포넌트 스캔을 하지 않기 때문에  테스트용 클래스에 빈 정보를 등록하고 해당 테스트에서 사용하게 설정해줬다.\n\n해당 클래스를 상속받아 작성한 테스트 중 일부\n```java\n@DisplayName(\"할일 생성 요청\")  \n@Test  \nvoid test1() throws Exception {  \n    //given  \n    given(userRepository.findById(eq(TEST_USER_ID))).willReturn(Optional.of(TEST_USER));  \n  \n    //when  \n    ResultActions action = mockMvc.perform(post(\"/api/todos\")  \n        .contentType(MediaType.APPLICATION_JSON)  \n        .accept(MediaType.APPLICATION_JSON)  \n        .header(JwtUtil.AUTHORIZATION_HEADER, token())  \n        .content(objectMapper.writeValueAsString(TEST_TODO_REQUEST_DTO)));  \n  \n    //then  \n    action.andExpect(status().isCreated());  \n    verify(todoService, times(1))  \n        .saveTodo(any(UserDto.class), any(TodoRequestDto.class));  \n}\n```\n\nBDD mockito를 사용하여 좀 더 가독성을 높이려고 했고, 컨트롤러 레이어만 테스트하기 때문에 service나 repository는 @MockBean으로 선언하여 사용하였다. \n\n## Service Test\n\n작성한 코드\n```java\n@ExtendWith(MockitoExtension.class)  \npublic class TodoServiceTest implements TodoFixture {  \n  \n    @InjectMocks  \n    TodoServiceImpl todoService;  \n  \n    @Mock  \n    TodoRepository todoRepository;  \n  \n    @DisplayName(\"할일 생성\")  \n    @Test  \n    void test1() {  \n        //given  \n        Todo testTodo = TEST_TODO;  \n        given(todoRepository.save(any(Todo.class))).willReturn(testTodo);  \n  \n        //when  \n        TodoResponseDto actual =  \n            todoService.saveTodo(TEST_USER_DTO, TEST_TODO_REQUEST_DTO);  \n  \n        //then  \n        TodoResponseDto expected = new TodoResponseDto(testTodo);  \n        assertThat(actual).isEqualTo(expected);  \n    }\n}\n```\n\n서비스레이어의 비즈니스 로직이 잘 작동하는지가 관건이므로 나머지는 Mock으로 처리해줬다.\n\n`@InjectMocks`\nMock객체들을 해당 객체에 주입하도록 설정해준다.\n\n`@Mock`\n데이터베이스에 저장되었는지는 관심사가 아니므로 가짜 객체를 설정해주었다.\n\n## @DataJpaTest\n\nJpa 관련 컴포넌트를 테스트하는 데 사용되는 어노테이션이다. \n전체 ApplicationContext를 로드하지 않고 DataJpaRepository와 관련된 빈들만을 로드한다.\n\n작성한 테스트\n```java\n@DataJpaTest  \n@ActiveProfiles(\"test\") \npublic class TodoRepositoryTest implements TodoFixture {  \n  \n    @Autowired  \n    TodoRepository todoRepository;  \n  \n    @Autowired  \n    UserRepository userRepository;  \n  \n    @BeforeEach  \n    void setUp() {  \n        userRepository.save(TEST_USER);  \n    }  \n  \n    @DisplayName(\"작성일 내림차순 정렬 조회\")  \n    @Test  \n    void test1() {  \n        //given  \n        Todo testTodo1 =  \n            TodoHelper.get(TEST_TODO, 1L, LocalDateTime.now().minusMinutes(2), TEST_USER);  \n        Todo testTodo2 =  \n            TodoHelper.get(TEST_TODO, 2L, LocalDateTime.now().minusMinutes(1), TEST_USER);  \n        Todo testTodo3 =  \n            TodoHelper.get(TEST_TODO, 3L, LocalDateTime.now(), TEST_USER);  \n        todoRepository.save(testTodo1);  \n        todoRepository.save(testTodo2);  \n        todoRepository.save(testTodo3);  \n  \n        //when  \n        List<Todo> actual = todoRepository.findAllByOrderByCreatedAtDesc();  \n  \n        //then  \n        List<LocalDateTime> times = actual.stream()  \n            .map(Timestamped::getCreatedAt)  \n            .toList();  \n        assertThat(times.get(2)).isBefore(times.get(1));  \n        assertThat(times.get(1)).isBefore(times.get(0));  \n    }  \n}\n```\n\nDataJpa의 기본 CRUD 기능은 라이브러리의 기능으로 간주하고 테스트를 작성하지 않았다.\n커스텀하게 작성한 쿼리메서드를 테스트하는 메서드를 작성해보았다.\n\n@DataJpaTest는 기본적으로 h2 데이터베이스를 사용하게 되어있는데, \n실제 데이터베이스를 사용하려면  `@AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE)` 을 추가하면 된다. \n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*"},{"excerpt":"트랜잭션(Transaction) 트랜잭션은 데이터베이스에서 수행되는 작업의 단위를 나타내며, 더 이상 쪼갤 수 없는 쿼리들의 묶음을 말한다. 트랜잭션은 ACID 특징을 따르며, ACID는 원자성(Atomicity), 일관성(Consistency), 격리성(Isolation), 지속성(Durability)을 나타낸다. ACID 특징 1. 원자성(Atomic…","fields":{"slug":"/transaction/"},"frontmatter":{"date":"February 21, 2024","title":"Transaction","tags":["database","transaction"]},"rawMarkdownBody":"\n## 트랜잭션(Transaction)\n\n트랜잭션은 데이터베이스에서 수행되는 작업의 단위를 나타내며, 더 이상 쪼갤 수 없는 쿼리들의 묶음을 말한다. 트랜잭션은 ACID 특징을 따르며, ACID는 원자성(Atomicity), 일관성(Consistency), 격리성(Isolation), 지속성(Durability)을 나타낸다.\n\n## ACID 특징\n\n### 1. 원자성(Atomicity)\n\n- 트랜잭션을 구성하는 작업들은 모두 성공하거나 모두 실패하는 특성을 갖는다.\n- 중간 단계에서 오류가 발생하면 이전 상태로 롤백되어 어떠한 영향도 주지 않아야한다.\n\n### 2. 일관성(Consistency)\n\n- 트랜잭션 실행 전과 실행 후에 데이터베이스는 일관된 상태를 유지해야 한다\n- 트랜잭션은 데이터베이스의 무결성 제약조건을 만족해야 한다\n\n### 3. 격리성(Isolation)\n\n- 동시에 여러 트랜잭션이 실행될 때, 각 트랜잭션은 서로 간섭받지 않고 독립적으로 실행되는 특성\n- 특정 트랜잭션이 다른 트랜잭션의 작업을 볼 수 없다\n\n### 4. 지속성(Durability)\n\n- 트랜잭션이 성공적으로 완료되면 그 결과는 영구적으로 저장되어야 한다\n- 시스템 장애 또는 다시 시작해도 데이터베이스는 변하지 않아야 한다\n\n## Commit과 Rollback 연산\n\n### Commit\n\n- 트랜잭션의 모든 작업이 성공적으로 완료되었고, 결과를 데이터베이스에 반영하는 명령\n- Commit이 수행되면 트랜잭션은 영구적으로 적용된다\n\n### Rollback\n\n- 트랜잭션의 실패나 에러가 발생한 경우, 이전 상태로 되돌리는 명령\n- 데이터베이스에 아무런 영향을 미치지 않은 것처럼 트랜잭션을 취소한다.\n\n---\n참고\n\nhttps://docs.oracle.com/en/database/oracle/oracle-database/19/cncpt/transactions.html"},{"excerpt":"프로젝트 기간(2024.02.07 - 2024.02.15) 완성 레포지토리 해당 프로젝트를 진행하며 느낀 점에 대한 기록  지켜야 할 것은 문서로 남기기 첫 협업 이후 의 중요성을 깨달았고 이번 프로젝트에서는 '이걸 지켜주세요' 라고 말하는게 아니라 지켜야할 것은 문서로 작성하기로 했다.  코드 컨벤션을 IDE에 적용, 설정하기 위한 레퍼런스를 공유했고,…","fields":{"slug":"/second_coop/"},"frontmatter":{"date":"February 15, 2024","title":"두번째 협업 회고","tags":["cooperation","retrospective"]},"rawMarkdownBody":"\n프로젝트 기간(2024.02.07 - 2024.02.15)\n\n[완성 레포지토리](https://github.com/jinkshower/OTT_Suggestion)\n\n해당 프로젝트를 진행하며 느낀 점에 대한 기록 \n\n## 지켜야 할 것은 문서로 남기기\n\n[첫 협업](https://jinkshower.github.io/first_team_assignment/) 이후 `문서화`의 중요성을 깨달았고 이번 프로젝트에서는 '이걸 지켜주세요' 라고 말하는게 아니라 지켜야할 것은 문서로 작성하기로 했다. \n\n코드 컨벤션을 IDE에 적용, 설정하기 위한 레퍼런스를 공유했고, 깃 컨벤션과 브랜치 전략을 프로젝트 초기단계에 모두 같이 모여 정했다. \n\n![Pasted image 20240215205740](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/327a4bad-38ce-4946-b347-ef77829241d1)\n\nJDK 버전,  gitignore, dependency 설정 등 각자의 로컬환경에서 충돌이 일어날 수 있는 부분을 모두 통일했다.\n\n## 설계와 공통코드 \n\n서로 다른 코드 스타일을 가진 개발자들과 같이 '잘' 코드를 작성하려면 공통된 보일러플레이트가 있어야 한다고 생각했다. \n\n공통코드를 같이 프로그래밍하고, 그 것을 지켜야할 `규칙`으로 만든다면 push할 때 조금 덜 두려워질 것이라고 생각했다. \n\nIntelliJ의 CodeWithMe를 사용하여 5명이 동시에 의견을 주고 받으며 프로그래밍을 진행했고, 개인적으로 설계에서 중요하다고 생각하는 패키지구조와 도메인 클래스명들을 몹 프로그래밍으로 구현했다.\n\n`문서화` 와 `공통코드`를 통해 프로젝트 전반적으로 Merge시 충돌이 적었으며 이는 곧 생산성의 증진으로 이어졌다.\n\n## Approve를 누른다는 건\n\n주어진 프로젝트 기간이 길지 않았기 때문에 페어프로그래밍이나 몹프로그래밍으로 구현한 부분은 Approve를 급하게 누르고 진행하려는 경향이 있었다. \n\n같이 작성한 코드니까 문제가 생기지 않겠다고 생각했지만, 프로젝트 중반이 되는 지점에서 파일 개수가 많아지고 새로운 기능을 구현할수록 이전 코드와 일관성이 떨어지거나, 끼워 맞추는 식의 코드가 발생했다.\n\n해당 문제가 발생할때마다 회의를 통해 공통된 코드에 대한 인식을 리마인드하는 시간을 가져서 문제를 해결했지만 같이 프로그래밍을 한 것이라도, 세세한 코드 리뷰나 PR 기록을 남기지 않으면 결국 내가 그 코드를 마주해야 함을 깨달았다. \n\nApprove를 누른다는 건, 이 코드를 내가 작성한 것으로 생각하겠다는 허락이라는 생각이 들었다. \n\n## 협업\n\n이번 프로젝트에서 열정적이고 좋은 팀원들을 만나게 되어 혼자서 했다면 훨씬 오래 걸릴 구현들을 빠르게 진행할 수 있었지만 만약 그렇지 않았다면? 내가 함께 일하게 될 팀원들은 내 이상과 많이 다를 수도 있다. \n\n결국 협업을 통해 더 좋은 결과를 내고 싶다면, 내가 이러한 부분을 문제로 생각한다면, 그 것을 문서로 남길 수 밖에 없다는 것을 다시금 새기게 되었다. \n\n또한 내 자신의 코드스타일에 좀 더 유연함을 가져야 함을 스스로 느끼게 되었다.  내가 개인적으로 맞다고 생각하는 방향은 다른 사람이 보기에 이해되지 않는 낯선 방향일수도 있다. \n\n그리고 이를 내가 설득할 수 없다면, 그건 내가 틀린 것일 수도 있지 않을까? \n\n"},{"excerpt":"HTTP 통신의 특징 쿠키와 세션에 대해 이야기하기 전에 이들이 필요한 배경에 대해 말하려면 Http통신의 특징에 대해 이야기해봐야 한다. \n클라이언트와 서버간 tcp/ip 통신을 계-속 이어나간다면, 그리고 다수의 클라이언트의 통신이 발생하는 상황이라면 서버의 자원은 남아나지 않을 것이다. 이에 따라 응답 이후 바로 접속을 종료하는 HTTP 1.0, 일…","fields":{"slug":"/cookie_session/"},"frontmatter":{"date":"February 10, 2024","title":"쿠키와 세션","tags":["http","cookie","session"]},"rawMarkdownBody":"\n\n## HTTP 통신의 특징\n\n쿠키와 세션에 대해 이야기하기 전에 이들이 필요한 배경에 대해 말하려면 Http통신의 특징에 대해 이야기해봐야 한다.\n\n`비연결성(Connectionless)`\n클라이언트와 서버간 tcp/ip 통신을 계-속 이어나간다면, 그리고 다수의 클라이언트의 통신이 발생하는 상황이라면 서버의 자원은 남아나지 않을 것이다.\n\n이에 따라 응답 이후 바로 접속을 종료하는 HTTP 1.0, 일정시간 접속을 유지해 3 Way Handshake의 오버헤드 비용을 줄인 HTTP 1.1처럼 HTTP는 기본이 연결을 유지하지 않는 모델로 발전해왔다.\n\n`무상태(Stateless)`\n모든 HTTP 요청, 응답은 독립적이고 이전이나 이후의 요청과 무관하다.  서버는 다수의 요청에서 사용자에 대한 어떠한 정보도 저장할 필요가 없다. \n\n무상태인 HTTP의 특성에 따라 서버를 확장하기가 쉽다.(어떠한 서버든지 요청에 알맞게 응답할 수 있으므로)\n\n하지만 이러한 특성에도 불구하고 우리는 웹을 이용할때 독립적인 요청, 응답을 받는 것 같지 않다. \n\n내 장바구니에는 내가 이전에 담은 물품이 들어있고 로그인한 웹페이지는 계속 해서 내 정보를 보여주고 있기 때문이다.\n\n이러한 지속적인 연결 경험을 위한 기술이 쿠키와 세션이다.\n\n## 쿠키와 세션\n\n### 쿠키\n\n쿠키란 클라이언트에 저장될 목적으로 생성한 작은 정보를 담은 파일이다. \n클라이언트는 이 정보를 저장해 놓고 요청시에 같이 전달한다. \n\n예를 들어 로그인시 서버에서 쿠키를 만들어 응답메시지의 헤더에 넣어 보내면 클라이언트는 쿠키 저장소에 이를 저장하고 이후 요청시에 쿠키를 같이 넣어 보내면 로그인한 나의 정보에 맞는 웹페이지를 응답하는 방식이다.\n\n쿠키는 \n\n1. Name 이름 쿠키의 식별자 값\n2. Value 쿠키의 값\n3. Domain 쿠키가 저장된 도메인\n4. Path 쿠키가 사용되는 경로\n5. Expires 만료기한\n\n으로 구성된다. \n\n보통 사용자 로그인 세션관리와 광고 정보 트래킹 등에 사용된다. \n\n하지만 쿠키는 항상 서버에 전송되다 보니 네트워크 트래픽을 추가로 유발할 수 있고, 갈취되기 쉽기 때문에 보안에 민감한 데이터는 쿠키로 다루면 안된다. \n\n### 세션\n\n세션은 클라이언트 측이 아닌 서버에 정보를 일정시간 저장하는 방식이다.\n\n서버에서 클라이언트 별로 식별값인 세션Id를 부여하고 클라이언트에 대한 정보를 서버에 저장한 후, 생성된 세션Id를 쿠키로 만들어(세션 쿠키) 클라이언트에 응답한다.\n\n이 후 클라이언트는 요청에 세션Id를 담아 전송하고 이러한 세션은 클라이언트가 브라우저를 닫거나 클라이언트가 로그아웃을 하거나, 설정한 기간을 넘길 시 유효하다. \n\n세션은 쿠키와 달리 사용자의 민감한 정보를 서버에 저장하기 때문에 보안 면에서 쿠키보다 더 낫다는 장점이 있다.\n\n하지만 서버에 클라이언트 정보를 저장하는 것은 서버 용량을 차지하기도 하고 서버의 무상태성을 지향하기 힘들게 만든다\n\n만약 모든 클라이언트의 정보를 하나의 서버가 가지고 있다면 다른 서버를 이용한 확장이 어려울 것이다. \n\n이를 보완하기 위해 세션 요청을 하나의 서버로 몰아주는 Sticky Session방식이나 세션을 저장소를 따로 생성해 저장하는 방법을 택하곤 한다. \n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n참고\n\nhttps://www.geeksforgeeks.org/difference-between-session-and-cookies/\n\nhttps://developer.mozilla.org/ko/docs/Web/HTTP/Cookies\n\n모든 개발자를 위한 HTTP 웹 기본 지식 - 김영한\n\n\n\n"},{"excerpt":"인수 테스트란 무엇일까? 알아보고 적용해본 인수테스트  학습 계기 개인 과제 중 테스트에 관련해 튜터님께 피드백을 받게 되었다\n\n이와 관련해 고민과 검색을 하다  라는 키워드를 찾아내게 되었다.  인수테스트 인수테스트(Acceptance Test)란 소프트웨어 테스팅 기법 중 하나로 소프트웨어의 수용성을 테스트하는 기법이다. 인수테스트를 통해 비즈니스 요…","fields":{"slug":"/acceptance_test/"},"frontmatter":{"date":"February 04, 2024","title":"인수 테스트(Acceptance Test)","tags":["spring","acceptancetest","testing"]},"rawMarkdownBody":"\n\n인수 테스트란 무엇일까? 알아보고 적용해본 인수테스트 \n\n## 학습 계기\n\n개인 과제 중 테스트에 관련해 튜터님께 피드백을 받게 되었다\n`서비스로서의 기능을 테스트하는 것도 중요하다`\n이와 관련해 고민과 검색을 하다  `인수 테스트`라는 키워드를 찾아내게 되었다. \n\n## 인수테스트\n\n인수테스트(Acceptance Test)란 소프트웨어 테스팅 기법 중 하나로 소프트웨어의 수용성을 테스트하는 기법이다.\n\n인수테스트를 통해 비즈니스 요구사항에 대한 소프트웨어의 적합성을 평가하고 소프트웨어를 회사가  `인수`해도 되는지를 측정한다고 한다.\n\n인수 테스트는 유저가 소프트웨어를 사용하는 시나리오를 적용하여 테스트를 진행하게 된다.\n\n예를 들어 'spring이라는 필터를 클릭하면 spring을 포함하는 목록을 보여준다'가 테스트 이름이 될 수 있겠다.\n\n## 적용하기\n\n자바에서는 `MockMvc`나 `RestAssured`를 이용해서 인수테스트를 진행한다고 한다. \n\n인수테스트는 실제 시나리오를 테스트하기 위함이기 때문에 @SpringBootTest로 웹 환경을 사용하는 `RestAssured`를 공부하기로 했다\n\n[공식문서](https://github.com/rest-assured/rest-assured/wiki/Usage) 와 각종 구글링으로 사용법을 익혔고\nassertJ와 합쳐서 테스트를 작성했다. \n\n```java\n@DisplayName(\"토큰을 가졌지만 할일의 userId와 동일하지 않은 id를 가진 유저는 할 일의 상태를 수정 할 수 없다\")  \n@Test  \nvoid test8() {  \n    //given  \n    postTodo(postRequestDto, validToken1);  \n  \n    //when  \n    ExtractableResponse<Response> response = RestAssured.given().log().all()  \n            .header(\"Authorization\", validToken2)  \n            .when().patch(\"/api/todos/1/status\")  \n            .then().log().all()  \n            .extract();  \n  \n    //then  \n    assertThat(response.statusCode()).isEqualTo(HttpStatus.BAD_REQUEST.value());  \n    assertThat(response.body().asString()).contains(\"작성자가 다릅니다.\");  \n}\n```\n\n그렇게 작성한 테스트 중 하나를 가져와봤다. \n\n`given`\n\nid를 가진 토큰으로 할일이 작성 되었을때\n\n`when`\n\n다른 id를 가진 토큰을 헤더에 포함한 `/api/todos/1/status`라는 http 요청을 보내면\n\n`then`\n\n응답의 상태코드가 400임을 확인하고 에러메시지를 확인한다\n\nwhen 부분의 코드를 자세히 보면 \n```java\n    ExtractableResponse<Response> response = RestAssured.given().log().all()   //요청에 대한 조건을 추가할 수 있다\n            .header(\"Authorization\", validToken2)  \n            .when().patch(\"/api/todos/1/status\")  // http요청을 보낼수있다\n            .then().log().all()  //응답을 모두 기록하여 추출할 수 있다\n            .extract();  \n```\n로 정리할 수 있겠다.\ngiven()과 then()에 log().all()을 추가해서 응답과 요청 내용을 콘솔에 찍어서 디버그에 유용하게 쓸 수도 있다. \n\n## 느낀점\n\n인수테스트를 작성하다보니 자연스레 api명세서 자체를 테스트하고 있음을 깨닫게 되었다.\n\n또한 하나의 기능을 테스트하기 위해서는 이전 기능이 모두 작동해야하기 때문에 넓은 테스트 커버리지를 달성할 수 있었다.\n\n무엇보다 요청과 응답을 모두 콘솔에 찍을 수 있기 때문에 postman으로 직접 실행하고 디버깅하는 것보다 편한 부분이 있었다.\n\n## 하지만\n\n예시코드에서 `postTodo()`를 다시 보면 한줄이라 간단해 보이지만\n`회원가입` -`로그인` 의 과정을 모두 거치고서야 할일을 등록할 수 있기 때문에 해당 클래스의 코드가 많아지는 것은 물론이고 테스트를 돌리는데 시간도 많이 걸린다.\n\n또한 random_port를 사용하는 테스트는 @Transactional이 적용되지 않기 때문에 테스트의 격리가 어려워 진다.\n\n아직 나만의 방법을 찾지 못해서 임시방편으로 @AfterEach로 데이터베이스를 초기화하고 있지만 좋은 방법은 아닌 것같다. \n\n당장 찾아본 방법은 \n\n1.  MockMvc를 사용한다\n2.  의도적으로 다른 데이터를 사용한다\n3.  @DirtiesContext를 사용한다\n\n정도 인데 테스트 격리에 대해 조금 더 생각이 정리되면 코드로 적어보고 글로 작성해보려고 한다.\n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n참고\n\nhttps://www.geeksforgeeks.org/acceptance-testing-software-testing/\n\nhttps://tecoble.techcourse.co.kr/post/2021-05-25-unit-test-vs-integration-test-vs-acceptance-test/\n\n\n\n\n"},{"excerpt":"개인과제를 진행하며 ArgumentResolver를 추가하여 코드를 개선한 기록 과제에서 만난 문제 개인 과제에서 밑과 같은 코드를 작성하게 되었다. Jwt토큰을 헤더에 넣는 방식으로 로그인 인증처리를 하고 있는데, 할일을 등록하기 위해서는 토큰 인증이 필요하고 request에서 헤더를 뽑아내서 토큰을 인증하고 인증이 통과하면 로직을 실행해야 하는 메서드…","fields":{"slug":"/argumentresolver/"},"frontmatter":{"date":"February 02, 2024","title":"ArgumentResolver추가로 코드 개선하기","tags":["spring","argumentresolver"]},"rawMarkdownBody":"\n\n개인과제를 진행하며 ArgumentResolver를 추가하여 코드를 개선한 기록\n\n## 과제에서 만난 문제\n\n개인 과제에서 밑과 같은 코드를 작성하게 되었다.\n\n```java\n@PostMapping  \npublic String postTodo(HttpServletRequest request) {  \n    String token = jwtUtil.getJwtFromHeader(request);  //헤더에서 토큰 뽑기\n    jwtUtil.validateToken(token);  //검증\n     \n    //\n    //  \n```\n\nJwt토큰을 헤더에 넣는 방식으로 로그인 인증처리를 하고 있는데, 할일을 등록하기 위해서는 토큰 인증이 필요하고 request에서 헤더를 뽑아내서 토큰을 인증하고 인증이 통과하면 로직을 실행해야 하는 메서드를 작성하게 되었다.\n\nTodo를 등록하는 것 이외에도 조회, 삭제 ,수정 모두에 토큰 검증이 필요하기 때문에 해당 코드의 중복을 막을 수 있는 방법을 찾게 되었다.\n\n## ArgumentResolver 간단하게 알아보기\n \n스프링을 쓰다 보면 다양한 파라미터를 어노테이션만 붙이고 사용한 경험이 있을 것이다.\n\n```java\npublic String hello(@PathVariable Long id,\n\t\t\t\t\t@RequestParam String username,\n\t\t\t\t\t   @RequestBody RequestDto requestDto...) {  \n    // 이들은 ?어디서? 오는 거지 ?\n}\n```\n\n스프링은 위처럼 다양한 메서드 파라미터들을 스프링 내부의 `RequestMappingHandlerAdapter` 에서 어노테이션 기반으로 처리해서 우리가 쓰는 @Controller에 보내준다.\n\n잠깐 해당 클래스의 내부를 살펴볼까? \n\n![Pasted image 20240204191203](https://github.com/jinkshower/jinkshower.github.io/assets/135244018/dd88399e-9392-421f-b877-4387b7988ee5)\n(클래스 내부의 일부다. 궁금하면 cmd+O로 검색 후 들어가보자!)\n\n해당 클래스 안에 보이는 `ArgumentResolver`들이  `DispatcherServlet`이 보내주는 http요청을 (밑에 살짝 보이는) `Converter`들을 이용해 우리가 필요로 하는 형태 변환해서 보내주고 있다.\n\n어떤 종류의 메서드 파라미터들을 할 수 있는지는 [공식문서](https://docs.spring.io/spring-framework/reference/web/webmvc/mvc-controller/ann-methods/arguments.html)를 참고하도록 하자.\n\n그럼 이 Jwt토큰 검증을 하는 ArgumentResolver를 추가하면 우리는 토큰 검증을 통과한 request만 받을 수 있지 않을까?\n\n## HandlerMethodArgumentResolver 구현하기\n\n내부에서 본 코드에서 `List<HandlerMethodArgumentResolver>`가 기억날지도 모르겠다.\n스프링이 제공하는 `ArgumentResolver`들은 `HandlerMethodArgumentResolver` interface를 구현하고 있다.\n\n```java\npublic interface HandlerMethodArgumentResolver {  \n    boolean supportsParameter(MethodParameter parameter);  \n  \n    @Nullable  \n    Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception;  \n}\n```\n\n즉, 우리는 이 인터페이스를 구현하고 아까 본 List에 넣어주기만 하면 된다. \n\n`supportsParameter()`는 해당 파라미터를 이 Resolver가 처리할 수 있는 지를 판단하고\n`resolveArgument()`를 호출해서 실제 객체를 생성하고 컨트롤러 호출시 넘겨준다\n\n그럼 구현해볼까? \n\n```java\n@Slf4j  \n@Component  \n@RequiredArgsConstructor  \npublic class AuthArgumentResolver implements HandlerMethodArgumentResolver {  \n  \n    private final JwtUtil jwtUtil;  \n    private final UserRepository userRepository;  \n  \n    @Override  \n    public boolean supportsParameter(MethodParameter parameter) {  \n        return parameter.hasParameterAnnotation(Login.class);//어노테이션\n    }  \n  \n    @Override  \n    public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer,  \n                                  NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception {  \n        HttpServletRequest request = (HttpServletRequest) webRequest.getNativeRequest();  \n  \n        String token = jwtUtil.getJwtFromHeader(request);  \n        if (!jwtUtil.validateToken(token)) {  \n            String errorMessage = \"토큰 검증에 실패했습니다.\";  \n            log.error(errorMessage);  \n            throw new InvalidTokenException(errorMessage);  \n        }  \n        Long userId = jwtUtil.getUserIdFromToken(token);  \n        User found = userRepository.findById(userId).orElseThrow(  \n                () -> {  \n                    String errorMessage = \"ID로 유저를 찾을 수 없습니다. 요청 ID: \" + userId;  \n                    log.error(errorMessage);  \n                    return new AuthenticationException(errorMessage);  \n                }  \n        );  \n        log.debug(\"검증 통과!\");  \n  \n        return new UserDto(found);  \n    }  \n}\n```\n이렇게 `resolveArgument()`에서 토큰 검증을 하고 토큰에서 id를 가지고 user를 만들어 UserDto로 보내주게 되었다.\n\n`supportsParameter()`에 있는 `Login.class`는 어노테이션 메소드 인자로 UserDto를 바로 받기 위해서 만들었다\n\n```java\n@Target(ElementType.PARAMETER) //파라미터로 이 어노테이션을 생성할 수 있다\n@Retention(RetentionPolicy.RUNTIME) \npublic @interface Login{ //어노테이션 클래스로 지정\n}\n```\n\n즉 `supportsParameter()`는 해당 파라미터에 @Login이 붙어있는지 보고,  `resolveArgument`는 해당 파라미터에 들어가는 요청에서 토큰을 검증하고 검증이 성공하면 UserDto를 보내주는 것이다.\n\n## WebMvcConfigurer에 등록하기\n\n스프링내에서 기능 확장을 하기 위해서는 `WebMvcConfigurer`를 상속받아서 인터페이스 구현체를 등록해줘야 한다.\n\n```java\n@Configuration  \n@RequiredArgsConstructor  \npublic class WebConfig implements WebMvcConfigurer {  \n  \n    private final AuthArgumentResolver authArgumentResolver;  \n  \n    @Override  \n    public void addArgumentResolvers(List<HandlerMethodArgumentResolver> resolvers) {  \n        resolvers.add(authArgumentResolver);  \n    }  \n}\n```\n\n이렇게 등록해줬다. \n\n## 완성코드\n\n그럼 아까 처음에 본 postTodo()는 어떻게 변했을까?\n\n```java\n@PostMapping  \npublic String postTodo(@Login UserDto userDto) {  \n    //\n    //  \n```\n\n이렇게 토큰이 검증된 User 정보를 편하고 간편하게 쓸 수 있게 되었다.\n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n\n참고\n\n스프링 부트와 AWS로 혼자 구현하는 웹서비스 - 이동욱\n\n스프링 MVC 1편, 백엔드 웹 개발 핵심기술 - 김영한\n"},{"excerpt":"JDBC Template는 어떻게 생겨났을까? JDBC코드를 개선하며 살펴보는 기록 JDBC template가 있기 전 간단하게 users 테이블에 User를 저장하고 삭제하는 \bUserDao 메서드들을 JDBC로 작성해봤다\nDataSource는 클래스를 주입받아 사용하고 있다. 자원을 쓰는 부분을 모두 null 체크를 하고 반환해줘야 하기 때문에 try…","fields":{"slug":"/jdbc_template/"},"frontmatter":{"date":"January 29, 2024","title":"JDBC 에서 JDBC Template","tags":["spring","jdbc","designpattern"]},"rawMarkdownBody":"\n\nJDBC Template는 어떻게 생겨났을까? JDBC코드를 개선하며 살펴보는 기록\n\n## JDBC template가 있기 전\n\n간단하게 users 테이블에 User를 저장하고 삭제하는 \bUserDao 메서드들을 JDBC로 작성해봤다\nDataSource는 `SimpleDriverDataSource`클래스를 주입받아 사용하고 있다.\n\n```java\npublic void add(User user) throws SQLException {  \n    Connection c = null;  \n    PreparedStatement ps = null;  \n  \n    try {  \n        c = dataSource.getConnection();  \n        ps = c.prepareStatement(\"insert into users(id, name, password) value(?,?,?)\");  \n        ps.setString(1, user.getId());  \n        ps.setString(2, user.getName());  \n        ps.setString(3, user.getPassword());  \n  \n        ps.executeUpdate();  \n    } catch (SQLException e) {  \n    } finally {  \n        if (ps != null) {  \n            try{  \n                ps.close();  \n            } catch (SQLException e) {  \n            }  \n        }  \n        if (c != null) {  \n            try {  \n                c.close();  \n            } catch (SQLException e) {  \n            }  \n        }  \n    }  \n}\n\npublic void delete() throws SQLException {  \n    Connection c = null;  \n    PreparedStatement ps = null;  \n  \n    try {  \n        c = dataSource.getConnection();  \n        ps = c.prepareStatement(\"delete from users\");  \n        ps.executeUpdate();  \n    } catch (SQLException e) {  \n    } finally {  \n        if (ps != null) {  \n            try{  \n                ps.close();  \n            } catch (SQLException e) {  \n            }  \n        }  \n        if (c != null) {  \n            try {  \n                c.close();  \n            } catch (SQLException e) {  \n            }  \n        }  \n    }  \n}\n```\n\n자원을 쓰는 부분을 모두 null 체크를 하고 반환해줘야 하기 때문에 try-catch-finally 중 제외할 부분은 없다.\n\n일단 catch하는 부분은 비워놨는데 여기서 예외처리를 추가하면 코드는 더 길어질 것이다. \n\n위 코드는 아무런 문제가 없지만 간단한 기능들을 쓰는데 이만한 코드를 작성하는 게 맞나?라는 생각이 들 수 밖에 없다. \n\n더 문제인 점은 db에 접근하는다른 기능을 만드려면 위 코드와 아주 유사한 메소드를  똑같이 생성해줘야 한다는 점이다.\n\n## 전략패턴 활용\n\n전략패턴을 활용하여  반복되는 부분을 줄여볼 수 있을 것 같다.\n\n우리가 관심 있는 부분은 쿼리문이 바뀌는 ps 부분이니 이 부분을 interface화 하고,우리가 원하는 기능을 implement하는 클래스로 만들고, 반복되는 부분에는 전략의 구현클래스를 주입 받도록 할 수 있을 것이다. \n\ninterface 클래스\n```java\npublic interface StatementStrategy {  \n    PreparedStatement makePreparedStatement(Connection c) throws SQLException;  \n}\n```\n\nadd와 delete 클래스\n```java\npublic class AddStatement implements StatementStrategy {  \n    User user;    //add 는 user가 필요하니 주입 받는다\n    public AddStatement(User user) {  \n        this.user = user;  \n    }  \n    @Override  \n    public PreparedStatement makePreparedStatement(Connection c) throws SQLException {  \n        PreparedStatement ps = c.prepareStatement(\"insert into users(id, name, password) value(?,?,?)\");  \n        ps.setString(1, user.getId());  \n        ps.setString(2, user.getName());  \n        ps.setString(3, user.getPassword());  \n        return ps;  \n    }  \n}\n\npublic class DeleteStatement implements StatementStrategy {  \n    @Override  \n    public PreparedStatement makePreparedStatement(Connection c) throws SQLException {  \n        PreparedStatement ps = c.prepareStatement(\"delete from users\");  \n        return ps;  \n    }  \n}\n```\n\n전략패턴을 활용하여 구현체를 주입받기\n```java\npublic void jdbcWithStatementStrategy(StatementStrategy statement) throws SQLException {  \n    Connection c = null;  \n    PreparedStatement ps = null;  \n  \n    try {  \n        c = dataSource.getConnection();  \n        ps = statement.makePreparedStatement(c);//구현체마다 다른 ps를 받을 수 있다\n  \n        ps.executeUpdate();  \n    } catch (SQLException e) {  \n        throw e;  \n    } finally {  \n        close(ps, c);  \n    }  \n}  \n  \nprivate void close(PreparedStatement ps, Connection c) throws SQLException {  \n    if (ps != null) {  \n        try {  \n            ps.close();  \n        } catch (SQLException e) {  \n        }  \n    }  \n    if (c != null) {  \n        try {  \n            c.close();  \n        } catch (SQLException e) {  \n        }  \n    }  \n}\n  \n```\n\n전략패턴을 사용하는 add, delete 메서드\n```java\npublic void add(User user) throws SQLException {  \n    StatementStrategy st = new AddStatement(user);  \n    jdbcWithStatementStrategy(st);  \n}\n\npublic void delete() throws SQLException {  \n    StatementStrategy st = new DeleteStatement();  \n    jdbcWithStatementStrategy(st);  \n}\n```\n\n\n구체화된 전략클래스들을 주입받아 실행하는 추상화된 메서드를 만들었다. close()또한 메서드 추출을 진행했다.\n\n이제 우리는 필요한 기능이 생길때 interface를 상속받은 클래스를 만들고 해당 메서드를 이용만 하면 된다.\n\n처음 코드와 비교해보면 add, delete의 코드 라인 수가 훨씬 많이 줄었다.\n\n그리고 다른 기능이 추가 구현되어도 전략 클래스를 새로 만들고 추상화된 메서드에 파라미터로 넣는 간단한 기능만 만들면 되니 확장성을 갖췄다고도 볼 수 있다.\n\n하지만 전통적인 전략패턴의 문제점을 여전히 가지고 있는데, 바로 구현클래스가 계속 늘어나는 단점이다. 또한 add(), delete()가 직접 전략의 구현체를 생성하는 것도 확장성에 문제를 일으킬 수 있다.\n\n## 템플릿/ 콜백 패턴\n\n`템플릿/콜백 패턴` 이란\n\n중복되는 부분이 있는 코드에서 변경이 일어나지 않는 부분을 `템플릿`, 변경이 일어나는 부분을 `콜백`으로 분리하여 변화되는 부분만 인자로 넘겨주는 디자인 패턴이다\n\n전략패턴으로 개선한 위의 UserDao를 템플릿/콜백으로 더 개선해볼 수 있다.\n\n```java\npublic class JDBCContext {  \n  \n    private DataSource dataSource;  \n  \n    public JDBCContext(DataSource dataSource) {  \n        this.dataSource = dataSource;  \n    }  \n//주목! \npublic void executeSql(final String query, String... args) throws SQLException {  \n    jdbcWithStatementStrategy(c -> {  \n                PreparedStatement ps = c.prepareStatement(query);  \n                for (int i = 0; i < args.length; i++) {  \n                    ps.setString(i + 1, args[i]);  \n                }  \n                return ps;  \n            }  \n    );  \n}\n  \nprivate void jdbcWithStatementStrategy(StatementStrategy stmt) throws SQLException {  \n        Connection c = null;  \n        PreparedStatement ps = null;  \n  \n        try {  \n            c = dataSource.getConnection();  \n            ps = stmt.makePreparedStatement(c);  \n  \n            ps.executeUpdate();  \n        } catch (SQLException e) {  \n            throw e;  \n        } finally {  \n            close(ps, c);  \n        }  \n    }  \n  \nprivate void close(PreparedStatement ps, Connection c) throws SQLException {  \n        if (ps != null) {  \n            try {  \n                ps.close();  \n            } catch (SQLException e) {  \n            }  \n        }  \n        if (c != null) {  \n            try {  \n                c.close();  \n            } catch (SQLException e) {  \n            }  \n        }  \n    }  \n}\n```\n\n이전 jdbcWithStatementStrategy를 가진 새로운 `JDBCContext`클래스를 만들어냈다.\n즉, 반복되는 부분을 `템플릿`으로 만들어 낸 것이다.\n\n그리고 public 메서드인 executeSql()를 추가했는데 이에 주목해보자\n\n앞선 전략패턴의 문제점이었던 구현클래스가 계속 늘어나는 문제를 람다를 사용한 익명내부 클래스 생성로 해결했다. \n\n또한 varargs를 사용하여 add()처럼 필요한 인자수가 늘어나는 문제를 해결했다. \n\n그렇다면 UserDao는 어떻게 변했을까? \n```java\nprivate JDBCContext jdbcContext;  // 주입 받는다. \n//\n//\npublic void add(final User user) throws SQLException {  \n    jdbcContext.executeSql(\"insert into users(id, name, password) value(?,?,?)\",  \n            user.getId(), user.getName(), user.getPassword());  \n}\n\npublic void delete() throws SQLException {  \n    jdbcContext.executeSql(\"delete from users\");  \n}\n```\n\n이제 UserDao는 DB에 어떻게 연결되고, 예외를 처리하고, 자원을 반환하는지 상관하지 않아도 된다. \n우리가 관심있는 query문만 DB에 Access하는 DAO의 실목적에 맞는 객체가 되었다.\n\n## JDBC Template\n\n이쯤에서 JDBC Template를 사용해서 같은 기능을 만들어 보자\n\n```java\nprivate JdbcTemplate jdbcTemplate;;  // 주입 받는다. \n//\n//\npublic void add(final User user) throws SQLException {  \n    jdbcTemplate.update(\"insert into users(id, name, password) values(?,?,?)\",  \n            user.getId(), user.getName(), user.getPassword());  \n}\n```\n\n같은 기능을 하는 JdbcTemplate의 update()메서드다\n우리가 위에서 템플릿/콜백 패턴으로 만들어낸 add()기능과 똑같다. \n\n이쯤에서 해당 update() 메서드의 내부구현을 보자\n\n*JDBC template의 내부 코드를 따라가니 너무 복잡하다면 스킵하고 결론만 봐도 괜찮습니다*\n\n*+저의 부족하고 주관적인 의견으로 보는 과정이므로 틀린 부분이 있을 수 있습니다*\n\n1층\n```java \npublic int update(String sql, @Nullable Object... args) throws DataAccessException {  \n    return this.update(sql, this.newArgPreparedStatementSetter(args));  \n}\n```\nsql문과 이후 파라미터로 varargs를 써서 여러개의 Object를 받고 있다.\n일단 sql에 집중해보자\n\n2층\n```java\npublic int update(String sql, @Nullable PreparedStatementSetter pss) throws DataAccessException {  \n    return this.update((PreparedStatementCreator)(new SimplePreparedStatementCreator(sql)), (PreparedStatementSetter)pss);  \n}\n```\n쿼리문을 SimplePreparedStatementCreator에 생성자 파라미터로 넘겨주고 이를 PreparedStatementCreator로 형변환을 해주고 있다 \n\nSimplePreparedStatementCreator \n```java\nprivate static class SimplePreparedStatementCreator implements PreparedStatementCreator, SqlProvider {  \n    private final String sql;  \n    public SimplePreparedStatementCreator(String sql) {  \n        Assert.notNull(sql, \"SQL must not be null\");  \n        this.sql = sql;  \n    }  \n    public PreparedStatement createPreparedStatement(Connection con) throws SQLException {  \n        return con.prepareStatement(this.sql);  //여기\n    }  \n    public String getSql() {  \n        return this.sql;  \n    }  \n}\n```\ncreatePreparedStatement()에 주목하면 될 것 같다. \n우리가 계속 보던 prepareStatement()를 return하고 있고,\nsql의 not null을 assert하고 sql도 따로 가질 수 있는 wrapper클래스로 이해하면 될 것 같다 \n\n3층\n```java\nprotected int update(final PreparedStatementCreator psc, @Nullable final PreparedStatementSetter pss) throws DataAccessException {  \n    this.logger.debug(\"Executing prepared SQL update\");  \n    return updateCount((Integer)this.execute(psc, (ps) -> {  \n        boolean var9 = false;  \n  \n        Integer var4;  \n        try {  \n            var9 = true;  \n            if (pss != null) {  \n                pss.setValues(ps);  \n            }  \n  \n            int rows = ps.executeUpdate();  \n            if (this.logger.isTraceEnabled()) {  \n                this.logger.trace(\"SQL update affected \" + rows + \" rows\");  \n            }  \n  \n            var4 = rows;  \n            var9 = false;  \n        } finally {  \n            if (var9) {  \n                if (pss instanceof ParameterDisposer parameterDisposer) {  \n                    parameterDisposer.cleanupParameters();  \n                }  \n  \n            }  \n        }  \n  \n        if (pss instanceof ParameterDisposer parameterDisposerx) {  \n            parameterDisposerx.cleanupParameters();  \n        }  \n  \n        return var4;  \n    }, true));  \n}\n```\nthis.execute()에서 쿼리문을 처리하고 있다. \n한층만 더 내려가보자\n\n4층\n드디어 도착!\n```java\nprivate <T> T execute(PreparedStatementCreator psc, PreparedStatementCallback<T> action, boolean closeResources) throws DataAccessException {  \n    //  \n    //    \n    Connection con = DataSourceUtils.getConnection(this.obtainDataSource());  \n    PreparedStatement ps = null;  \n    Object var18;\n    //   \n    //  \n    try {  \n        ps = psc.createPreparedStatement(con);  \n        T result = action.doInPreparedStatement(ps);  \n        var18 = result;\n    } catch (SQLException var14) {  \n        //  \n        //\n\t   } finally {  \n       JdbcUtils.closeStatement(ps);  \n        DataSourceUtils.releaseConnection(con, this.getDataSource());  \n    }  \n  \n    if (closeResources) {  \n        JdbcUtils.closeStatement(ps);  \n        DataSourceUtils.releaseConnection(con, this.getDataSource());  \n    }  \n    //  \n    //\n    return var18;\n```\n예외처리에 관련된 부분은 주석처리하고 중요하다 생각되는 부분만 가져왔다. \n\n내부코드를 보면 Connection을 가져오고, Object 타입인 var18에 쿼리문 실행의 결과를 넣고 있다. 또한 try-catch-finally로 쓴 자원을 반환하고 있다. 그리고 result를 담은 var18을 반환하고 있다.\n\n우리가 만들어진 템플릿/콜백 패턴으로 만들어낸 JDBCContext와 같은 구조를 가지고 있는 것을 확인할 수 있다.\n\n## 결론\n\n순수한 JDBC를 개선하는 과정에서 변경되지 않는 부분을 따로 두고, 변경되는 부분만 추출하는 작업을 해보았다.\n\n그리고 이 과정에서 나온 결과물은 JDBCTemplate의 내부 구조와 아주 유사했다.\n\n즉, JDBC Template라는 것은 순수 JDBC에 디자인패턴(템플릿/콜백)을 적용하여 콜백에 해당하는 부분을 Public API로 드러낸 클래스라고 할 수 있다.\n\n*틀린 부분이나 부족한 부분에 대한 피드백은 언제나 환영합니다*\n\n---\n\n\n참고\n\n토비의 스프링 3.1 vol \n\n\n"},{"excerpt":"개인과제를 진행하며 Spring의 AOP 예외 처리로 코드를 개선한 기록. 과제에서 마주한 문제 개인 과제를 진행하는 도중 Entity를 수정할때 비밀번호가 다를 경우 예외를 던지는 코드를 작성하게 되었다. 이 애플리케이션이 Java로 돌아가는 커맨드라인 프로그램이었다면 종료되었을 것이다. 하지만 Spring에서 별다른 예외처리를 하지 않고 그냥 thro…","fields":{"slug":"/spring_exception/"},"frontmatter":{"date":"January 24, 2024","title":"AOP를 통한 Spring 예외처리","tags":["spring","exception","exceptionhandler"]},"rawMarkdownBody":"\n개인과제를 진행하며 Spring의 AOP 예외 처리로 코드를 개선한 기록.\n\n## 과제에서 마주한 문제\n\n개인 과제를 진행하는 도중 Entity를 수정할때 비밀번호가 다를 경우 예외를 던지는 코드를 작성하게 되었다.\n```java\n@Transactional\npublic ScheduleResponseDto updateSchedule(Long id, ScheduleRequestDto scheduleRequestDto) {\n    Schedule schedule = findSchedule(id);\n    validatePassword(schedule.getPassword(), scheduleRequestDto.getPassword());\n\n    schedule.update(scheduleRequestDto);\n    return new ScheduleResponseDto(schedule);\n}\n\nprivate void validatePassword(String origin, String input) {  \n    if(!origin.equals(input)) {  \n        throw new IllegalArgumentException(\"[ERROR] 패스워드가 다릅니다\");  \n    }  \n}\n```\n이 애플리케이션이 Java로 돌아가는 커맨드라인 프로그램이었다면 종료되었을 것이다.\n\n하지만 Spring에서 별다른 예외처리를 하지 않고 그냥 throw로 던지는 예외의 경우 500상태코드를 반환하게 된다. \n\n`사용자` 가 알맞은 비밀번호를 입력하지 않았는데, 500 상태코드가 반환된다.\n서버 에러 메시지를 보고 계속 같은 요청을 하는 사용자도 생길테고, 이는 전반적인 사용자 경험에 큰 악영향을 미친다\n또한 사용자는 자신의 요청에 대한 피드백을 제대로 받지 못한다.\n\n## 첫 시도, Early Return\n\n```java\n@Transactional  \npublic ResponseEntity<String> updateSchedule(Long id, ScheduleRequestDto requestDto) {  \n    Schedule schedule = findSchedule(id);  \n    if (!schedule.getPassword().equals(requestDto.getPassword())) {  \n        return ResponseEntity.badRequest().body(\"비밀번호가 다릅니다\");  \n    }   // early return\n\n    schedule.update(requestDto);  \n    return ResponseEntity.ok().body(\"수정 완료!\");  \n}\n```\n수정하는 메서드에서 검증을 진행하고, 비밀번호가 다를 경우 상태코드와 함께 메시지를 전달하게 바꾸어 보았다.\n\n하지만 해당 방법은 \n1. 검증이 필요한 모든 메서드에 early return을 적용해야 하고,\n2. 검증 로직에 변경이 있을 경우 응답객체의 생성방식도 변경해야 한다.\n3. 더불어 Service에서 응답객체를 만들기 때문에 Controller의 역할이 희미해진다 (싱크홀 안티패턴의 가능성)\n\n## 두번째 시도, ResponseStatusException\n\n```java\nprivate void validatePassword(String origin, String input) {  \n    if(!origin.equals(input)) {  \n        throw new ResponseStatusException(HttpStatus.BAD_REQUEST, \"[ERROR] 패스워드가 다릅니다\");\n    }  \n}\n```\nSpring에서 제공하는 `ResponseStatusException`을 이용해 보았다. \n파라미터로 HttpStatus와 String의 메시지를 넣을 수 있으니 500 상태코드 대신 내가 원하는 코드와 메시지를 전달할 수 있다.\n\n하지만 위의 방법으로는 내가 직접 예외처리 코드를 작성하지 않은 다른 예외는 처리할 수 없다. \n\n(내 의견으로는) 500 상태 코드는 정말 서버내부에서 크리티컬한 문제가 났을 때만 보여줘야한다고 생각한다. \n\n즉, 내가 해당방법으로 하나하나 예외처리 해준 케이스 이외에 사용자의 잘못된 요청으로 예외가 발생하면 여전히 500 상태코드가 보여진다. \n\n## 세번째 시도, @ExceptionHandler\n\n```java\n@ExceptionHandler(RuntimeException.class)  \npublic ResponseEntity<String> handleException() {  \n    //..  \n    return ResponseEntity.badRequest().body(\"[ERROR] 잘못된 입력입니다\");  \n}\n```\n해당 코드를 @Controller에 적용했다.\n\n@ExceptionHandler에 예외클래스를 명시한 후 @Controller에서 지정된 예외 클래스 하위의 예외가 발생시 어노테이션이 붙은 메서드를 실행하게 했다. \n\n이를 통해 직접 예외처리한 로직 이외에도 예외처리를 할 수 있게 되었고\n무엇보다도 500상태코드를 반환할 예외를 직접 지정할수 있게되었다. \n\n이 코드를 좀 더 확장성있게 쓸수 있지 않을까? \n\n## 확장하기, @ContollerAdvice\n\n```java\n@ControllerAdvice  \npublic class ControllerAdvice {  \n    @ExceptionHandler  \n    public ResponseEntity<String> handleException(Exception e) {  \n        return ResponseEntity.internalServerError().body(e.getMessage());  \n    }  \n@ExceptionHandler(IllegalArgumentException.class)  \n    public ResponseEntity<String> handleIllegalArgumentException(IllegalArgumentException e) {  \n        return ResponseEntity.badRequest().body(e.getMessage());  \n    }  \n}\n```\n\n@ControllerAdvice를 어노테이션으로 가지는 새로운 클래스를 작성했다.\n\n해당 어노테이션은 AOP(Aspect Oriented Programming)의 방식으로 예외처리를 많은 컨트롤러에서 `공통`으로 처리해야할 요소로 보고 @Controller어노테이션을 가진 모든 컨트롤러에서 발생하는 예외에 앞서 적용한 @ExceptionHandler의 예외처리 로직을 적용해 준다.\n\n코드를 보면 알수 있듯이 여러가지 Exception에 대해 어떤 응답을 보내줄지 메서드를 추가해서 확장할 수 있으므로 우리는 Java코드를 작성하는 것처럼 예외처리를 할 수 있게 되었고, 커스텀예외를 추가하는 것도 물론 가능하다. \n\n\n---\n참고\n\nhttps://mangkyu.tistory.com/204\n\nhttps://tecoble.techcourse.co.kr/post/2020-07-28-global-exception-handler/ \n\n"},{"excerpt":"의존성 주입(DI)포스팅에서 이어지는 내용입니다. Inversion of Control (제어의 역전) 이란? 객체의 컨트롤이나 프로그램의 일정부분을 프레임워크의 컨테이너으로 옮기는 소프트웨어 설계의 원리를 뜻한다. \n이 원리는 여러가지 디자인 패턴(전략 패턴, 서비스 로케이터 패턴, 팩토리 패턴)으로 실현될 수 있고 특히 의존성 주입(DI)로 가장 두드…","fields":{"slug":"/ioc_container/"},"frontmatter":{"date":"January 20, 2024","title":"IoC와 스프링 컨테이너","tags":["spring","ioc","container"]},"rawMarkdownBody":"\n의존성 주입(DI)[포스팅](https://jinkshower.github.io/dependency_injection/)에서 이어지는 내용입니다.\n\n## Inversion of Control (제어의 역전) 이란?\n\n객체의 컨트롤이나 프로그램의 일정부분을 프레임워크의 컨테이너으로 옮기는 소프트웨어 설계의 원리를 뜻한다. \n이 원리는 여러가지 디자인 패턴(전략 패턴, 서비스 로케이터 패턴, 팩토리 패턴)으로 실현될 수 있고 특히 의존성 주입(DI)로 가장 두드러지게 나타낼 수 있다.\n\n## 자바로 보는 제어의 역전\n\n```java\npublic class House {  \n    private Tv tv = new Tv();  \n    public House() {  \n    }  \n}\n```\n\n위 코드에서 House는 Tv클래스에 대한 제어권을 가지고 있다\n즉,  House는 `tv`라는 참조변수에 어떤 Tv가 들어올지 스스로가 정하고 있다고 볼 수 있다. \n\n여기에 DI를 적용해보자 \n```java\npublic class House {  \n    private Tv tv;  \n    public House(Tv tv) {  \n        this.tv = tv;  \n    }  \n}\n```\n\nHouse가 가지고 있던 제어권이 외부로 넘어갔다.\n즉, tv 객체를 생성하고 참조변수로 이어주는 역할을 더 이상 House가 하고 있지 않다.\n\n이렇게 House 객체는 객체를 생성하는 책임에서 벗어나게 되었고, 자신의 비즈니스 로직만 알아서 잘 실행하는 바람직한 객체가 되었다. \n\n하지만 이렇게 외부로 넘어간 제어권은 어디에 있을까? \nTv를 가지는 House를 만들기 위해서 우리의 코드 어디선가는 반드시\n`House house = new House(new Tv());`\n로 새로운 House를 만드는 호출을 해주어야만 한다. \n\n즉, 위 코드가 적힌 곳이 Main 이든, HouseFactory든 계속 제어권을 외부로 옮기는 것을 반복하다보면 어느 객체는 그 넘겨진 제어권을 실행해야 하는 것이다.\n\n## 의존성을 주입하는 객체 만들기\n\n그렇다면 의존성을 모두 한 곳에서 주입, 즉 제어권을 한 객체가 가지고 있다면 유지보수하기가 훨씬 쉬워지지 않을까? \n\n```java\npublic class AppConfig {  \n    public House house() {  \n        return new House(tv());  \n    }  \n    public Tv tv() {  \n//        return new Tv();  \n        return new SmartTv(): //tv interface를 가정\n    }\n}\n```\n\nAppConfig라는 객체를 생성하고, 여기에 모든 의존성 주입하는 코드를 작성했다. \n\n의존성 주입의 모든 장점을 유지하면서 제어권을 한 객체가 가지게 했기 때문에 이제 우리는 새로운 tv를 가진 House를 만들고 싶을 때 이 한 파일에 있는 코드 한 줄만 수정하면 된다. \n\n이렇게 어떤 객체가 어떻게 생성될 지, 프로그램을 구성하는 역할을 비즈니스 로직을 실행하는 객체들로부터 분리시킴으로써 우리는 해당 프로그램을 유지보수하는데에 엄청난 이점을 갖게 되었다.  \n\n하지만 여전히 의문이 든다.\n그럼 AppConfig는 어디서 생성하나?\nAppConfig 안의 house()를 호출하는 객체가 여전히 제어권을 가지고 있는 것 아닐까? \n\n## 스프링 컨테이너\n\n풀리지 않는 이 연쇄를 프레임워크로 넘김으로써 해결할 수 있다. \nIoC 컨테이너를 가지고 있는 프레임워크는 객체를 생성하고, 구성하고, 의존관계에 맞게 주입해주는 기능을 가지고 있다. \n\nSpring은 `ApplicationContext` interface로 IoC컨테이너 기능을 수행하고 있고, 구현체들은 다양한 설정 메타데이터(xml, java code, annotation)를 읽고, 이를 `Bean`이라는 객체로 만들어 준다. \n\n## \b스프링 컨테이너 사용하기\n\n```java\n@Configuration  \npublic class AppConfig {  \n    @Bean  \n    public House house() {  \n        return new House(tv());  \n    }  \n    @Bean  \n    public Tv tv() {  \n//        return new Tv();  \n        return new SmartTv():  \n    }  \n}\n```\n\n```xml\n<bean id=\"tv\" class=\"{class path}\" /> \n<bean id=\"house\" class=\"{class path}\"> \n    <constructor-arg name=\"tv\" ref=\"tv\" /> \n</bean>\n```\n\nAnnotation을 사용하거나, xml 파일로 객체 구성정보를 메타데이터화 할 수 있다. \n\nAnnotation이나 xml모두 각각의 장단점을 가지고 있는데\nAnnotation은 물론 편리하고 간단한것이 큰 장점이며 xml은  소스코드를 건드리지 않고, 컴파일을 하지도 않으면서도 메타데이터를 변경할 수 있다.\n\n위와 같이 메타데이터를 작성하면 \n스프링이 대신 객체를 각각 `Bean`으로 등록함은 물론 House에 Tv를 넣어서 생성하는 것과 같은 의존관계 설정도 자동으로 해주며 객체 라이프사이클관리도 해준다.\n\n즉, 우리는 아래와 같은 코드를 작성할 필요 없어지고 \n```java\nAppConfig appConfig = new AppConfig();\nHouse house = appConfig.house();\n```\n\n*객체 생성, 관리에 대한 제어권이 프로그래머에서 역전(Inverse) 되어 프레임워크가 맡게 된다.*\n\n---\n\n참고 \n\nhttps://www.baeldung.com/inversion-control-and-dependency-injection-in-spring\n\nhttps://docs.spring.io/spring-framework/reference/core/beans/basics.html\n\nhttps://www.inflearn.com/course/%EC%8A%A4%ED%94%84%EB%A7%81-%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8"},{"excerpt":"Preface 혼자 개발 공부를 하다 처음으로 팀으로 개발을 해보는 경험을 하게 되었다. \n자바로 캠프 관리 커맨드라인 프로그램을 만드는 Tiny, Tiny 프로젝트였지만 이 과정에서 개발을 같이 한다는 것이 무엇인지 많은 것을 깨닫게 되어서 글로 기록하고 싶어졌다.  의견 모으기 팀원들도 협업으로 개발을 해본 경험이 없었기 때문에 리더인 나의 역할이 막…","fields":{"slug":"/first_team_assignment/"},"frontmatter":{"date":"January 17, 2024","title":"첫 팀 과제 회고","tags":["team assignment","retrospective"]},"rawMarkdownBody":"\n## Preface\n\n혼자 개발 공부를 하다 처음으로 팀으로 개발을 해보는 경험을 하게 되었다. \n자바로 캠프 관리 커맨드라인 프로그램을 만드는 Tiny, Tiny 프로젝트였지만 이 과정에서 개발을 같이 한다는 것이 무엇인지 많은 것을 깨닫게 되어서 글로 기록하고 싶어졌다. \n\n## 의견 모으기\n\n팀원들도 협업으로 개발을 해본 경험이 없었기 때문에 리더인 나의 역할이 막중하다는 것을 느끼게 되었다. \n어떻게 설계를 할지, 어떻게 구현할 기능을 나눌지, 그리고 그 코드들을 어떻게 합칠지 모두 백지상태에서 정해야 했다.\n\n다행히도 우리 팀은 1주정도의 시간을 통해 모두 각자의 의견을 내고 그 의견에 대해 경청하고 고민하는 팀문화를 빌딩해왔었고 설계부터 제출까지 모든 과정을 같이 의견을 모으며 하나하나 해결해나갔다. \n\n아래는 각 과정에서 우리 팀이 어떤 문제를 마주쳤고, 어떻게 해결해 나갔는지의 기록이다\n\n## 설계\n\n팀 과제를 하기 전 개인과제를 통해 `설계에 대한 고민`이 충분히 필요하다는 것이 팀의 공통의견이었고 설계를 어떻게 해야 할지에 대해 두가지 의견으로 나뉘어졌다. \n\n1. 다 같이 기능명세서를 작성하고 그를 바탕으로 설계도를 만들기  \n2. 각자 4장의 기능명세서와 설계도를 작성한 후 합친다\n\n토의 끝에 2번으로 결정했고 그 이유는 다음과 같다\n\n1. 각자가 기능명세서를 작성함으로써 팀원 모두 해결해야할 문제점에 대해 깊이 생각할 시간을 가질 수 있다.\n2.  4장의 설계도를 가지고 더 나은 설계를 고를 수 있다 \n\n### 문제점 \n\n이후, 각자 기능명세와 설계도를 합칠 때 문제점이 발생했다.\n\n누군가는 MarkDown으로 기능명세를 작성하고 클래스 다이어그램을 그려오고, 누군가는 머릿속에서 정리하고, 클래스 다이어그램에 모든 것을 그린 사람도 있는 등 각자가 제각기의 방식으로 기능명세와 설계도를 완성했기에 이를 합치는데 많이 시간과 비용이 소요되었다.\n\n정리하자면\n1. 기능명세, 설계도에 대한 공통의 마인드 모델이 없었고 \n2. 각자를 만들어내는 툴이 일치하지 않았다.\n\n### 해결\n\n툴을 확정하고 기능명세와 설계도를 팀내에서 나름대로 정의를 내렸다.\n\n선택된 툴   \n`기능명세` : MarkDown   \n`설계도` : Draw.io\n\n정의   \n`기능명세` : 조사, 수식여구등을 생략하고 구현해야할 모든 기능을 최대한 구체화한 글   \n`설계도` : 클래스 명과 필드명으로 한 객체를 표현하고 연관관계만 선으로 표현한 다이어그램 \n\n으로 결정하여 해당 문제를 해결했다 \n\n## 구현\n\n기능명세와 설계도를 완성한 후, 이 둘을 바탕으로 클래스 다이어그램에서 정한 클래스명, 필드명을 다 같이 몹 프로그래밍으로 구현했다.\n\n이 후 구현할 기능을 각자의 희망대로 분배했고 다행히 희망사항이 겹치지 않아 각자 비슷한 분량의 기능을 구현하게 되었다. \n\n내가 맡은 기능은 `수강생 점수 등록` 이었다.\n\n### 문제점\n\n구현 과정에서 많은 문제점이 발생했다.\n\n개인적인 문제점은 내가 기능을 만들기 위해서는 미리 구현되어야할 `수강생 등록`을 다른 팀원이 구현 중이었기 때문에 팀원이 구현할 기능에 대해 짐작만 하고 코드를 작성할 수 밖에 없었다. \n이는 결국 개인적으로 만족스럽지 못한 코드로 이어지게 되었다.\n\n팀적인 문제점은 몹 프로그래밍으로 작성한 초기 코드를 각자 로컬로 받으면서 발생했는데, \n1. JDK 버전이 통일되지 않았고, `.gitignore`이 모두 달라 초기 환경설정에서 막히는 팀원들이 발생했다.\n2. git에 대한 이해도가 모두 달라서 프로젝트 진행이 불가한 팀원들이 발생했다.\n\n### 해결\n\n모든 해결 과정을 화면공유로 팀원 모두가 참여한 상태에서 함께 해결했다.\n\n1. JDK버전을 17로 통일했다.\n2. `idea` 폴더를 `.gitignore`에 포함시킨 후 브랜치를 다시 배포했다\n3.  git 관련 아티클을 재공유했고 문제가 생긴 팀원의 화면을 같이 보며 git 명령어를 재숙지했다.\n\n## 합치기\n\n위 과정을 해결한 후 각자 동일한 환경설정으로 맡은 기능을 구현할 수 있게 되었다.\n이후 시간을 정해 기능을 각자 완성하고 함께 모여 코드를 합치기로 하였다.\n\n1. `main`- `dev`-`feature` 로 브랜치를 나누었다\n2. `main`은 제출용 브랜치로 완성된 코드만 가지도록 하였다\n3. `dev`에 `feature`를 Pull Request로 Merge하였다\n\nMerge는 몹 프로그래밍으로 모두 합의를 통해 진행했다\n\n### 문제점\n\n합치기 과정에서 가장 큰 문제가 발생했다 \n\n내가 처음으로 pull request 를 Merge하고, conflict를 해결하는 과정을 화면으로 공유했고 차례대로 다른 팀원들의 Pull Request를 Merge하는 과정에서 발생한 문제점이다.\n\n1.   각자 기능을 4분할로 나누고 하나의 Pull Request를 합치니 하나의 PR 코드양이 너무 많았다\n2.  깃 가이드라인을 팀내에서 문서화 하지 않아 합의했던 브랜치 전략을 놓친 팀원이 발생했다 \n3.  코드 컨벤션을 팀내에서 문서화 하지 않아 서로의 코드를 이해하기 쉽지 않았다\n4.  1번과 3번이 겹쳐져 수많은 Conflict가 발생했고 이를 수습하는 식으로 Merge가 진행됐다\n\n### 해결\n\n합치기에서의 `문제점`이 너무 치명적이라 팀내에서 긴급하게 다음 방향을 토의했다. \n처음부터 다시하기 혹은 지금 디버깅하며 코드를 조금씩 고쳐나가기, 두 가지 의견이 나왔고 \n제출일이 당장 다음날이라 디버깅하며 코드를 조금씩 고쳐나가기로 결정했다 \n\n나와 팀원 한분이 번갈아 드라이버를 잡고 몹프로그래밍으로 디버깅을 했고 해당 과정이 많이 어려웠다. \n몇 백줄이 되는 익숙치 않은 변수, 메서드명, 코드 구조를 따라가야 했고, 테스트 코드도 작성하지 않았기 때문에 일일히 출력하고 Main을 돌려가며 디버깅을 진행했다.\n\n다행히도 각자가 기능을 잘 완성해주어서 해결해야할 문제가 그리 크지는 않았다.\n합치기를 마치고 이후 시간이 남을 정도였고, 추가 구현 기능은 `합치기` 과정에서 느낀 점을 토대로 페어프로그래밍으로 코드리뷰를 상세히 하여 Merge하였고 \n결과적으로 만족스러운 코드를 제출할 수 있게 되었다.\n\n완성 레포지토리\nhttps://github.com/jinkshower/CampManagement\n\n## 첫 협업이 나에게 남긴 것\n\n`합치기`를 `해결`하면서, 지금까지 왜 지켜야 하는지 와닿지 않았던\n`코드 컨벤션`, `커밋 컨벤션`, `깃 브랜치 전략` 들을 문서화하고 팀원 모두가 지키는 것에 대한 필요성을 뼈저리게 느끼게 되었다. \n\n또한 왜 모든 개발교육과정에서 `소통` 과 `공유`를 강조하는지를 이해할 수 있었다.\n우리 팀이 마지막에 그나마 만족스러운 결과를 낼 수 있었던 이유는 서로 의견을 가감없이 말하고 그에 대해 진중히 고민하고, 장단점을 살펴 대안을 고르는 `팀 문화`를 빌딩해왔기 때문이라고 생각한다\n\n개인적으로는 나의 `커뮤니케이션 방식`을 되돌아보는 계기가 되었다.\n나는 건의사항이 있으면 언제나 말해야 하고, 내가 생각할 때 불합리하거나 비효율적인 일은 언제나 토의를 통해 개선할 수 있다고 생각한다. \n\n하지만 그러한 의견을 내면서 `쿠션어`를 사용하는 등 이른바 `둥글게 말하기`는 연습이 필요한 부분인 것 같다. \n\n마지막으로 딱딱하게 글을 적었지만 혹시나 이 글을 볼지도 모르는 나의 첫 개발팀원분들에게 감사를 전한다. \n"},{"excerpt":"Algorithms 강의를 들으며 공부한 기록 Union Find 두 원소가 같은 집합내에 있는지 확인할때 사용하는 알고리즘이다.\nDynamic connectiviy의 자료구조 중 그래프에 edge가 추가되기만 하는 구조(Incremental connectivity) 에서 사용할 수 있다.   로 p와 q를 같은 집합으로 만들고\n로 p의 루트를 찾거나 로…","fields":{"slug":"/union_find/"},"frontmatter":{"date":"January 14, 2024","title":"Union Find","tags":["algorithms","union_find"]},"rawMarkdownBody":"\n\nAlgorithms 강의를 들으며 공부한 기록\n\n## Union Find\n\n두 원소가 같은 집합내에 있는지 확인할때 사용하는 알고리즘이다.\nDynamic connectiviy의 자료구조 중 그래프에 edge가 추가되기만 하는 구조(Incremental connectivity) 에서 사용할 수 있다. \n\n`union(int p, int q)` 로 p와 q를 같은 집합으로 만들고\n`find(int p)`로 p의 루트를 찾거나 `connected(int p, int q)`로 두 요소가 연결되었는가를 확인하는 알고리즘이다. \n\n```java\npublic interface UF {  \n    void union(int p, int q);  \n    boolean connected(int p, int q);  \n}\n```\n\n## Quick-find\n\n가장 기본적인 방법으로 id array에 각 요소를 매핑하고 `union(int p, int q)`가 호출되면 모든 id array를 loop로 돌며 p의 id와 같은 id를 q의 id로 변경하는 방법이다\n\n```java\npublic class QuickFindUF implements UF {  \n    private int[] id;  \n    public QuickFindUF(int N) {  \n        id = new int[N];  \n        for (int i = 0; i < N; i++) {  \n            id[i] = i; // id 배열 매핑\n        }  \n    } \n    @Override  \n    public boolean connected(int p, int q) {  \n        return id[p] == id[q];  // id 확인\n    }  \n    @Override  \n    public void union(int p, int q) {  \n        int pid = id[p];  \n        int qid = id[q];  \n        for (int i = 0; i < id.length; i++) {  \n            if (id[i] == pid) {  \n                id[i] = qid;  \n            }  \n        }  // p와 id가 같은 모든 id를 q의 id로 바꾼다 \n    }  \n}\n```\n\n- initialize : n\n- union : n\n- find : n\n\nn개의 요소에 n의 union은 n^2의 시간이 걸린다 \n\n## Quick-Union\n\nid array를 쓰는 것은 비슷하지만 이번에는 i의 부모를 id array에 매핑한다. 즉 `union(int p, int q)`를 호출하면 p의 부모의 id를 q의 부모의 id로 바꾼다. \n\n```java\npublic class QuickUnionUF implements UF {  \n    private int[] id; \n    public QuickUnionUF(int N) {  \n        id = new int[N];  \n        for (int i = 0; i < N; i++) {  \n            id[i] = i;  \n        }  \n    }  \n    private int root(int i) {        \n        while (i != id[i]) {  \n            i = id[i];  \n        }  \n        return i;  // 부모 루트를 최상단까지 찾는다 \n    }  \n    @Override  \n    public void union(int p, int q) {        \n        int i = root(p);  \n        int j = root(q);  \n        id[i] = j;  //p의 루트를 q의 루트로 바꾼다\n    }   \n    @Override  \n    public boolean connected(int p, int q) {  \n        return root(p) == root(q);  \n    }  \n}\n```\n- initialize : n\n- union : n (root 찾는 비용 포함)\n- find : n\n\nQuick-Find 보다 더 빠른 실행을 보이는 케이스도 있지만 알고리즘은 항상 최악의 경우를 상정해야 하므로 트리구조가 엄청나게 길거나 길이가 N이 될때 Quick-Find와 비슷하게 n^2의 실행시간을 가지게 된다\n\n## Quick Union 개선하기 \n\n### Weighting\n\n현재 Quick Union은 트리의 크기와 상관없이 무조건 p를 q의 루트에 갖다 \n붙이기 때문에 트리의 길이가 엄청나게 길어지는 문제점을 가지고 있다. \n\n하지만 union을 실행할때 트리의 사이즈를 비교하고 작은 트리를 보다 큰 트리에 연결하면 트리의 깊이를 짧게 유지할 수 있다\n\n```java\npublic class QuickUnionUF implements UF {  \n    private int[] id; \n    public QuickUnionUF(int N) {  \n        id = new int[N];  \n        for (int i = 0; i < N; i++) {  \n            id[i] = i;\n            sizes[i] = i; // size도 같이 매핑한다\n        }  \n    }  \n    private int root(int i) {        \n        while (i != id[i]) {  \n            i = id[i];  \n        }  \n        return i;\n    }  \n    @Override  \n    public void union(int p, int q) {        \n        int i = root(p);  \n        int j = root(q);  \n        if (i == j) {\n\t        return; //루트가 같으면 early return\n        }\n        if (size[i] < size[j]) {\n\t        id[i] = j;\n\t        size[j] += size[i]; \n        }\n        else {\n\t        id[j] = i;\n\t        size[i] += size[j]; //사이즈 비교 후 작은 트리를 큰 트리에 병합한다\n        }\n    }   \n    @Override  \n    public boolean connected(int p, int q) {  \n        return root(p) == root(q);  \n    }  \n}\n```\n\n- initialize : n\n- union : lg n(root 찾기 까지 포함)\n- find : lg n\n\n`왜 lg n의 비용으로 줄어들었을까?` \n\nT1의 크기가 3이고  T2의 크기가 5일때 T1에 있는 a를 T2에 있는 b에 연결시킨다고 가정해보자. \n\n이때 T1은 T2에 병합되고 a의 깊이는 1이 증가하게 된다. union을 호출할때 a의 깊이는 1이 증가하는데에 비해 a가 속한 트리의 크기는 3에서 8로 최소 2배이상이 증가하게 된다.\n\n이를 계속 실행하면 \n\n| a의 깊이 | 0 | 1 | 2 | 3 | ... | lg N |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| a가 속한 트리의 크기 | 1 | 2 | 4 | 8 |  | N |\n\n이 되기 때문에 find의 비용이 절감하게 된다\n\n### Path-Compressing\n\n위의 알고리즘을 Path-Compressing으로 더 개선할 수 있다   \n\n현재는 `root()`를 while문으로 모든 깊이의 노드를 반복하여 검색하고 있다.\n\n이 때 이 기능이 호출될때마다 각 호출된 노드의 id를 루트의 id로 바꿔주는 작업을 하면 트리의 깊이를 더 평탄하게 바꾸어 줄 수 있다.\n\n위의 코드의 root를 \n```java\n    private int root(int i) {        \n        while (i != id[i]) {  \n\t        id[i] = id[id[i]] // i의 루트를 부모 루트로!\n            i = id[i];  \n        }  \n        return i;\n    }  \n```\n이렇게 바꿔주기만 해도 트리의 깊이가 계속 평탄화 되면서 root를 호출하는 모든 기능의 비용이 절감된다. \n\n\n\n참고\nhttps://www.coursera.org/learn/algorithms-part1"},{"excerpt":"의존성이란 객체 지향 언어에서 A 객체가 B객체를 이용할때 A는 B를  고 표현한다.  A가 생성될 때 B가 필요할 수도 있고, A의 메서드의 파라미터로 B가 있을 수도 있다.  House객체가 Tv객체를 이용하는 예시를 들어보자 더이상 House객체가 변하지 않으면 위 코드는 문제가 없다.  하지만 House가 새로운 를 가져야 한다면? House를 테…","fields":{"slug":"/dependency_injection/"},"frontmatter":{"date":"January 11, 2024","title":"의존성 주입(Dependency Injection)","tags":["dependency_injection","java"]},"rawMarkdownBody":"\n## 의존성이란\n\n객체 지향 언어에서 A 객체가 B객체를 이용할때 A는 B를 `의존한다` 고 표현한다.  A가 생성될 때 B가 필요할 수도 있고, A의 메서드의 파라미터로 B가 있을 수도 있다. \n\nHouse객체가 Tv객체를 이용하는 예시를 들어보자\n\n```java\npublic class House {\n\tprivate Tv tv = new Tv(); // House는 Tv에 의존한다\n\t\n\tpublic House() {\n\t}\n}\n```\n\n더이상 House객체가 변하지 않으면 위 코드는 문제가 없다. \n\n하지만 House가 새로운 `SmartTv`를 가져야 한다면? House를 테스트할 때 다른 Tv 종류를 넣어보고 싶다면?\n\nTv를 인터페이스화 해서 다른 Tv를 넣을 수는 있지만 임시방편일 뿐이다. \n\n```java\npublic class House {\n\t//Tv tv = new Tv(); \n\tprivate Tv tv = new SmartTv(); //tv에 다른 구현체를 넣었지만..\n\t\n\tpublic House() {\n\t}\n}\n\npublic interface Tv {\n\tvoid turnOn();\n}\n\npublic class SmartTv implements Tv {\n\t// some logic\n}\n```\n\n이렇게 객체가 다른 객체에 강한 의존성을 지니면 코드를 재사용하거나 확장하는데에 문제점이 생긴다.\n\n## 의존성을 주입하자 \n\nHouse가 여러 Tv를 사용하는 것에 제약이 생긴 이유는 House가 어떠한 종류의 Tv를 자신의 상태로 가질지 미리 알고 있었기 때문이다 .\n\nDependency Injection은 객체 간의 의존관계를 느슨하게 설정해놓고 Compile Time이 아닌 Runtime에 객체가 의존하고 있는 객체를 생성 후 넣어주는 방식을 의미한다.\n\n\n- 생성자를 이용하는 방식\n\n```java\npublic class House {\n\tprivate Tv tv; //House는 어떤 Tv를 가질지 모른다\n\t\n\tpublic House(Tv tv) {\n\t\tthis.tv = tv; //외부에서 이미 생성된 tv를 주입받는다\n\t}\n}\n\npublic class Main {\n\tpublic static void main(String[] args) {\n\t\tHouse house = new House(new Tv()); //주입\n\t}\n}\n```\n\n- setter를 이용하는 방식\n\n```java\npublic class House {\n\tprivate Tv tv; //House는 어떤 Tv를 가질지 모른다\n\t\n\tpublic void setTv(Tv tv) {\n\t\tthis.tv = tv; //외부에서 이미 생성된 tv를 주입받는다\n\t}\n}\n\npublic class Main {\n\tpublic static void main(String[] args) {\n\t\tHouse house = new House();\n\t\thouse.setTv(new Tv()); //주입\n\t}\n}\n```\n\n## 의존성 주입의 장점\n\n1.  A가 B의 변경을 알 필요가 없어진다\n\nTv를 생성할때 리모컨, 버튼, 안테나 등 다양한 요소가 필요하다고 해보자. 의존성이 강할 때 Tv가 변경되면 House도 같이 변경되어야 했다. \n하지만 의존성을 주입하면 Tv가 어떻게 변경되어도 House객체 내의 코드는 수정할 필요가 없어진다. \n\n2.  A를 테스트하기 쉬워진다\n\nA와 B의 의존관계가 느슨해졌기 때문에 A와 B를 독립적으로 테스트 하는 것이 쉬워졌고 \nA에 interface화한 Tv의 여러 구현체를 주입시키는 테스트도 가능해진다. \n\n```java\n@Test  \nvoid test() {  \n    House house1 = new House(new Tv());  \n    House house2 = new House(new SmartTv());  \n}\n```\n\n\n3.  A의 public API가 명시적이게 된다\n\n의존성을 주입하기 전 House의 API를 보자\n```java\npublic class House()\n```\nHouse의 코드를 열어보지 않는 한 House가 Tv를 가지고 있는지 알 길이 없다. \n\n의존성을 주입한다면 \n```java\npublic class House(Tv tv);\n```\n가 될 것이고 House를 사용하고자 하는 다른 개발자들은 누구나 House가 Tv를 의존하는 객체임을 알 수 있다.\n"},{"excerpt":"개인 과제에서 을 적용해 코드를 개선한 기록 Builder Pattern? Effective Java는 많은 생성자 파라미터를 다루어야 할 경우 Builder Pattern을 고려하라고 말한다.  Builder Pattern은 원하는 객체를 바로 생성하는 것이 아니라, 클래스 내에 Builder라는 내부 클래스를 만든 후 Builder 클래스를 이용해 객…","fields":{"slug":"/builder_pattern/"},"frontmatter":{"date":"January 09, 2024","title":"Builder Pattern으로 코드 개선하기","tags":["builder_pattern","java"]},"rawMarkdownBody":"\n개인 과제에서 `Builder Pattern`을 적용해 코드를 개선한 기록\n\n---\n## Builder Pattern?\n\nEffective Java는 많은 생성자 파라미터를 다루어야 할 경우 Builder Pattern을 고려하라고 말한다. \n\nBuilder Pattern은 원하는 객체를 바로 생성하는 것이 아니라, 클래스 내에 Builder라는 내부 클래스를 만든 후 Builder 클래스를 이용해 객체를 생성하는 기법이다. \n\n## Menu\n\n```\npublic class Menu {    \n    private final String name;  \n    private final String description;  \n    private final double price;  \n    private final List<Option> options;\n}\n```\n와 같은 네 개의 멤버 변수를 가진 Menu 클래스를 만드려 한다. \n`name`과 `price`는 필수적으로 포함되어야 하지만 `description`과 `options`는 메뉴에 따라 있을 수도, 없을 수도 있는 선택 매개변수이다.\n\n## 첫 번째 시도, public 생성자\n\n```\npublic Menu(String name, String description, double price, List<Option> options) {\n\tthis.name = name;\n\tthis.description = description;\n\tthis.price = price;\n\tthis.options = options\n}\n```\n가장 기본적인 public 생성자이다. \n얼핏 보면 아무 문제도 없어보이지만 `Menu`를 인스턴스화하며 코드에서 사용하려 해보자\n```\nnew Menu(\"Shack Burger\", \"너무 맛있는 쉑버거\", 6.5, List.of(new   Option(\"Regular\", 0),  \n        new Option(\"Large\", 0.9))\n```\n이 코드는\n\n1.  같은 String타입의 `name`과 `descripton`이 정확한 순서로 쓰여져야 하고\n2.  `description`, `options`가 필요없는 경우를 대처할 수 없기 때문에\n\nMenu를 인스턴스화 할때마다 Menu의 생성자를 매번 확인해야 한다.  \n\n## 두번째 시도 Telescoping Constructor (점층적 생성자) \n\n위의 코드를 조금 개선해보자.\n```\npublic Menu(String name, String description, double price, List<Option> options) {  \n    this.name = name;  \n    this.description = description;  \n    this.price = price;  \n    this.options = new ArrayList<>(options);  \n}  \n  \npublic Menu(String name, double price) {  \n    this(name, \"\", price, new ArrayList<>());  \n}  \n  \npublic Menu(String name, String description, double price) {  \n    this(name, description, price, new ArrayList<>());  \n}\n\npublic Menu(String name, double price, List<Options> options) {\n\tthis(name, \"\", price, options);\n}\n```\n\n점층적 생성자를 이용해 `description`, `options`모두 없는 경우, 하나만 없는 경우의 조합을 상정하고 순서대로 `this()`를 호출하며 생성시 주어지지 않은 파라미터는 default 값을 이용하도록 해보았다.\n\n점층적 생성자를 이용해 이제 \n\n```\nnew Menu(\"Burger\", 1000)\n```\n\n위와 같은 Menu의 생성도 가능해지게 되었다.\n\n하지만 점층적 생성자 또한\n\n1. 순서를 기억하기 어렵고, \n2. Menu가 더 많은 파라미터를 요구하게 될 시의 조합을 가진 생성자를 더 생성해야 하기 때문에\n\n유지보수가 어렵다는 문제가 여전히 남아 있다. \n\n## 세번째 시도, Builder Pattern\n\n이러한 Menu 클래스를 Builder Pattern을 이용하여 리팩토링 해보았다.\n```\npublic static class Builder {  \n  \n    private final String name;  \n    private final double price;  \n  \n    private String description = \"\";  \n    private List<Option> options = new ArrayList<>();  \n  \n    public Builder(String name, double price) {  \n        this.name = name;  \n        this.price = price;  \n    }  \n  \n    public Builder description(String description) {  \n        this.description = description;  \n        return this;    }  \n  \n    public Builder options(List<Option> options) {  \n        this.options = new ArrayList<>(options);  \n        return this;    }  \n  \n    public Menu build() {  \n        return new Menu(this);  \n    }  \n}\n\nprivate Menu(Builder builder) {  \n    this.name = builder.name;  \n    this.description = builder.description;  \n    this.price = builder.price;  \n    this.options = builder.options;  \n}\n```\n\nMenu 클래스 생성자의 접근제어자를 `private`으로 두고 내부 클래스로 Builder를 만들었다.  `private`생성자는 `Builder`가 가져다준 매개변수를 저장한다. \n\nBuilder는 기본적으로 필수적인 매개변수인 `name`과 `price`를 생성자의 파라미터로 받고 선택적인 매개변수인 `description` 과 `options`를 초기화를 해주었다.\n\n필수 매개변수만 받고 나머지는 메서드 체이닝을 통해 setter와 같은 역할을 하며 마지막으로 `build()`메서드로만 Menu를 인스턴스화 할 수 있게 했다. \n\n이를 통해 \n```\nnew Menu.Builder(\"Shack Burger\", 6.5)  \n        .description(\"너무 맛있는 쉑버거\")  \n        .options(List.of(new Option(\"Single\", 0),  \n                new Option(\"Double\", 3.6)))  \n        .build()\n```\nMenu를 위와 같이 인스턴스화 할 수 있게 되었다. \n\n그리하여\n1. 생성자에 대한 컨트롤   \n정해진 방식으로만 객체가 생성될 수 있게 했고 \n\n2. 가독성    \n생성자 파라미터에 메서드 명을 붙임으로써 객체 생성시의 실수가 줄어든다. \n같은 타입의 멤버 변수를 파라미터로 받아 들일 시 순서가 헷갈리거나 잘못된 값을 저장할 수 있는 문제도 메서드 명을 지정해야 하므로 방지 할 수 있다.  \n\n3. 확장성  \n메서드를 추가하면 되기 때문에 4개 그 이상의 파라미터 확장 혹은 파라미터에 대한 검증 추가에 더 유연하게 대처할 수 있다 \n\n## 내가 느낀 Builder Pattern의 단점\n\n빌더 패턴을 사용하며 느낀 단점은\n\n1.  바로바로 생성할 수 있는 public 생성자와 달리 코드를 작성하는데 비용이 든다.\n2.  매개변수가 적은 경우 오히려 객체가 무거워진다 \n\n정도다. 하지만 테스트 코드 작성같이 다른 객체에서 Menu를 인스턴스화 할때 객체 생성에 실수가 줄어들고 이미 생성하고 있는 Menu 코드에 새로운 option을 추가한다거나 설명을 바꿀 때 편리함을 느껴서 매개변수가 많을 때는 Builder Pattern을 많이 사용할 것 같다."},{"excerpt":"'모든 개발자를 위한 HTTP'강의를 수강한 학습 기록 캐시란? 컴퓨터과학분야에서 데이터나 값을 미리 복사해 놓는 임시장소를 의미한다\nCPU에서 캐시메모리는 CPU, 메모리 사이에 위치하여 자주 쓰이는 데이터를 임시로 저장하여 둘 사이의 거리에 따른 접근시간을 줄이는데 사용된다.  캐시는 다양한 종류가 있지만 웹에서의 캐시도 CPU의 캐시 메모리와 비슷하…","fields":{"slug":"/http_cache/"},"frontmatter":{"date":"January 03, 2024","title":"웹 서비스 캐시","tags":["http","cache"]},"rawMarkdownBody":"\n'모든 개발자를 위한 HTTP'강의를 수강한 학습 기록\n\n## 캐시란?\n\n컴퓨터과학분야에서 데이터나 값을 미리 복사해 놓는 임시장소를 의미한다\nCPU에서 캐시메모리는 CPU, 메모리 사이에 위치하여 자주 쓰이는 데이터를 임시로 저장하여 둘 사이의 거리에 따른 접근시간을 줄이는데 사용된다. \n\n캐시는 다양한 종류가 있지만 웹에서의 캐시도 CPU의 캐시 메모리와 비슷하게 `자주 쓰이는 데이터를 가까운 곳에 저장하여 데이터 접근 시간을 줄인다` 는 원리를 가진다.\n\n## 웹 브라우저 캐시\n\n클라이언트에서 서버에 요청하여 10mb의 코끼리 이미지를 다운로드 받는다고 생각해보자.    \n\n캐시가 없다면 첫번째 요청에서는 10mb의 데이터를 전송받아야하지만 같은 사이트에 몇번이고 방문한다면 여러번의 요청마다 10mb의 데이터를 매번 다운로드 받아야 한다. 이러면 웹페이지의 로딩 속도는 매우 느려지고 좋지 않은 사용자 경험을 선사한다.\n\n반면 웹 브라우저를 사용하는 사용자의 로컬 환경에 캐시데이터를 저장하고 사용자가 같은 요청(코끼리 그림을 브라우저에 그려주세요)을 할 때는 서버에서 다운로드를 다시 받지 않고 캐시데이터를 바로 사용한다면 웹 페이지의 로딩 속도는 비약적으로 상승할 것이다. \n\n### 웹 브라우저 캐시 적용\n\n웹 브라우저에서 서버로 코끼리 그림을 최초로 요청하면 서버는 이에 대해`cache-control: max-age` 헤더를 포함한 응답을 준다.\n\n간단한 응답예시\n```\nHTTP/1.1 200 OK\nContent-Type:image/jpeg\nCache-Control: max-age=60\nContent-Length: 10295\n\n//\n{elephant image data}\n//\n```\n\n이때 `max-age` 에 해당하는 숫자는 이 캐시가 유효한 초단위를 의미한다.\n\n즉 이 코끼리 그림은 로컬의 캐시 저장소에서 60초간 유효하며 이 시간안에 클라이언트가 코끼리 그림을 다시 요청하면 서버에서 다시 코끼리 그림 데이터를 다운받는 게 아니라 캐시 저장소에 있는 복사된 데이터를 브라우저에서 렌더링 하게 되는 것이다.\n\n### 캐시 유효기간 \n\n당연히 캐시 유효기간이 지난 데이터의 경우 서버에 재요청을 하여 다시 다운로드 받아야 한다. 이 때 유효기간이 지난 캐시를 Stale하다라고 표현한다.    \n\n하지만 캐시 유효 기간이 지났지만 서버의 원본 데이터가 변경되지 않은 경우, 같은 데이터인데도 다시 다운로드 받아야 하면 이는 굉장히 비효율적이다. 이를 위해 캐시와 원본이 같은 데이터인지 검증하고, 같은 데이터라면 캐시를 사용하는 절차가 필요하다.\n\n## 캐시 유효성검증(Validation)과 조건부 요청(Conditional Request)\n\n캐시 데이터가 원본 데이터와 같은지 검증하기 위해 크게 두가지 방법을 사용한다\n\n### Last-Modified / If-Modified-Since\n\n첫 번재 방법은 원본 데이터가 마지막으로 수정된 시간을 응답 메시지 헤더로 추가하는 방법이다. \n\n```\nHTTP/1.1 200 OK\nContent-Type:image/jpeg\nCache-Control: max-age=60\nLast-Modified: Wed, 21 Oct 2015 07:28:00 GMT //added\nContent-Length: 10295\n\n//\n{elephant image data}\n//\n```\n이제 캐시 저장소에 코끼리 그림과 이 원본 데이터가 마지막으로 수정된 시각까지 저장을 하게 된다. 이제 클라이언트는 요청을 할때 \n```\nGET /elephant.jpg\nif-modified-since: Wed, 21 Oct 2015 07:28:00 GMT\n```\n'이 이후로 원본이 수정되었으면' 이라는  조건부 요청을 보내고 서버에서는 이러한 조건부 요청에 따라    \n조건이 만족하면(수정 되었으면) 200 OK로 다시 10mb의 코끼리 그림을 전송하고 조건을 불만족하면(수정 되지 않았다면 )\n```\nHTTP/1.1 304 Not Modified\nContent-Type:image/jpeg\nCache-Control: max-age=60\nLast-Modified: Wed, 21 Oct 2015 07:28:00 GMT //added\nContent-Length: 10295\n\n//\n\n//\n```\n10mb의 message body가 생략된 304 Not Modified의 응답 메시지를 보낸다. \n\n위 방법은 합리적으로 보이지만 단점도 가지고 있다\n1. 1초 미만 단위로 캐시 조정이 불가능하고\n2.  A->B->A 처럼 원본데이터가 수정되었지만 수정 전과 같은 데이터 일때\n3. 스페이스, 주석 변경처럼 크게 영향이 없는 경우\n에도 모두 재다운로드가 발생하기 때문이다.\n\n\n### ETag / If-None-Match\n\n\n위의 단점을 보완하고 캐시 로직을 서버에서 관리하고 싶을 때 ETag(Entity Tag)를 이용하게 된다 .\nETag는 각각의 캐시될 원본 데이터에 해쉬코드를 달아준다고 생각하면 된다. 데이터가 변경되고 이 데이터가 클라이언트에서 다시 다운로드 받아야 된다고 판단될 때 ETag를 변경해주면 된다. \n\n서버는 최초 응답 메시지에 코끼리 데이터와 함께 ETag를 제공한다.\n```\nHTTP/1.1 200 OK\nContent-Type:image/jpeg\nCache-Control: max-age=60\nETag: \"aaaaaaa\" //added\nContent-Length: 10295\n\n//\n{elephant image data}\n//\n```\n캐시 저장소는 이제 코끼리 그림 데이터를 ETag와 함께 저장하고 \n\n재요청시 조건부 요청을 보내게 된다.\n```\nGET /elephant.jpg\nif-none-match: \"aaaaaaa\"\n```\n위와 마찬가지로 조건을 불만족하면(수정 되지 않았다면 ) 서버는 304 상태코드와 메시지 바디가 없는 응답 메시지를 보내고 클라이언트는 캐시 저장소에 있는 코끼리 그림을 웹 브라우저에 렌더링한다.\n\n## 캐시 지시어\n\n캐시를 제어하는 방법은 위의 방법만 있는 게 아니다. 항상 최신이 되어야 하는 데이터나 개인정보 처럼 캐시를 해서는 안되는 데이터도 있기 때문이다.\n\nCache-directive(캐시 지시어)로 해당 데이터의 캐시가 어떻게 적용될지 설정할 수 있다. \n\n###  Cache-Control: no-cache\n\n데이터를 캐시해도 되지만 항상 Origin서버에 검증하고 사용되어야 하는 데이터다. 즉, 항상 최신의 데이터를 가지게 하고 싶을 때 사용한다\n\n### Cache-Control: no-store\n\n민감한 정보가 포함된 데이터이므로 캐시해서는 안된다.\n\n### Cache-Control: must-revalidate\n\n캐시 만료 후 최초조회시 Origin서버에 검증해야한다\nOrigin 서버 접근 실패시 반드시 오류가 발생해야한다.\n\n### Cache-Control: public\n\n응답이 public 캐시에 저장되어도 된다 (프록시 서버에 저장되어도 된다)\n\n### Cache-Control: private\n\n응답이 해당 사용자 만을 위한 것 private 캐시에 저장해야함(기본값)\n\n"},{"excerpt":"'혼자 공부하는 시스템구조&운영체제'를 읽고 공부한 내용 프로세스 Process? Program? 우리는 흔히  라고 말한다.\n이는 '보조기억장치에 있는 데이터 뭉치'인 을  '메모리에 적재하고 cpu가 실행'하여 화 한다와 같은 말이다.  PCB Process Control Block 모든 프로세스는 CPU를 사용해야 하지만 CPU의 자원은 한정적이다.…","fields":{"slug":"/process_thread/"},"frontmatter":{"date":"December 28, 2023","title":"프로세스와 스레드","tags":["process","thread","operating_system"]},"rawMarkdownBody":"\n'혼자 공부하는 시스템구조&운영체제'를 읽고 공부한 내용\n## 프로세스 \n\n### Process? Program?   \n우리는 흔히 `프로그램을 실행한다` 라고 말한다.\n이는 '보조기억장치에 있는 데이터 뭉치'인 `프로그램`을  '메모리에 적재하고 cpu가 실행'하여 `프로세스`화 한다와 같은 말이다. \n\n### PCB Process Control Block    \n모든 프로세스는 CPU를 사용해야 하지만 CPU의 자원은 한정적이다.\n그래서 프로세스의 실행 순서와 자원관리를 위해 프로세스마다 `PCB`라는 부가 정보를 Kernel 영역에 프로세스과 함께 저장한다.\n\n>[PCB에 저장되는 정보]   \n>PID(Process ID) : 프로세스 고유의 번호\n>Registers: 레지스터의 중간값   \n>Process state: 대기,준비, 실행 등의 프로세스 상태  \n>CPU scheduling information : 언제, 어떤 순서로 cpu 할당 받을지의 정보   \n>Memory: 프로세스의 메모리 적재 주소, 베이스 레지스터, 한계 레지스터, 페이지 테이블    \n>List of Open files : 입출력장치 정보\n\n### 이 PCB로 어떻게 관리하나?\nCPU는 하나의 프로세스를 처음부터 끝까지 실행하고 다음 프로세스로 넘어가는 게 아니라 여러개의 프로세스를 일부씩 실행한다. \n\n이 때 한 프로세스의 작업을 멈추고 그 작업까지의 중간 값(프로그램 카운터, 각종 레지스터 값 등)을 필수적으로 저장해야 하는데 이것을 PCB에 저장하는 것이다. \n\n이 중간 정보는 `문맥Context`으로 추상화하여 표현되고 기존 프로세스의 문맥을 백업하고 다음 프로세스의 문맥을 읽어서 실행하는 것을 `문맥 교환 Context Switching` 이라고 한다. \n(이 문맥 교환의 속도가 빨라지면 프로세스들은 동시에 실행되는 것처럼 보인다)\n\n>[동시에 실행되는 것 '처럼' 보인다]   \n>프로세스간의 문맥교환으로 동시에 실행되는 것처럼 보인다는 것은 동시성(Concurrency)를 의미한다.   \n>정말 동시에 실행되는 것은  CPU의 다수 코어에 의한 병렬성(Parallelism)을 의미한다.\n\n## 프로세스의 구성\n\n`정적 할당 영역`   \n코드 영역Code Segment : 기계어로 이루어진 명령어. Read-Only\n\n데이터 영역 Data Segment : 프로그램 실행 동안 유지되는 데이터 \nex) 전역변수, 상수\n\n`동적 할당 영역`   \n힙 영역 Heap Segment : 프로그래머가 직접 할당할 수 있는 저장 공간 \nex)생성자, 인스턴스 (C에서 malloc()과 free()로 관리하는 영역) \n\n스택 영역 Stack Segment : 함수의 호출에 의해 할당되고 함수 종료시 소멸하는 데이터 \nex)매개변수, 지역변수 \n\n*즉, PCB가 있는 커널영역과 사용자 영역을 구성하는 위 네가지 영역으로 프로세스가 구성된다*\n\n## 프로세스의 상태\n\n프로세스는 번갈아 실행되는 과정에서 여러 상태를 거치는데, 이 상태를 cpu는 알아야 한다. \n입출력장치를 사용하는 프로세스의 경우 입력 완료 인터럽트를 기다려야 실행할 수 있는 경우가 있고, 실행이 끝난 프로세스는 메모리를 반환해야 하기 때문이다. \n그래서 운영체제는 PCB에 프로세스의 상태를 저장하고 이를 읽어낸다. \n\n- 생성 상태(new)\n- 준비 상태(ready)\n- 대기상태(blocked) \n- 실행상태(running)\n- 종료상태(terminated)\n\n`준비상태 vs 대기상태`   \n준비상태는 cpu에서 메모리를 할당받으면 실행할 수 있는 상태를 의미한다. \n준비상태에서 실행상태가 되는 것을 **dispatch**라고 한다 \n\n대기상태는 입출력 장치의 완료 신호(인터럽트)를 기다리는 등 특정 이벤트의 발생을 기다리는 상태를 의미한다. 대기상태에서 이벤트가 완료되면 **준비상태**가 된다. \n\n## 스레드\n\n### 프로세스와 스레드\n\n스레드는 `프로세스를 구성하는 실행 단위`를 의미한다 \n스레드는 **프로세스의 자원**을 공유하며 여러개가 존재 할 수 있다. \n스레드들은 위에서 살펴 본 프로세스의 스택 영역에 존재하며 프로세스의 코드,데이터, 힙영역을 다른 스레드들과 공유할 수 있다. \n\n### 멀티스레드\n\n멀티프로세스와 비슷하게 스레드 또한 문맥교환을 하며 동시성을 가지고 처리된다. \n\n멀티스레드는 한 프로세스 내에서 필요한 자원을 공유하기 때문에 새로운 프로세스를 실행하는 것보다 효율적으로 메모리를 관리할 수 있다.\n\n하지만 자원을 공유한다는 것은 한 스레드의 오류가 다른 스레드에 영향을 미칠 수 있음을 의미한다\n\n### 프로세스와 스레드의 동기화 문제 \n\n```java\npublic class Calculator {  \n    static int count = 0;  \n  \n    public static void main(String[] args) {  \n        int max = 10;  \n  \n        for (int i = 0; i < maxCount; i++) {  \n            new Thread(() -> {  \n                count++;  \n                System.out.println(count);  \n            }).start();  \n        }  \n    }  \n}\n```\n위 코드는 얼핏 보면 문제가 없어보이지만 \n```\n1\n3\n4\n2\n5\n6\n```\n실행결과는 뒤죽박죽인채로 나온다.   \n이렇게 같은 전역변수에 스레드가 무분별한 순서로 접근하거나, 정해진 순서대로 실행되어야 할 스레드가 순서대로 실행되지 않는 때 이 멀티 스레드는 '`Synchronization Issue`를 가진다'라고 한다.\n\n### 프로세스 동기화 문제? 스레드 동기화 문제?\n\n둘 다 동기화 문제를 가질 수 있다.   \n위 예시에서는 스레드의 동기화를 사용했지만 `실행의 흐름`을 가지는 모든 것들이 동기화 문제를 내재하고 있다.   \n\n예를 들어 `Book.txt`를 쓰는 프로세스 A와 같은 파일을 읽는 프로세스 B가 있다면 두 프로세스가 공유자원에 접근할 수 있다는 것이고 `A->B`의 정해진 순서대로 실행되어야 하기 때문이다. "},{"excerpt":"Gatsby로 Github pages 개인 블로그 만들기 TIL을 적는 블로그와 기술블로그를 분리하고 싶어서 따로 웹사이트 만들 방법을 찾다가 github pages과 연동하여 손쉽게 웹사이트를 만들 수 있는 SSG 프레임워크를 찾게 되었다. 여러가지가 있지만 가장 많이 쓰이는 것들은  과  이다.  처음에는 로 웹사이트를 만들었지만 몇 가지 고치고 싶은…","fields":{"slug":"/gatsby_website/"},"frontmatter":{"date":"December 26, 2023","title":"Gatsby와 Github Pages로 개인 블로그 만들기","tags":["gatsby","github_pages"]},"rawMarkdownBody":"\n## Gatsby로 Github pages 개인 블로그 만들기\n\nTIL을 적는 블로그와 기술블로그를 분리하고 싶어서 따로 웹사이트 만들 방법을 찾다가 github pages과 연동하여 손쉽게 웹사이트를 만들 수 있는 SSG 프레임워크를 찾게 되었다. 여러가지가 있지만 가장 많이 쓰이는 것들은 `Jekyll` 과 `Gatsby` 이다. \n\n처음에는 `Jekyll`로 웹사이트를 만들었지만 몇 가지 고치고 싶은 사항들이 보였는데 나는 Ruby를 잘 모르기 때문에 내가 나중에 커스텀하기에 조금 무리가 있는 것 같아 `Gatsby`로 프레임워크를 바꾸게 되었다.\n\n나는 미리 만들어진 [테마](https://github.com/devHudi/gatsby-starter-hoodie) 를 사용했다.(감사합니다)\n\n## Gatsby 설치 와 웹사이트 설정\n\nGatsby cli를 설치해준다 \n```\nnpm install -g gatsby-cli\n```\n\n`Gatsby Starter Library`에서 마음에 드는 테마를 선택할 수 있다.\n원하는 테마를 고른 뒤\n\n```\nnpx gatsby new {local-folder-name} {theme-name}\n```\n을 실행해 로컬에 Gatsby 템플레이트를  만든다.\n\n```\ncd {local-folder-name}\ngatsby develop\n```\n을 실행하면 로컬에서 개츠비 서버가 구동된다. 서버 주소는 `http://localhost:8000`이다. \n\n## Github Repository연결\n\nGithub의 새 리포지토리를 생성한다.\n나는 `{username}.github.io`로 이름을 지정했다. \n다른 이름을 쓰거나 소스코드용 리포지토리를 따로 두고 싶다면 \n[Gatsby 공식문서](https://www.gatsbyjs.com/docs/how-to/previews-deploys-hosting/how-gatsby-works-with-github-pages/) 를 참고하길 바란다\n\n리포지토리를 생성했다면\n```\ngit remote add origin {github-https-address}\n```\n를 실행해 본인이 만든 웹사이트 폴더와 원격 저장소를 연결해준다.\n\n그리고 커스텀 블로그 설정, 포스트 작성등을 한 후 \n```\ngit add .\ngit commit -m \"{commit-name}\"\ngit push origin main\n```\n위를 실행하여 원격 저장소에 로컬의 변경사항을 푸쉬해준다.\n\n## 배포 방법 정하기\n\n배포 방법에는 여러가지가 있지만 대표적인 2가지만 설명한다\n\n1. Netlify\nGithub과 유연하게 연동 되고 무료인 Netlify의 배포 시스템을 이용할 수 있다.\n[A Step-by-Step Guide: Gatsby on Netlify](https://www.netlify.com/blog/2016/02/24/a-step-by-step-guide-gatsby-on-netlify/) 공식문서\n2. Github pages\nGithub pages에서 제공하는 `gh-pages`를 이용하여 배포할 수 있다\n[How Gatsby Works with GitHub Pages](https://www.gatsbyjs.com/docs/how-to/previews-deploys-hosting/how-gatsby-works-with-github-pages/)공식 문서\n\n나는 Github pages를 이용했는데 Netlify는 커스텀 도메인이 없으면 `{smt}.netflify.app`를 도메인으로 제공해주는데 이 도메인보다 Github pages가 제공하는 `{smt}.github.io` 도메인이 마음에 들어서다(...)\n\n\n## Github Pages로 배포하기 \n\n배포용 브랜치 설정\ngh-pages는 배포용 브랜치가 따로 있어야 한다.   \n우리가 만든 gatsby 프로젝트의 main 브랜치에서 블로깅 작업을 했다면 public 폴더에 index.html이 있을텐데 기본적으로는 `.gitignore`에서 public을 푸쉬하지 않게 설정되어 있다.\n\n현 상태에서 배포용 브랜치를 하나 만들어 둔다.\n```\ngit branch deploy\n```\n\n이 배포용 브랜치에 public 폴더를 따로 업로드하는 작업을 gh-pages가 해준다. \n`gh-pages`패키지를 설치하자.\n\n```\nnpm install gh-pages\n```\n\n>[Trouble Shooting]   \n나는 npm install 과중에서 dependency conflict가 발생했다.   \nnode 7 버전 이후 부터는 peer dependency를 자동으로 설치하기 때문에 이미 있는 dependency와 버전이 다를 경우 충돌이 발생한다고 한다\n\n>-> 위의 install 커맨드에 `--force` 를 추가해서 충돌이 일어난 peer dependency를 강제 설치하거나 `--legacy-peer-deps`로 자동설치를 막는 방법이 있다. 나는 `--force`로 설치했다.\n\n\n다음은 `package.json` 에 배포에 사용할 스크립트를 추가해주면 된다.\n```\n\"scripts\": {\n    \"deploy\": \"gatsby build && gh-pages -d public -b deploy\"\n}\n```\n\n그리고\n```\nnpm run deploy \n```\n를 실행한다.   \n   \n>[Trouble Shooting]   \n>위 커맨드를 실행했을 때 Segmentation Fault오류가 발생했는데 잘못된 메모리 접근이라는 오류였기 때문에 `npm run clean`으로 캐쉬를 삭제 한후 다시 위 커맨드를 실행해주었다.\n\n이후\nGithub repository의 Settings-Pages\n`Build and deployment`에서 배포용 브랜치로 전환해준다.\n\n이렇게 까지 하면 `Actions`에서 웹사이트를 배포해주고 브라우저에서\n```\nhttps://{user-name}.github.io/\n```\n 주소로 접속이 가능해진다. \n\n"},{"excerpt":"정적 팩토리 메서드란? Java에서는   연산자를 이용하여 클래스의 인스턴스를 생성하는 것 외에  메서드를 사용하여 인스턴스를 반환 받는 기법이 있다. 간단한 예시로 사용방법을 알아 보자.  이  클래스는 두개의 변수를 받는 생성자를 가지고 있다.  여기에  를 추가함으로써 인스턴스를 반환 받는 다른  를  생성하는 기법이라고 할 수 있다.  정적팩토리 …","fields":{"slug":"/staticfactorymethod/"},"frontmatter":{"date":"December 24, 2023","title":"정적 팩토리 메서드, 언제 쓸까?","tags":["java","staticfactorymethod"]},"rawMarkdownBody":"## 정적 팩토리 메서드란?\n\nJava에서는 `new`  연산자를 이용하여 클래스의 인스턴스를 생성하는 것 외에 `static` 메서드를 사용하여 인스턴스를 반환 받는 기법이 있다.\n\n간단한 예시로 사용방법을 알아 보자. \n```java\nclass Car {  \n    String name;  \n  \n    Car(String name) {   \n        this.name = name;  \n    }  \n}\n```\n\n이 `Car` 클래스는 두개의 변수를 받는 생성자를 가지고 있다.  여기에\n```java\nstatic Car from(String name) {  \n    return new Car(name);  \n}\n```\n\n`정적 팩토리 메서드` 를 추가함으로써 인스턴스를 반환 받는 다른 `통로` 를  생성하는 기법이라고 할 수 있다. \n\n## 정적팩토리 메서드, 왜 쓰나?\n\n`Effective Java`는 1장에서 `생성자 대신 정적 팩토리 메서드를 고려하라` 라고 말한다. 그리고 그에 대한 장점과 단점에 대해 설명하는데 이와 관련된 잘 정리된 글이 많이 있으므로 [링크](https://tecoble.techcourse.co.kr/post/2020-05-26-static-factory-method/)   \n이 포스트에서는 내가  **개인적으로** 언제 이 기법을 사용하는지 서술해 보려고 한다.\n\n### 이름이 있는 것이 나은 경우\n\n위의 예시는 없다치고 사용자가 입력한 텍스트로  Car 객체를 생성한다고 가정해보자\n\n```java\npublic void createCar(String input) {  \n    Car car1 = new Car(input);  \n    Car car2 = Car.from(input);  \n}\n```\n\n위의 두 줄의 코드는 같은 기능을 하지만 이 코드를 읽는 사람에게는 다른 의미로 해석되곤 한다. \n`new` 연산자는 `이 Car는 input을 멤버 변수로 가지는군` 이라면\n`from` 은 `이 input은 객체 내에서 특정한 로직으로 변환되겠군` 이라는 멘탈 모델을 제공한다. \n\n`이름을 가질 수 있다` 는 것이 정적 팩토리 메서드의 가장 큰 장점인 만큼 이름이 있는 것이 나은 경우에 해당 기법을 쓴다.\n\n### 한 가지 방법으로만 객체가 생성되게 하고 싶을 때\n\n우리는 다른 프로그래머 혹은 미래의 나 자신이 실수로라도 User 클래스를 적합하지 않은 id로 생성하는 것을 막고 싶다.\n\n```java\nclass User {  \n    int id;  \n  \n    private User(int id) {  \n        this.id = id;  \n    }  \n  \n    static User from(int id) {  \n        if (isInvalidId(id)) {  \n            return null;  \n        }  \n        return new User(id);  \n    }  \n}\n```\n\n따라서 이 때는 `private` 으로 `new`연산자의 객체 생성을 막고, 정적 팩토리 메서드가 아니면 이 객체를 인스턴스화 할 수 없게 만들수 있다.\n이는 `싱글톤패턴` 의 사용과도 일맥상통한다\n\n### 같은 객체가 여러번 쓰여야할 때\n\n같은 객체가 여러번 조회, 캐싱되는 경우에 쓰인다. 한번 만들어 놓고 계속 사용하거나 미리 캐싱된 객체가 없는 경우에*만*  객체를 생성해 메모리를 아낄 수 있다.\n```java \nclass CarFactory {  \n  \n    static final Map<String, Car> cars = new HashMap<>();  \n  \n    static {  \n        cars.put(\"a\", new Car(\"a\"));  \n        cars.put(\"b\", new Car(\"b\"));  \n        cars.put(\"c\", new Car(\"c\"));  \n    }  \n\t  //if cache doesn't contains key, only then instantiate new car\n    static Car from(String text) {  \n        if (cars.containsKey(text)) {  \n            return cars.get(text);  \n        }  \n        return new Car(text);  \n    }  \n}\n```\n\n\n>[정리]   \n 1.생성자의 파라미터에 들어가는 값이 그대로 객체의 상태가 되지 않는 경우   \n 2.지정된 경우 이외의 객체 생성을 막고 싶은 경우   \n 3.여러번 쓰이는 같은 객체에 불필요한 메모리를 할당하고 싶지 않은 경우 \n\n이외에도 여러가지 경우가 있지만 나 같은 경우 위의 세가지의 경우에 정적 팩토리 메서드의 필요성을 느끼고 사용하고 있다.\n\n## private으로 생성자 막기\n\n`Effective Java` 는 private으로 생성자를 제한하고 정적 팩토리 메서드만을 두는 것은 단점이자 장점이라 서술한다. private으로 생성자를 막으면 하위 클래스를 만들 수 없기 때문에 `Composition`을 자연스럽게 지향하게 되고, 객체의 불변성에 기여할 수 있기 때문이라고 한다. \n\n```java\nclass CarSet {  \n    private final Map<Car, String> cars;  \n    \n    public CarSet(Map<Car, String> cars) {  \n        this.coins = new HashMap<>(cars);  \n    }  \n  \n    public static CarSet from(String text) {  \n        //Complicated Logic..   \n        //..  \n        return new CarSet(cars);  \n    }  \n}\n```\n하지만 public 생성자와 정적 팩토리 메서드를 같이 가지는 객체도 장점이 있다고 생각한다\ntext를 검증과 파싱으로 만드는 정적 팩토리 메서드를 따로 두고 이미 만들어진 map으로도 해당 객체가생성되게 하면 이 객체의 `재사용성` 이 늘어날 수 있기 때문이다."}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}